{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e2cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# We'll be dumping and reading the data from this directory\n",
    "DATA_DIR_PATH = os.path.join(os.getcwd())\n",
    "CORA_PATH = os.path.join(DATA_DIR_PATH, 'cora')  # this is checked-in no need to make a directory\n",
    "\n",
    "#\n",
    "# Cora specific constants\n",
    "#\n",
    "\n",
    "# Thomas Kipf et al. first used this split in GCN paper and later Petar Veličković et al. in GAT paper\n",
    "CORA_TRAIN_RANGE = [0, 140]  # we're using the first 140 nodes as the training nodes\n",
    "CORA_VAL_RANGE = [140, 140+500]\n",
    "CORA_TEST_RANGE = [1708, 1708+1000]\n",
    "CORA_NUM_INPUT_FEATURES = 1433\n",
    "CORA_NUM_CLASSES = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b70404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's define these simple functions for loading/saving Pickle files - we need them for Cora\n",
    "\n",
    "# All Cora data is stored as pickle\n",
    "def pickle_read(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "def pickle_save(path, data):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "461ff449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll pass the training config dictionary a bit later\n",
    "def load_graph_data(training_config, device):\n",
    "    dataset_name = training_config['dataset_name'].lower()\n",
    "    should_visualize = training_config['should_visualize']\n",
    "\n",
    "    if dataset_name == DatasetType.CORA.name.lower():\n",
    "\n",
    "        # shape = (N, FIN), where N is the number of nodes and FIN is the number of input features\n",
    "        node_features_csr = pickle_read(os.path.join(CORA_PATH, 'node_features.csr'))\n",
    "        # shape = (N, 1)\n",
    "        node_labels_npy = pickle_read(os.path.join(CORA_PATH, 'node_labels.npy'))\n",
    "        # shape = (N, number of neighboring nodes) <- this is a dictionary not a matrix!\n",
    "        adjacency_list_dict = pickle_read(os.path.join(CORA_PATH, 'adjacency_list.dict'))\n",
    "\n",
    "        # Normalize the features (helps with training)\n",
    "        node_features_csr = normalize_features_sparse(node_features_csr)\n",
    "        num_of_nodes = len(node_labels_npy)\n",
    "\n",
    "        # shape = (2, E), where E is the number of edges, and 2 for source and target nodes. Basically edge index\n",
    "        # contains tuples of the format S->T, e.g. 0->3 means that node with id 0 points to a node with id 3.\n",
    "        topology = build_edge_index(adjacency_list_dict, num_of_nodes, add_self_edges=True)\n",
    "\n",
    "        # Note: topology is just a fancy way of naming the graph structure data \n",
    "        # (aside from edge index it could be in the form of an adjacency matrix)\n",
    "\n",
    "        if should_visualize:  # network analysis and graph drawing\n",
    "            plot_in_out_degree_distributions(topology, num_of_nodes, dataset_name)  # we'll define these in a second\n",
    "            visualize_graph(topology, node_labels_npy, dataset_name)\n",
    "\n",
    "        # Convert to dense PyTorch tensors\n",
    "\n",
    "        # Needs to be long int type because later functions like PyTorch's index_select expect it\n",
    "        topology = torch.tensor(topology, dtype=torch.long, device=device)\n",
    "        node_labels = torch.tensor(node_labels_npy, dtype=torch.long, device=device)  # Cross entropy expects a long int\n",
    "        node_features = torch.tensor(node_features_csr.todense(), device=device)\n",
    "\n",
    "        # Indices that help us extract nodes that belong to the train/val and test splits\n",
    "        train_indices = torch.arange(CORA_TRAIN_RANGE[0], CORA_TRAIN_RANGE[1], dtype=torch.long, device=device)\n",
    "        val_indices = torch.arange(CORA_VAL_RANGE[0], CORA_VAL_RANGE[1], dtype=torch.long, device=device)\n",
    "        test_indices = torch.arange(CORA_TEST_RANGE[0], CORA_TEST_RANGE[1], dtype=torch.long, device=device)\n",
    "\n",
    "        return node_features, node_labels, topology, train_indices, val_indices, test_indices\n",
    "    else:\n",
    "        raise Exception(f'{dataset_name} not yet supported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddd8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features_sparse(node_features_sparse):\n",
    "    assert sp.issparse(node_features_sparse), f'Expected a sparse matrix, got {node_features_sparse}.'\n",
    "\n",
    "    # Instead of dividing (like in normalize_features_dense()) we do multiplication with inverse sum of features.\n",
    "    # Modern hardware (GPUs, TPUs, ASICs) is optimized for fast matrix multiplications! ^^ (* >> /)\n",
    "    # shape = (N, FIN) -> (N, 1), where N number of nodes and FIN number of input features\n",
    "    node_features_sum = np.array(node_features_sparse.sum(-1))  # sum features for every node feature vector\n",
    "\n",
    "    # Make an inverse (remember * by 1/x is better (faster) then / by x)\n",
    "    # shape = (N, 1) -> (N)\n",
    "    node_features_inv_sum = np.power(node_features_sum, -1).squeeze()\n",
    "\n",
    "    # Again certain sums will be 0 so 1/0 will give us inf so we replace those by 1 which is a neutral element for mul\n",
    "    node_features_inv_sum[np.isinf(node_features_inv_sum)] = 1.\n",
    "\n",
    "    # Create a diagonal matrix whose values on the diagonal come from node_features_inv_sum\n",
    "    diagonal_inv_features_sum_matrix = sp.diags(node_features_inv_sum)\n",
    "\n",
    "    # We return the normalized features.\n",
    "    return diagonal_inv_features_sum_matrix.dot(node_features_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "105e087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edge_index(adjacency_list_dict, num_of_nodes, add_self_edges=True):\n",
    "    source_nodes_ids, target_nodes_ids = [], []\n",
    "    seen_edges = set()\n",
    "\n",
    "    for src_node, neighboring_nodes in adjacency_list_dict.items():\n",
    "        for trg_node in neighboring_nodes:\n",
    "            # if this edge hasn't been seen so far we add it to the edge index (coalescing - removing duplicates)\n",
    "            if (src_node, trg_node) not in seen_edges:  # it'd be easy to explicitly remove self-edges (Cora has none..)\n",
    "                source_nodes_ids.append(src_node)\n",
    "                target_nodes_ids.append(trg_node)\n",
    "\n",
    "                seen_edges.add((src_node, trg_node))\n",
    "\n",
    "    if add_self_edges:\n",
    "        source_nodes_ids.extend(np.arange(num_of_nodes))\n",
    "        target_nodes_ids.extend(np.arange(num_of_nodes))\n",
    "\n",
    "    # shape = (2, E), where E is the number of edges in the graph\n",
    "    edge_index = np.row_stack((source_nodes_ids, target_nodes_ids))\n",
    "\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfce4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class GATLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation #3 was inspired by PyTorch Geometric: https://github.com/rusty1s/pytorch_geometric\n",
    "\n",
    "    But, it's hopefully much more readable! (and of similar performance)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # We'll use these constants in many functions so just extracting them here as member fields\n",
    "    src_nodes_dim = 0  # position of source nodes in edge index\n",
    "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
    "\n",
    "    # These may change in the inductive setting - leaving it like this for now (not future proof)\n",
    "    nodes_dim = 0      # node dimension (axis is maybe a more familiar term nodes_dim is the position of \"N\" in tensor)\n",
    "    head_dim = 1       # attention head dim\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n",
    "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_out_features = num_out_features\n",
    "        self.concat = concat  # whether we should concatenate or average the attention heads\n",
    "        self.add_skip_connection = add_skip_connection\n",
    "\n",
    "        #\n",
    "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
    "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
    "        #\n",
    "\n",
    "        # You can treat this one matrix as num_of_heads independent W matrices\n",
    "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "\n",
    "        # After we concatenate target node (node i) and source node (node j) we apply the \"additive\" scoring function\n",
    "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
    "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
    "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
    "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "\n",
    "        # Bias is definitely not crucial to GAT - feel free to experiment (I pinged the main author, Petar, on this one)\n",
    "        if bias and concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))\n",
    "        elif bias and not concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        if add_skip_connection:\n",
    "            self.skip_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "        else:\n",
    "            self.register_parameter('skip_proj', None)\n",
    "\n",
    "        #\n",
    "        # End of trainable weights\n",
    "        #\n",
    "\n",
    "        self.leakyReLU = nn.LeakyReLU(0.2)  # using 0.2 as in the paper, no need to expose every setting\n",
    "        self.activation = activation\n",
    "        # Probably not the nicest design but I use the same module in 3 locations, before/after features projection\n",
    "        # and for attention coefficients. Functionality-wise it's the same as using independent modules.\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.log_attention_weights = log_attention_weights  # whether we should log the attention weights\n",
    "        self.attention_weights = None  # for later visualization purposes, I cache the weights here\n",
    "\n",
    "        self.init_params()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization\n",
    "        #\n",
    "\n",
    "        in_nodes_features, edge_index = data  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'\n",
    "\n",
    "        # shape = (N, FIN) where N - number of nodes in the graph, FIN - number of input features per node\n",
    "        # We apply the dropout to all of the input node features (as mentioned in the paper)\n",
    "        # Note: for Cora features are already super sparse so it's questionable how much this actually helps\n",
    "        in_nodes_features = self.dropout(in_nodes_features)\n",
    "\n",
    "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
    "        # We project the input node features into NH independent output features (one for each attention head)\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        nodes_features_proj = self.dropout(nodes_features_proj)  # in the official GAT imp they did dropout here as well\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation\n",
    "        #\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
    "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
    "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
    "        # by the edge index.\n",
    "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)\n",
    "\n",
    "        # shape = (E, NH, 1)\n",
    "        attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n",
    "        # Add stochasticity to neighborhood aggregation\n",
    "        attentions_per_edge = self.dropout(attentions_per_edge)\n",
    "\n",
    "        #\n",
    "        # Step 3: Neighborhood aggregation\n",
    "        #\n",
    "\n",
    "        # Element-wise (aka Hadamard) product. Operator * does the same thing as torch.mul\n",
    "        # shape = (E, NH, FOUT) * (E, NH, 1) -> (E, NH, FOUT), 1 gets broadcast into FOUT\n",
    "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
    "\n",
    "        # This part sums up weighted and projected neighborhood feature vectors for every target node\n",
    "        # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n",
    "\n",
    "        #\n",
    "        # Step 4: Residual/skip connections, concat and bias\n",
    "        #\n",
    "\n",
    "        out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\n",
    "        return (out_nodes_features, edge_index)\n",
    "\n",
    "    #\n",
    "    # Helper functions (without comments there is very little code so don't be scared!)\n",
    "    #\n",
    "\n",
    "    def neighborhood_aware_softmax(self, scores_per_edge, trg_index, num_of_nodes):\n",
    "        \"\"\"\n",
    "        As the fn name suggest it does softmax over the neighborhoods. Example: say we have 5 nodes in a graph.\n",
    "        Two of them 1, 2 are connected to node 3. If we want to calculate the representation for node 3 we should take\n",
    "        into account feature vectors of 1, 2 and 3 itself. Since we have scores for edges 1-3, 2-3 and 3-3\n",
    "        in scores_per_edge variable, this function will calculate attention scores like this: 1-3/(1-3+2-3+3-3)\n",
    "        (where 1-3 is overloaded notation it represents the edge 1-3 and its (exp) score) and similarly for 2-3 and 3-3\n",
    "         i.e. for this neighborhood we don't care about other edge scores that include nodes 4 and 5.\n",
    "\n",
    "        Note:\n",
    "        Subtracting the max value from logits doesn't change the end result but it improves the numerical stability\n",
    "        and it's a fairly common \"trick\" used in pretty much every deep learning framework.\n",
    "        Check out this link for more details:\n",
    "\n",
    "        https://stats.stackexchange.com/questions/338285/how-does-the-subtraction-of-the-logit-maximum-improve-learning\n",
    "\n",
    "        \"\"\"\n",
    "        # Calculate the numerator. Make logits <= 0 so that e^logit <= 1 (this will improve the numerical stability)\n",
    "        scores_per_edge = scores_per_edge - scores_per_edge.max()\n",
    "        exp_scores_per_edge = scores_per_edge.exp()  # softmax\n",
    "\n",
    "        # Calculate the denominator. shape = (E, NH)\n",
    "        neigborhood_aware_denominator = self.sum_edge_scores_neighborhood_aware(exp_scores_per_edge, trg_index, num_of_nodes)\n",
    "\n",
    "        # 1e-16 is theoretically not needed but is only there for numerical stability (avoid div by 0) - due to the\n",
    "        # possibility of the computer rounding a very small number all the way to 0.\n",
    "        attentions_per_edge = exp_scores_per_edge / (neigborhood_aware_denominator + 1e-16)\n",
    "\n",
    "        # shape = (E, NH) -> (E, NH, 1) so that we can do element-wise multiplication with projected node features\n",
    "        return attentions_per_edge.unsqueeze(-1)\n",
    "\n",
    "    def sum_edge_scores_neighborhood_aware(self, exp_scores_per_edge, trg_index, num_of_nodes):\n",
    "        # The shape must be the same as in exp_scores_per_edge (required by scatter_add_) i.e. from E -> (E, NH)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge)\n",
    "\n",
    "        # shape = (N, NH), where N is the number of nodes and NH the number of attention heads\n",
    "        size = list(exp_scores_per_edge.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes\n",
    "        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)\n",
    "\n",
    "        # position i will contain a sum of exp scores of all the nodes that point to the node i (as dictated by the\n",
    "        # target index)\n",
    "        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)\n",
    "\n",
    "        # Expand again so that we can use it as a softmax denominator. e.g. node i's sum will be copied to\n",
    "        # all the locations where the source nodes pointed to i (as dictated by the target index)\n",
    "        # shape = (N, NH) -> (E, NH)\n",
    "        return neighborhood_sums.index_select(self.nodes_dim, trg_index)\n",
    "\n",
    "    def aggregate_neighbors(self, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):\n",
    "        size = list(nodes_features_proj_lifted_weighted.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes  # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = torch.zeros(size, dtype=in_nodes_features.dtype, device=in_nodes_features.device)\n",
    "\n",
    "        # shape = (E) -> (E, NH, FOUT)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(edge_index[self.trg_nodes_dim], nodes_features_proj_lifted_weighted)\n",
    "        # aggregation step - we accumulate projected, weighted node features for all the attention heads\n",
    "        # shape = (E, NH, FOUT) -> (N, NH, FOUT)\n",
    "        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)\n",
    "\n",
    "        return out_nodes_features\n",
    "\n",
    "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
    "        \"\"\"\n",
    "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
    "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
    "\n",
    "        \"\"\"\n",
    "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
    "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
    "\n",
    "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
    "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
    "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
    "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
    "\n",
    "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
    "\n",
    "    def explicit_broadcast(self, this, other):\n",
    "        # Append singleton dimensions until this.dim() == other.dim()\n",
    "        for _ in range(this.dim(), other.dim()):\n",
    "            this = this.unsqueeze(-1)\n",
    "\n",
    "        # Explicitly expand so that shapes are the same\n",
    "        return this.expand_as(other)\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"\n",
    "        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n",
    "            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
    "\n",
    "        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n",
    "        Feel free to experiment - there may be better initializations depending on your problem.\n",
    "\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    def skip_concat_bias(self, attention_coefficients, in_nodes_features, out_nodes_features):\n",
    "        if self.log_attention_weights:  # potentially log for later visualization in playground.py\n",
    "            self.attention_weights = attention_coefficients\n",
    "\n",
    "        if self.add_skip_connection:  # add skip or residual connection\n",
    "            if out_nodes_features.shape[-1] == in_nodes_features.shape[-1]:  # if FIN == FOUT\n",
    "                # unsqueeze does this: (N, FIN) -> (N, 1, FIN), out features are (N, NH, FOUT) so 1 gets broadcast to NH\n",
    "                # thus we're basically copying input vectors NH times and adding to processed vectors\n",
    "                out_nodes_features += in_nodes_features.unsqueeze(1)\n",
    "            else:\n",
    "                # FIN != FOUT so we need to project input feature vectors into dimension that can be added to output\n",
    "                # feature vectors. skip_proj adds lots of additional capacity which may cause overfitting.\n",
    "                out_nodes_features += self.skip_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        if self.concat:\n",
    "            # shape = (N, NH, FOUT) -> (N, NH*FOUT)\n",
    "            out_nodes_features = out_nodes_features.view(-1, self.num_of_heads * self.num_out_features)\n",
    "        else:\n",
    "            # shape = (N, NH, FOUT) -> (N, FOUT)\n",
    "            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out_nodes_features += self.bias\n",
    "\n",
    "        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "712beef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The most interesting and hardest implementation is implementation #3.\n",
    "    Imp1 and imp2 differ in subtle details but are basically the same thing.\n",
    "\n",
    "    So I'll focus on imp #3 in this notebook.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection=True, bias=True,\n",
    "                 dropout=0.6, log_attention_weights=False):\n",
    "        super().__init__()\n",
    "        assert num_of_layers == len(num_heads_per_layer) == len(num_features_per_layer) - 1, f'Enter valid arch params.'\n",
    "\n",
    "        num_heads_per_layer = [1] + num_heads_per_layer  # trick - so that I can nicely create GAT layers below\n",
    "\n",
    "        gat_layers = []  # collect GAT layers\n",
    "        for i in range(num_of_layers):\n",
    "            layer = GATLayer(\n",
    "                num_in_features=num_features_per_layer[i] * num_heads_per_layer[i],  # consequence of concatenation\n",
    "                num_out_features=num_features_per_layer[i+1],\n",
    "                num_of_heads=num_heads_per_layer[i+1],\n",
    "                concat=True if i < num_of_layers - 1 else False,  # last GAT layer does mean avg, the others do concat\n",
    "                activation=nn.ELU() if i < num_of_layers - 1 else None,  # last layer just outputs raw scores\n",
    "                dropout_prob=dropout,\n",
    "                add_skip_connection=add_skip_connection,\n",
    "                bias=bias,\n",
    "                log_attention_weights=log_attention_weights\n",
    "            )\n",
    "            gat_layers.append(layer)\n",
    "\n",
    "        self.gat_net = nn.Sequential(\n",
    "            *gat_layers,\n",
    "        )\n",
    "\n",
    "    # data is just a (in_nodes_features, edge_index) tuple, I had to do it like this because of the nn.Sequential:\n",
    "    # https://discuss.pytorch.org/t/forward-takes-2-positional-arguments-but-3-were-given-for-nn-sqeuential-with-linear-layers/65698\n",
    "    def forward(self, data):\n",
    "        return self.gat_net(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27d39fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# checking whether you have a GPU\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDatasetType\u001b[39;00m(enum\u001b[38;5;241m.\u001b[39mEnum):\n\u001b[1;32m      8\u001b[0m     CORA \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU\n",
    "\n",
    "class DatasetType(enum.Enum):\n",
    "    CORA = 0\n",
    "    \n",
    "config = {\n",
    "    'dataset_name': DatasetType.CORA.name,\n",
    "    'should_visualize': False\n",
    "}\n",
    "\n",
    "node_features, node_labels, edge_index, train_indices, val_indices, test_indices = load_graph_data(config, device)\n",
    "\n",
    "print(node_features.shape, node_features.dtype)\n",
    "print(node_labels.shape, node_labels.dtype)\n",
    "print(edge_index.shape, edge_index.dtype)\n",
    "print(train_indices.shape, train_indices.dtype)\n",
    "print(val_indices.shape, val_indices.dtype)\n",
    "print(test_indices.shape, test_indices.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "459d71ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1433.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.008781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  1433.000000\n",
       "mean      0.000698\n",
       "std       0.008781\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       0.000000\n",
       "max       0.111111"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(node_features[0].cpu().numpy()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08fcb409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features[train_indices].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512f6b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708])\n",
      "torch.Size([2708])\n",
      "torch.Size([2708])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  ..., False, False, False], device='cuda:0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora', split=\"public\") ## public\n",
    "dataset.transform = T.NormalizeFeatures()\n",
    "data = dataset[0]\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "X = data.x.to(device) #node_features # dataset[0].x.to(device) #node_features # dataset[0].x.to(device)\n",
    "y = data.y.to(device) #node_labels #dataset[0].y.to(device)\n",
    "\n",
    "test = data.test_mask.to(device) #test_indices #dataset[0].test_mask.to(device)\n",
    "train = data.train_mask.to(device) #train_indices # dataset[0].train_mask.to(device)\n",
    "val = data.val_mask.to(device) #val_indices # dataset[0].val_mask.to(device)\n",
    "\n",
    "\n",
    "y_train = y[train].to(device)\n",
    "y_val = y[val].to(device)\n",
    "y_test = y[test].to(device)\n",
    "\n",
    "edge_index = data.edge_index.to(device) #edge_index#dataset[0].edge_index.to(device)\n",
    "edge_index = add_self_loops(edge_index)[0]\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e383e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 2705, 2706, 2707],\n",
       "        [ 633, 1862, 2582,  ..., 2705, 2706, 2707]], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae07996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "from torch.nn import ELU, Softmax, Dropout\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def glorot(shape):\n",
    "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial_tensor = torch.FloatTensor(shape[0], shape[1]).uniform_(-init_range, init_range).type(torch.float32)\n",
    "    return torch.nn.parameter.Parameter(data=initial_tensor, requires_grad=True)\n",
    "\n",
    "##glorot & bengio init; early stoppping\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, hidden_dim, out_dim):\n",
    "        super(GAT, self).__init__()\n",
    "        heads = 8\n",
    "        self.conv1 = GATConv(X.shape[1], hidden_dim, heads=heads, dropout=0.6, add_self_loops=False)\n",
    "        self.conv2 = GATConv(hidden_dim*heads, out_dim, dropout=0.6, heads=1, concat=False, add_self_loops=False)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        X, edge_index = data\n",
    "        p = 0.6\n",
    "        X = F.dropout(X, p=p, training=self.training)\n",
    "        X, (edges, attn_weights_1) = self.conv1(X, edge_index, return_attention_weights=True)\n",
    "        X = F.elu(X)\n",
    "        X = F.dropout(X, p=p, training=self.training)\n",
    "        X, (edges, attn_weights_2) = self.conv2(X, edge_index, return_attention_weights=True)\n",
    "        return F.log_softmax(X, dim=1), (attn_weights_1, attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88953876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 100\n",
      "Epoch: 200\n",
      "Epoch: 300\n",
      "Epoch: 400\n",
      "Epoch: 500\n",
      "tensor(0.8340, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "import torch\n",
    "from torch import tensor\n",
    "import copy\n",
    "\n",
    "# def evaluate(model, mask):\n",
    "#     global X, edge_index, y_test\n",
    "    \n",
    "#     with torch.inference_mode():\n",
    "#         model.eval()\n",
    "#         logits, (attn_weights_1, attn_weights_2) = model((X,edge_index))\n",
    "#         y_pred = logits.argmax(-1)\n",
    "#         y_pred_test = y_pred[mask]\n",
    "#         matching = (y_pred_test == y_test).sum()\n",
    "#         acc = matching / y_test.shape[0]\n",
    "#     return acc\n",
    "accs = []\n",
    "accuracy = None\n",
    "best_model = None\n",
    "for i in range(1):\n",
    "    \n",
    "    model = GAT(8, 7).to(device)# GAT(2, [8,1], [X.shape[1], 8, 7], add_skip_connection=False, bias=True,\n",
    "                     #dropout=0.6, log_attention_weights=True).to(device) #GAT(8, dataset.num_classes).to(device)\n",
    "    model.reset_parameters()\n",
    "    optim = Adam(model.parameters(), lr=0.005, weight_decay=0.0005 )\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "    last_loss = 0\n",
    "    # increased_loss = 0\n",
    "    patience = 100\n",
    "\n",
    "    min_loss = 0\n",
    "    max_acc = 0\n",
    "    curr_step  = 0\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs = 1000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {str(epoch)}\") \n",
    "        model.train()\n",
    "        logits, (attn_weights_1, attn_weights_2) = model((X,edge_index))\n",
    "        \n",
    "    #     loss = loss_fn(logits[train], y_train)\n",
    "        loss = F.nll_loss(logits[train], y_train)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            logits,(attn_weights_1, attn_weights_2) = model((X,edge_index))\n",
    "            val_loss = F.nll_loss(logits[val], y_val)\n",
    "            y_pred = logits.argmax(-1)\n",
    "            y_pred_val = y_pred[val]\n",
    "            matching = (y_pred_val == y_val).sum()\n",
    "            acc = matching / y_val.shape[0]\n",
    "\n",
    "\n",
    "#             acc = acc.cpu().numpy()\n",
    "\n",
    "#             if acc >= max_acc or loss.item() <= min_loss:\n",
    "#                 curr_step = 0\n",
    "#                 max_acc = max(acc, max_acc)\n",
    "#                 min_loss = min(loss.item(), min_loss)\n",
    "#             else:\n",
    "#                 curr_step += 1\n",
    "#                 if patience == curr_step:\n",
    "#                     print(epoch)\n",
    "#                     break\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            with torch.inference_mode():\n",
    "                model.eval()\n",
    "                logits, (attn_weights_1, attn_weights_2) = model((X,edge_index))\n",
    "                y_pred = logits.argmax(-1)\n",
    "                y_pred_test = y_pred[test]\n",
    "                matching = (y_pred_test == y_test).sum()\n",
    "                accuracy = matching / y_test.shape[0]\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "        val_losses.append(val_loss.item())\n",
    "        if patience > 0 and epoch > epochs // 2:\n",
    "            tmp = tensor(val_losses[-(patience + 1):-1])\n",
    "            if val_loss > tmp.mean().item():\n",
    "                break\n",
    "    print(accuracy)\n",
    "    \n",
    "    accs.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e32ef79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    logits, (attn_weights_1, attn_weights_2) = best_model((X,edge_index))\n",
    "    y_pred = logits.argmax(-1)\n",
    "    y_pred_test = y_pred[test].cpu()\n",
    "    df[\"pred\"] = y_pred_test.cpu()\n",
    "    df[\"true\"] = y_test.cpu()\n",
    "df.to_csv(\"gat_pred_0834.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6ea03843",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, (attn_weights_1, attn_weights_2) = model((X,edge_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c0eb0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If features of neighboring nodes are similar to each other (e.g. based on cosine similarity), how high are the respective attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "eeba64fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2983, 0.2400, 0.2829,  ..., 0.4678, 0.2193, 0.1894], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_1[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e1f188df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13264,)\n",
      "(13264,)\n",
      "PearsonRResult(statistic=0.7705918116989339, pvalue=0.0)\n",
      "(13264,)\n",
      "PearsonRResult(statistic=0.7524490012402268, pvalue=0.0)\n",
      "(13264,)\n",
      "PearsonRResult(statistic=0.7774745653128273, pvalue=0.0)\n",
      "(13264,)\n",
      "PearsonRResult(statistic=0.7601042068634979, pvalue=0.0)\n",
      "(13264,)\n",
      "PearsonRResult(statistic=0.7808525261111766, pvalue=0.0)\n",
      "(13264,)\n",
      "PearsonRResult(statistic=0.7802990223019417, pvalue=0.0)\n",
      "(13264,)\n",
      "PearsonRResult(statistic=0.7772642331363361, pvalue=0.0)\n",
      "(13264,)\n",
      "PearsonRResult(statistic=0.7771160567103308, pvalue=0.0)\n",
      "PearsonRResult(statistic=0.7814432715763204, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "source_lift = X.index_select(0, edge_index[0])\n",
    "target_lift = X.index_select(0, edge_index[1])\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "score = cos(source_lift, target_lift)\n",
    "exp_score = torch.exp(score)\n",
    "summed_exp_score = torch.zeros_like(exp_score).scatter(0, edge_index[1],exp_score, reduce=\"add\")\n",
    "target_lifted_summed_exp_score = summed_exp_score.index_select(0, edge_index[1])\n",
    "normalized_scores = exp_score / target_lifted_summed_exp_score\n",
    "y = normalized_scores.cpu().numpy()\n",
    "print(y.shape)\n",
    "for i in range(8):\n",
    "    x = attn_weights_1[:,i].squeeze().cpu().detach().numpy()\n",
    "    print(x.shape)\n",
    "    res = stats.pearsonr(x, y)\n",
    "    print(res)\n",
    "    \n",
    "x = attn_weights_2.squeeze().cpu().detach().numpy()\n",
    "res = stats.pearsonr(x, y)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aff6eb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.att_src',\n",
       "              tensor([[[-0.2039,  0.4802, -0.4523,  0.2791,  0.1785,  0.2704,  0.1466,\n",
       "                        -0.1215],\n",
       "                       [-0.2673,  0.4094,  0.0263,  0.4597,  0.3036,  0.3103, -0.1237,\n",
       "                        -0.0064],\n",
       "                       [ 0.3324, -0.3757, -0.1585,  0.2742, -0.2757, -0.4746, -0.1436,\n",
       "                        -0.4312],\n",
       "                       [ 0.4503,  0.1946, -0.2227, -0.3955, -0.0531, -0.2799, -0.4734,\n",
       "                        -0.0487],\n",
       "                       [ 0.0528,  0.4646, -0.2926, -0.0858,  0.3943, -0.0182, -0.3293,\n",
       "                        -0.2251],\n",
       "                       [ 0.5097, -0.0889, -0.1569, -0.3378, -0.4267,  0.1275, -0.4548,\n",
       "                        -0.0540],\n",
       "                       [ 0.2209,  0.1460, -0.2964,  0.5937, -0.3204, -0.0217, -0.2116,\n",
       "                         0.4255],\n",
       "                       [ 0.1549, -0.3991, -0.1879,  0.4243, -0.0795,  0.3225, -0.0078,\n",
       "                        -0.1255]]], device='cuda:0')),\n",
       "             ('conv1.att_dst',\n",
       "              tensor([[[ 0.1790, -0.1341,  0.1330,  0.3155, -0.0899, -0.2408,  0.1116,\n",
       "                        -0.3309],\n",
       "                       [-0.1704,  0.1647,  0.0929, -0.0439, -0.1847, -0.1221,  0.1595,\n",
       "                        -0.0963],\n",
       "                       [ 0.0976, -0.0408,  0.0960,  0.1321,  0.0618, -0.0379,  0.1245,\n",
       "                        -0.1509],\n",
       "                       [ 0.2159, -0.0716, -0.2476,  0.0088, -0.1396, -0.1797, -0.2477,\n",
       "                        -0.1044],\n",
       "                       [-0.0601,  0.1761,  0.0976, -0.1358,  0.3662,  0.1876, -0.2739,\n",
       "                         0.0534],\n",
       "                       [ 0.0888,  0.2195,  0.0931, -0.0861, -0.1404,  0.1103, -0.1087,\n",
       "                        -0.2688],\n",
       "                       [-0.1780,  0.0166, -0.3143,  0.0079, -0.1322,  0.2373, -0.2954,\n",
       "                         0.1474],\n",
       "                       [ 0.1696,  0.0668, -0.0471,  0.2108, -0.1197,  0.0112,  0.3517,\n",
       "                         0.1207]]], device='cuda:0')),\n",
       "             ('conv1.bias',\n",
       "              tensor([ 2.6206e-02, -6.3853e-02, -1.7203e-02, -4.1525e-02,  1.1432e-02,\n",
       "                       5.5041e-02, -7.9709e-02,  1.1085e-01,  1.3847e-01,  5.4985e-02,\n",
       "                       1.0510e-01,  2.5927e-02,  6.2023e-02,  1.0968e-01,  3.3133e-02,\n",
       "                       7.6725e-02,  3.6093e-02,  8.7288e-04, -2.1102e-02, -6.3110e-03,\n",
       "                       1.5906e-01,  7.5855e-02,  2.5640e-02,  1.2013e-01, -3.0788e-02,\n",
       "                       1.0194e-01,  6.3975e-02,  9.3063e-02,  1.2228e-01,  4.3683e-02,\n",
       "                       4.9244e-03,  4.3914e-02,  1.4525e-01,  1.0841e-01, -5.6008e-02,\n",
       "                       1.1673e-01,  4.4404e-02,  1.6301e-01, -9.9285e-03,  4.2299e-02,\n",
       "                       8.2718e-02, -4.2981e-02,  3.9800e-02, -8.0744e-02,  1.2003e-01,\n",
       "                       1.0709e-01,  1.0899e-01,  4.9525e-02, -7.3409e-02,  3.1035e-02,\n",
       "                       1.6596e-01,  6.4285e-05,  1.3359e-01, -6.2633e-03, -9.1181e-03,\n",
       "                       4.2766e-02,  8.0392e-02,  9.3323e-02, -6.3432e-02,  6.0823e-02,\n",
       "                      -1.0654e-02, -7.4767e-02, -8.1908e-02,  1.2942e-02], device='cuda:0')),\n",
       "             ('conv1.lin_src.weight',\n",
       "              tensor([[ 0.0591, -0.0327,  0.0801,  ...,  0.0424,  0.0084,  0.0627],\n",
       "                      [-0.0334,  0.1642,  0.0435,  ..., -0.0215,  0.0787, -0.0339],\n",
       "                      [ 0.0778, -0.0643,  0.0970,  ...,  0.0193,  0.0219, -0.0533],\n",
       "                      ...,\n",
       "                      [ 0.0141,  0.0472, -0.0704,  ...,  0.0121,  0.0543, -0.0256],\n",
       "                      [-0.0105, -0.0050,  0.0391,  ..., -0.0253, -0.0147, -0.0056],\n",
       "                      [ 0.0030,  0.0582, -0.0234,  ...,  0.0254, -0.0003, -0.0026]],\n",
       "                     device='cuda:0')),\n",
       "             ('conv1.lin_dst.weight',\n",
       "              tensor([[ 0.0591, -0.0327,  0.0801,  ...,  0.0424,  0.0084,  0.0627],\n",
       "                      [-0.0334,  0.1642,  0.0435,  ..., -0.0215,  0.0787, -0.0339],\n",
       "                      [ 0.0778, -0.0643,  0.0970,  ...,  0.0193,  0.0219, -0.0533],\n",
       "                      ...,\n",
       "                      [ 0.0141,  0.0472, -0.0704,  ...,  0.0121,  0.0543, -0.0256],\n",
       "                      [-0.0105, -0.0050,  0.0391,  ..., -0.0253, -0.0147, -0.0056],\n",
       "                      [ 0.0030,  0.0582, -0.0234,  ...,  0.0254, -0.0003, -0.0026]],\n",
       "                     device='cuda:0')),\n",
       "             ('conv2.att_src',\n",
       "              tensor([[[ 0.2894,  0.5459, -0.3200, -0.2463, -0.2425, -0.4914,  0.4283]]],\n",
       "                     device='cuda:0')),\n",
       "             ('conv2.att_dst',\n",
       "              tensor([[[-0.5838,  0.3717,  0.3508, -0.5919, -0.3097, -0.5661,  0.9481]]],\n",
       "                     device='cuda:0')),\n",
       "             ('conv2.bias',\n",
       "              tensor([-0.1343, -0.0689,  0.1470,  0.0208,  0.1054,  0.0076, -0.0008],\n",
       "                     device='cuda:0')),\n",
       "             ('conv2.lin_src.weight',\n",
       "              tensor([[ 0.3150, -0.2218,  0.4257, -0.4060,  0.1556, -0.3563, -0.3303, -0.4252,\n",
       "                       -0.3778, -0.1598, -0.1086,  0.3580, -0.4536, -0.4592,  0.1511, -0.3898,\n",
       "                        0.5249, -0.1804, -0.3140,  0.2821,  0.4840,  0.1836,  0.3614, -0.2912,\n",
       "                        0.4234, -0.0081, -0.2365, -0.3429, -0.4425, -0.3955,  0.3741, -0.4977,\n",
       "                       -0.1990,  0.4257, -0.0527, -0.2477,  0.5106, -0.1549,  0.0542, -0.0385,\n",
       "                       -0.3061, -0.0803,  0.4716,  0.3213,  0.4928, -0.0932, -0.3549, -0.1783,\n",
       "                       -0.2861,  0.2819,  0.1969, -0.3372,  0.3383, -0.3345,  0.4696,  0.3892,\n",
       "                        0.0432,  0.5596,  0.3049, -0.1679,  0.3496,  0.1970,  0.3857, -0.4674],\n",
       "                      [ 0.2421,  0.2861,  0.3265,  0.2058, -0.3760, -0.2750,  0.4031, -0.1287,\n",
       "                       -0.1805, -0.3801, -0.4773, -0.4561, -0.2031, -0.3180, -0.4170, -0.0322,\n",
       "                       -0.2872, -0.2987, -0.1961, -0.4416, -0.2048, -0.2102,  0.3318, -0.4673,\n",
       "                       -0.4225,  0.3568, -0.1303,  0.4090, -0.2827,  0.4401,  0.1757, -0.0905,\n",
       "                       -0.4253, -0.4093, -0.3541,  0.2578, -0.2944, -0.1267,  0.2188,  0.3067,\n",
       "                        0.3194,  0.3502, -0.1091,  0.3557, -0.3633, -0.4211, -0.3116,  0.4028,\n",
       "                        0.3695, -0.2965, -0.3859,  0.2629, -0.1165,  0.3292,  0.4349,  0.1948,\n",
       "                       -0.4014, -0.0039,  0.2692, -0.2149,  0.0098, -0.1137,  0.2527,  0.1712],\n",
       "                      [-0.3150, -0.4289,  0.4328,  0.1674,  0.0298,  0.3393,  0.2790,  0.2310,\n",
       "                        0.1487, -0.3369,  0.2050, -0.5005,  0.1907, -0.2075, -0.3227,  0.3412,\n",
       "                       -0.1477, -0.4976, -0.3576, -0.5190,  0.3097,  0.3727, -0.4295,  0.1498,\n",
       "                       -0.3782,  0.3341,  0.4336, -0.3065, -0.2995,  0.2591, -0.3537, -0.5064,\n",
       "                       -0.1847,  0.3743, -0.4587,  0.3712,  0.2978,  0.3339, -0.4625, -0.3046,\n",
       "                        0.3877, -0.3937,  0.3598, -0.2202, -0.0996, -0.4347,  0.1197,  0.1343,\n",
       "                       -0.4567,  0.1569, -0.5276, -0.0435,  0.4567, -0.3114, -0.1373,  0.3621,\n",
       "                        0.4323,  0.4968, -0.1685, -0.1165,  0.2067, -0.3903, -0.2722, -0.2793],\n",
       "                      [ 0.3361, -0.4923, -0.4469, -0.2985,  0.0402,  0.4479, -0.3619,  0.4421,\n",
       "                        0.1023,  0.3725,  0.4091, -0.4197, -0.2787,  0.3920,  0.3888,  0.2860,\n",
       "                        0.2022,  0.5255, -0.1338, -0.4219, -0.4448,  0.2934,  0.0047,  0.3664,\n",
       "                       -0.3031, -0.3563,  0.3980, -0.2869,  0.4630,  0.3010,  0.5557,  0.1589,\n",
       "                       -0.2254, -0.3883,  0.2121, -0.1617, -0.4384,  0.4494,  0.3806, -0.2470,\n",
       "                        0.3701,  0.0412, -0.1569, -0.5340,  0.4837,  0.5255,  0.3884, -0.0965,\n",
       "                        0.2002,  0.1345,  0.3883,  0.2779, -0.3787,  0.4044, -0.3047,  0.2495,\n",
       "                       -0.3941,  0.4110, -0.4680, -0.2526, -0.3088, -0.2756, -0.3587,  0.2244],\n",
       "                      [ 0.2142, -0.2652, -0.3112, -0.2865, -0.3872, -0.2078, -0.3691,  0.3236,\n",
       "                        0.2946, -0.4783,  0.4439,  0.3177,  0.1332,  0.3037, -0.3016, -0.5357,\n",
       "                        0.4341, -0.3687, -0.3393, -0.0230,  0.3814,  0.2096,  0.2886, -0.3362,\n",
       "                        0.3023,  0.4669, -0.5544,  0.4149,  0.5226, -0.2700, -0.4625,  0.2836,\n",
       "                        0.4704, -0.3594,  0.1486,  0.4482, -0.4590,  0.1070, -0.4616, -0.2456,\n",
       "                        0.1496, -0.3047, -0.1995, -0.2136,  0.4094,  0.4502, -0.4526, -0.1777,\n",
       "                       -0.3189, -0.4596,  0.4928, -0.4721,  0.2007, -0.4173,  0.4309, -0.4251,\n",
       "                        0.0698, -0.2437,  0.1775,  0.5467, -0.5451, -0.2520,  0.4782,  0.2710],\n",
       "                      [-0.3342,  0.1349, -0.5128, -0.3252, -0.4644,  0.1413, -0.2039,  0.2918,\n",
       "                       -0.3918,  0.5099,  0.0946,  0.1591,  0.4882,  0.2976, -0.3717,  0.0497,\n",
       "                       -0.2252, -0.3239,  0.2030,  0.2989,  0.4111, -0.5262,  0.4058,  0.4273,\n",
       "                       -0.0791,  0.3455, -0.3602,  0.3487, -0.4273, -0.3129,  0.0128,  0.2355,\n",
       "                        0.3780,  0.1726, -0.3338, -0.1933,  0.2857,  0.3758,  0.2010,  0.5436,\n",
       "                       -0.3685,  0.3107,  0.4224,  0.3435, -0.2588, -0.2540,  0.4280,  0.5096,\n",
       "                       -0.4254,  0.2401, -0.1775,  0.0391,  0.4348,  0.4062, -0.3078, -0.0347,\n",
       "                       -0.3695, -0.0992, -0.2204, -0.3060,  0.2593,  0.3190, -0.2831, -0.3636],\n",
       "                      [-0.3218, -0.2717, -0.3849,  0.3586,  0.5392, -0.3675,  0.1640, -0.1201,\n",
       "                       -0.3418,  0.0789, -0.4685,  0.2346,  0.3466,  0.2757,  0.2990,  0.0572,\n",
       "                        0.3557,  0.4582,  0.5087,  0.2732, -0.2498, -0.4633, -0.4882,  0.3077,\n",
       "                        0.3345, -0.4519,  0.3542, -0.2985,  0.1661, -0.3621, -0.1442,  0.3275,\n",
       "                        0.4742, -0.4153,  0.4016, -0.4297, -0.1274, -0.4519, -0.0962, -0.0736,\n",
       "                       -0.2900,  0.3800, -0.3632,  0.1395, -0.2352, -0.0571,  0.4005, -0.2408,\n",
       "                       -0.1952, -0.3700,  0.0088, -0.3911, -0.2672,  0.2375, -0.4831, -0.2899,\n",
       "                        0.2682,  0.0748,  0.3470,  0.3680,  0.0809,  0.3734,  0.1621,  0.3878]],\n",
       "                     device='cuda:0')),\n",
       "             ('conv2.lin_dst.weight',\n",
       "              tensor([[ 0.3150, -0.2218,  0.4257, -0.4060,  0.1556, -0.3563, -0.3303, -0.4252,\n",
       "                       -0.3778, -0.1598, -0.1086,  0.3580, -0.4536, -0.4592,  0.1511, -0.3898,\n",
       "                        0.5249, -0.1804, -0.3140,  0.2821,  0.4840,  0.1836,  0.3614, -0.2912,\n",
       "                        0.4234, -0.0081, -0.2365, -0.3429, -0.4425, -0.3955,  0.3741, -0.4977,\n",
       "                       -0.1990,  0.4257, -0.0527, -0.2477,  0.5106, -0.1549,  0.0542, -0.0385,\n",
       "                       -0.3061, -0.0803,  0.4716,  0.3213,  0.4928, -0.0932, -0.3549, -0.1783,\n",
       "                       -0.2861,  0.2819,  0.1969, -0.3372,  0.3383, -0.3345,  0.4696,  0.3892,\n",
       "                        0.0432,  0.5596,  0.3049, -0.1679,  0.3496,  0.1970,  0.3857, -0.4674],\n",
       "                      [ 0.2421,  0.2861,  0.3265,  0.2058, -0.3760, -0.2750,  0.4031, -0.1287,\n",
       "                       -0.1805, -0.3801, -0.4773, -0.4561, -0.2031, -0.3180, -0.4170, -0.0322,\n",
       "                       -0.2872, -0.2987, -0.1961, -0.4416, -0.2048, -0.2102,  0.3318, -0.4673,\n",
       "                       -0.4225,  0.3568, -0.1303,  0.4090, -0.2827,  0.4401,  0.1757, -0.0905,\n",
       "                       -0.4253, -0.4093, -0.3541,  0.2578, -0.2944, -0.1267,  0.2188,  0.3067,\n",
       "                        0.3194,  0.3502, -0.1091,  0.3557, -0.3633, -0.4211, -0.3116,  0.4028,\n",
       "                        0.3695, -0.2965, -0.3859,  0.2629, -0.1165,  0.3292,  0.4349,  0.1948,\n",
       "                       -0.4014, -0.0039,  0.2692, -0.2149,  0.0098, -0.1137,  0.2527,  0.1712],\n",
       "                      [-0.3150, -0.4289,  0.4328,  0.1674,  0.0298,  0.3393,  0.2790,  0.2310,\n",
       "                        0.1487, -0.3369,  0.2050, -0.5005,  0.1907, -0.2075, -0.3227,  0.3412,\n",
       "                       -0.1477, -0.4976, -0.3576, -0.5190,  0.3097,  0.3727, -0.4295,  0.1498,\n",
       "                       -0.3782,  0.3341,  0.4336, -0.3065, -0.2995,  0.2591, -0.3537, -0.5064,\n",
       "                       -0.1847,  0.3743, -0.4587,  0.3712,  0.2978,  0.3339, -0.4625, -0.3046,\n",
       "                        0.3877, -0.3937,  0.3598, -0.2202, -0.0996, -0.4347,  0.1197,  0.1343,\n",
       "                       -0.4567,  0.1569, -0.5276, -0.0435,  0.4567, -0.3114, -0.1373,  0.3621,\n",
       "                        0.4323,  0.4968, -0.1685, -0.1165,  0.2067, -0.3903, -0.2722, -0.2793],\n",
       "                      [ 0.3361, -0.4923, -0.4469, -0.2985,  0.0402,  0.4479, -0.3619,  0.4421,\n",
       "                        0.1023,  0.3725,  0.4091, -0.4197, -0.2787,  0.3920,  0.3888,  0.2860,\n",
       "                        0.2022,  0.5255, -0.1338, -0.4219, -0.4448,  0.2934,  0.0047,  0.3664,\n",
       "                       -0.3031, -0.3563,  0.3980, -0.2869,  0.4630,  0.3010,  0.5557,  0.1589,\n",
       "                       -0.2254, -0.3883,  0.2121, -0.1617, -0.4384,  0.4494,  0.3806, -0.2470,\n",
       "                        0.3701,  0.0412, -0.1569, -0.5340,  0.4837,  0.5255,  0.3884, -0.0965,\n",
       "                        0.2002,  0.1345,  0.3883,  0.2779, -0.3787,  0.4044, -0.3047,  0.2495,\n",
       "                       -0.3941,  0.4110, -0.4680, -0.2526, -0.3088, -0.2756, -0.3587,  0.2244],\n",
       "                      [ 0.2142, -0.2652, -0.3112, -0.2865, -0.3872, -0.2078, -0.3691,  0.3236,\n",
       "                        0.2946, -0.4783,  0.4439,  0.3177,  0.1332,  0.3037, -0.3016, -0.5357,\n",
       "                        0.4341, -0.3687, -0.3393, -0.0230,  0.3814,  0.2096,  0.2886, -0.3362,\n",
       "                        0.3023,  0.4669, -0.5544,  0.4149,  0.5226, -0.2700, -0.4625,  0.2836,\n",
       "                        0.4704, -0.3594,  0.1486,  0.4482, -0.4590,  0.1070, -0.4616, -0.2456,\n",
       "                        0.1496, -0.3047, -0.1995, -0.2136,  0.4094,  0.4502, -0.4526, -0.1777,\n",
       "                       -0.3189, -0.4596,  0.4928, -0.4721,  0.2007, -0.4173,  0.4309, -0.4251,\n",
       "                        0.0698, -0.2437,  0.1775,  0.5467, -0.5451, -0.2520,  0.4782,  0.2710],\n",
       "                      [-0.3342,  0.1349, -0.5128, -0.3252, -0.4644,  0.1413, -0.2039,  0.2918,\n",
       "                       -0.3918,  0.5099,  0.0946,  0.1591,  0.4882,  0.2976, -0.3717,  0.0497,\n",
       "                       -0.2252, -0.3239,  0.2030,  0.2989,  0.4111, -0.5262,  0.4058,  0.4273,\n",
       "                       -0.0791,  0.3455, -0.3602,  0.3487, -0.4273, -0.3129,  0.0128,  0.2355,\n",
       "                        0.3780,  0.1726, -0.3338, -0.1933,  0.2857,  0.3758,  0.2010,  0.5436,\n",
       "                       -0.3685,  0.3107,  0.4224,  0.3435, -0.2588, -0.2540,  0.4280,  0.5096,\n",
       "                       -0.4254,  0.2401, -0.1775,  0.0391,  0.4348,  0.4062, -0.3078, -0.0347,\n",
       "                       -0.3695, -0.0992, -0.2204, -0.3060,  0.2593,  0.3190, -0.2831, -0.3636],\n",
       "                      [-0.3218, -0.2717, -0.3849,  0.3586,  0.5392, -0.3675,  0.1640, -0.1201,\n",
       "                       -0.3418,  0.0789, -0.4685,  0.2346,  0.3466,  0.2757,  0.2990,  0.0572,\n",
       "                        0.3557,  0.4582,  0.5087,  0.2732, -0.2498, -0.4633, -0.4882,  0.3077,\n",
       "                        0.3345, -0.4519,  0.3542, -0.2985,  0.1661, -0.3621, -0.1442,  0.3275,\n",
       "                        0.4742, -0.4153,  0.4016, -0.4297, -0.1274, -0.4519, -0.0962, -0.0736,\n",
       "                       -0.2900,  0.3800, -0.3632,  0.1395, -0.2352, -0.0571,  0.4005, -0.2408,\n",
       "                       -0.1952, -0.3700,  0.0088, -0.3911, -0.2672,  0.2375, -0.4831, -0.2899,\n",
       "                        0.2682,  0.0748,  0.3470,  0.3680,  0.0809,  0.3734,  0.1621,  0.3878]],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd9488d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GAT' object has no attribute 'gat_net'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat_net\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattention_weights[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;66;03m#[0].squeeze()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GAT' object has no attribute 'gat_net'"
     ]
    }
   ],
   "source": [
    "model.gat_net[0].attention_weights[:,0].shape#[0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7de6a08",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GAT' object has no attribute 'gat_net'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(stats\u001b[38;5;241m.\u001b[39mpearsonr(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat_net\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattention_weights[:, i]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu(), normalized_scores\u001b[38;5;241m.\u001b[39mcpu()))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GAT' object has no attribute 'gat_net'"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "for i in range(8):\n",
    "    print(stats.pearsonr(model.gat_net[0].attention_weights[:, i].squeeze().cpu(), normalized_scores.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e14b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.pearsonr(model.gat_net[1].attention_weights.squeeze().cpu(), normalized_scores.cpu()) ##todo other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ba4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gat_net[1].attention_weights.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619cb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_scores.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9204c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(epoch+1), train_losses)\n",
    "plt.plot(range(epoch+1), val_losses)\n",
    "plt.legend([\"train\", \"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8920482a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
