Models;Cora;CiteSeer;PubMed
GraphAware;"{0 hops: {'model': LogisticRegression(C=9.89464848441749, l1_ratio=0.1083794378326286,
                      max_iter=8192, tol=3.9088643651368724e-05),
   'user_function': user_function,
   'attention_config': {'inter_layer_normalize': True,
    'use_pseudo_attention': True,
    'cosine_eps': 0.001,
    'dropout_attn': None}},
  3 hops: {'model': LogisticRegression(C=4.9289832447362025, l1_ratio=0.1761472021705791,
                      max_iter=8192, tol=0.017104071425396022),
   'user_function': norm_user_function,
   'attention_config': None},
  8 hops: {'model': LogisticRegression(C=3.726771337407598, l1_ratio=0.032154684509317244,
                      max_iter=128, tol=7.557643967339885e-05),
   'user_function': norm_user_function,
   'attention_config': {'inter_layer_normalize': True,
    'use_pseudo_attention': True,
    'cosine_eps': 0.001,
    'dropout_attn': None}}}";"{0 hops: {'model': LogisticRegression(C=3.9057765563512103, l1_ratio=0.3841472539205063,
max_iter=512, tol=0.00037394547447174774),
   'user_function': user_function, 'attention_config': {'inter_layer_normalize': True,    'use_pseudo_attention': True,
    'cosine_eps': 0.01,
    'dropout_attn': None}},
  3 hops: {'model': LogisticRegression(C=3.161729301367482, l1_ratio=0.41400664400322995,
max_iter=128, tol=0.0022154364103027916),   'user_function': user_function,   'attention_config': {'inter_layer_normalize': False,
    'use_pseudo_attention': True,
    'cosine_eps': 0.01,
    'dropout_attn': None}},
  8 hops: {'model': LogisticRegression(C=4.484384767955908, l1_ratio=0.31672332033998696,
max_iter=16384, tol=0.000418404361811845),
   'user_function': user_function,
   'attention_config': {'inter_layer_normalize': False,
    'use_pseudo_attention': True,
    'cosine_eps': 0.01,
 'dropout_attn': None}}";"{0 hops: {'model': LogisticRegression(C=19.012946053218332, max_iter=4096,
                      tol=0.011693958862167635),
   'user_function': upd_user_function,
   'attention_config': {'inter_layer_normalize': True,
    'use_pseudo_attention': True,
    'cosine_eps': 0.001,
    'dropout_attn': None}},
  3 hops: {'model': LogisticRegression(C=2.3417904147486635, max_iter=4096,
                      tol=1.877242326314165e-05),
   'user_function': norm_user_function,
   'attention_config': {'inter_layer_normalize': False,
    'use_pseudo_attention': True,
    'cosine_eps': 0.01,
    'dropout_attn': None}},
  8 hops: {'model': LogisticRegression(C=8.816857543671555, max_iter=256, tol=0.0003864950814262107),
   'user_function': norm_user_function,
   'attention_config': None}}"
ChebNets (2 layers);epochs: 200, learning rate: 0.01, weight decay: 0.0005, early stopping: 10, hidden dimension: 16, dropout: 0.5;epochs: 200, learning rate: 0.01, weight decay: 0.0005, early stopping: 10, hidden dimension: 16, dropout: 0.5;epochs: 200, learning rate: 0.01, weight decay: 0.0005, early stopping: 10, hidden dimension: 16, dropout: 0.5
GCN (2 layers);epochs: 200, learning rate: 0.01, weight decay: 0.0005, early stopping: 10, hidden dimension: 16, dropout: 0.5;epochs: 200, learning rate: 0.01, weight decay: 0.0005, early stopping: 10, hidden dimension: 16, dropout: 0.5;epochs: 200, learning rate: 0.01, weight decay: 0.0005, early stopping: 10, hidden dimension: 16, dropout: 0.5
GAT (2 layers);epochs: 1000, learning rate: 0.005, weight decay: 0.0005, early stopping: 100, hidden dimension: 8, dropout: 0.6, heads: 8;epochs: 1000, learning rate: 0.005, weight decay: 0.0005, early stopping: 100, hidden dimension: 8, dropout: 0.6, heads: 8;epochs: 1000, learning rate: 0.005, weight decay: 0.0005, early stopping: 100, hidden dimension: 8, dropout: 0.6, heads: 8
