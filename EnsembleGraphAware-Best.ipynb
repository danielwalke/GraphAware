{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3bf6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.base import BaseEstimator\n",
    "from typing import TypedDict\n",
    "import numpy as np\n",
    "import numpy\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "USER_FUNCTIONS = {\n",
    "    'sum': lambda origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors: sum_neighbors,\n",
    "    'mean': lambda origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors: sum_neighbors / num_neighbors,\n",
    "    'diff_of_origin_mean': lambda origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors: origin_features - sum_neighbors / num_neighbors,\n",
    "    'diff_of_updated_mean': lambda origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors: updated_features - sum_neighbors / num_neighbors,\n",
    "    'sum_of_origin_mean': lambda origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors: origin_features + sum_neighbors / num_neighbors,\n",
    "    'sum_of_updated_mean': lambda origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors: updated_features + sum_neighbors / num_neighbors,\n",
    "}\n",
    "## Assumption: the overall prediction perf improved when the performance of inidividual predictiors improves\n",
    "##TODO More input_validation, grid search method whoch accepts the same params\n",
    "class Framework:    \n",
    "    \n",
    "    def __init__(self, user_functions, \n",
    "                 hops_list:list[int],\n",
    "                 clfs:list,\n",
    "                 gpu_idx:int|None=None,\n",
    "                 handle_nan:float|None=None,\n",
    "                attention_configs:list=[]) -> None:\n",
    "        self.user_functions = user_functions\n",
    "        self.hops_list:list[int] = hops_list\n",
    "        self.clfs:list[int] = clfs\n",
    "        self.trained_clfs = None\n",
    "        self.gpu_idx:int|None = gpu_idx\n",
    "        self.handle_nan:float|int|None = handle_nan\n",
    "        self.attention_configs = attention_configs\n",
    "        self.device:torch.DeviceObjType = torch.device(f\"cuda:{str(self.gpu_idx)}\") if self.gpu_idx is not None and torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    \n",
    "    def update_user_function(self):\n",
    "        if self.user_function in USER_FUNCTIONS:\n",
    "            self.user_function = USER_FUNCTIONS[self.user_function]\n",
    "        else:\n",
    "            raise Exception(f\"Only the following string values are valid inputs for the user function: {[key for key in USER_FUNCTIONS]}. You can also specify your own function for aggregatioon.\")\n",
    "            \n",
    "    def get_features(self,\n",
    "                     X:torch.FloatTensor|numpy._typing.NDArray,\n",
    "                     edge_index:torch.LongTensor|numpy._typing.NDArray,\n",
    "                     mask:torch.BoolTensor|numpy._typing.NDArray,\n",
    "                    is_training:bool = False) -> tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        if mask is None:\n",
    "            mask = torch.ones(X.shape[0]).type(torch.bool)\n",
    "#         if isinstance(self.user_function, str):\n",
    "#             self.update_user_function()\n",
    "        ## To tensor\n",
    "        X = Framework.get_feature_tensor(X)\n",
    "        edge_index = Framework.get_edge_index_tensor(edge_index)\n",
    "        mask = Framework.get_mask_tensor(mask)\n",
    "        \n",
    "        ## To device\n",
    "        X = self.shift_tensor_to_device(X)\n",
    "        edge_index = self.shift_tensor_to_device(edge_index)\n",
    "        mask = self.shift_tensor_to_device(mask)\n",
    "        \n",
    "        aggregated_train_features_list = []\n",
    "        ## Aggregate\n",
    "        for hop_idx in range(len(self.hops_list)):\n",
    "            neighbor_features = self.aggregate(X, edge_index, hop_idx, is_training)\n",
    "            aggregated_train_features_list.append(neighbor_features[mask])\n",
    "        \n",
    "        return aggregated_train_features_list\n",
    "    \n",
    "    def aggregate(self, X:torch.FloatTensor, edge_index:torch.LongTensor,hop_idx, is_training:bool=False) -> torch.FloatTensor: \n",
    "        original_features = X\n",
    "        features_for_aggregation:torch.FloatTensor = torch.clone(X)\n",
    "        hops_list = self.hops_list[hop_idx]\n",
    "        for i, hop in enumerate(range(hops_list)):\n",
    "            if self.attention_configs[hop_idx] and self.attention_configs[hop_idx][\"inter_layer_normalize\"]:\n",
    "                features_for_aggregation = torch.nn.functional.normalize(features_for_aggregation, dim = 0)\n",
    "            source_lift = features_for_aggregation.index_select(0, edge_index[0])\n",
    "            target = edge_index[1]\n",
    "            \n",
    "            if self.attention_configs[hop_idx] and self.attention_configs[hop_idx][\"use_pseudo_attention\"]:\n",
    "                source_lift = self.apply_attention_mechanism(source_lift, features_for_aggregation, target,self.attention_configs[hop_idx], is_training)\n",
    "            \n",
    "            summed_neighbors = torch.zeros_like(features_for_aggregation, device=self.device).scatter_reduce(0, target.unsqueeze(0).repeat(features_for_aggregation.shape[1], 1).t(), source_lift, reduce=\"sum\", include_self = False)\n",
    "            summed_neighbors = torch.zeros_like(features_for_aggregation, device=self.device).scatter_(0, target.unsqueeze(0).repeat(features_for_aggregation.shape[1], 1).t(), source_lift, reduce=\"add\")\n",
    "            multiplied_neighbors = torch.ones_like(features_for_aggregation, device=self.device).scatter_reduce(0, target.unsqueeze(0).repeat(features_for_aggregation.shape[1], 1).t(), source_lift, reduce=\"prod\", include_self = False)\n",
    "            mean_neighbors = torch.zeros_like(features_for_aggregation, device=self.device).scatter_reduce(0, target.unsqueeze(0).repeat(features_for_aggregation.shape[1], 1).t(), source_lift, reduce=\"mean\", include_self = False)\n",
    "            max_neighbors = torch.zeros_like(features_for_aggregation, device=self.device).scatter_reduce(0, target.unsqueeze(0).repeat(features_for_aggregation.shape[1], 1).t(), source_lift, reduce=\"amax\", include_self = False)\n",
    "            min_neighbors = torch.zeros_like(features_for_aggregation, device=self.device).scatter_reduce(0, target.unsqueeze(0).repeat(features_for_aggregation.shape[1], 1).t(), source_lift, reduce=\"amin\", include_self = False)\n",
    "\n",
    "            num_source_neighbors = torch.zeros(features_for_aggregation.shape[0], dtype=torch.float, device=self.device)\n",
    "            num_source_neighbors.scatter_reduce(0, target, torch.ones_like(target, dtype=torch.float, device=self.device), reduce=\"sum\", include_self = False)\n",
    "            num_source_neighbors = num_source_neighbors.unsqueeze(-1)\n",
    "\n",
    "            user_function = self.user_functions[hop_idx]\n",
    "            updated_features = features_for_aggregation ## just renaming so that the key in the user function is clear\n",
    "            user_function_kwargs = {\n",
    "                                'original_features':original_features,\n",
    "                                'updated_features':updated_features,\n",
    "                                'summed_neighbors':summed_neighbors,\n",
    "                                'multiplied_neighbors':multiplied_neighbors,\n",
    "                                'mean_neighbors':mean_neighbors,\n",
    "                                'max_neighbors':max_neighbors,\n",
    "                                'min_neighbors':min_neighbors,\n",
    "                                'num_source_neighbors':num_source_neighbors,\n",
    "                                'hop':hop}\n",
    "            out = user_function(user_function_kwargs)\n",
    "            \n",
    "            if self.handle_nan is not None:\n",
    "                out = torch.nan_to_num(out, nan=self.handle_nan)\n",
    "            features_for_aggregation = out\n",
    "        return features_for_aggregation\n",
    "    \n",
    "    def apply_attention_mechanism(self, source_lift:torch.FloatTensor,\n",
    "                                  features_for_aggregation:torch.FloatTensor,\n",
    "                                  target:torch.LongTensor,\n",
    "                                  attention_config,\n",
    "                                 is_training:bool = False) -> torch.FloatTensor:\n",
    "        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        score = cos(source_lift, features_for_aggregation.index_select(0, target))\n",
    "        dropout_tens = None\n",
    "        \n",
    "        origin_scores = torch.clone(score)\n",
    "        if attention_config[\"cosine_eps\"]:\n",
    "            score[score < attention_config[\"cosine_eps\"]] = -torch.inf\n",
    "        if attention_config[\"dropout_attn\"] is not None and is_training:\n",
    "            dropout_tens = torch.FloatTensor(score.shape[0]).uniform_(0, 1)\n",
    "            score[dropout_tens < attention_config[\"dropout_attn\"]] = -torch.inf\n",
    "        exp_score = torch.exp(score)\n",
    "        summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce=\"add\")\n",
    "        target_lifted_summed_exp_score = summed_exp_score.index_select(0, target)\n",
    "        normalized_scores = exp_score / target_lifted_summed_exp_score\n",
    "        source_lift = normalized_scores.unsqueeze(1) * source_lift\n",
    "        return source_lift\n",
    "    \n",
    "    def fit(self,\n",
    "            X_train:torch.FloatTensor|numpy._typing.NDArray,\n",
    "            edge_index:torch.LongTensor|numpy._typing.NDArray,\n",
    "            y_train:torch.LongTensor|numpy._typing.NDArray,\n",
    "            train_mask:torch.BoolTensor|numpy._typing.NDArray|None,\n",
    "            kwargs_list = None\n",
    "            ) -> BaseEstimator:   \n",
    "        if train_mask is None:\n",
    "            train_mask = torch.ones(X_train.shape[0]).type(torch.bool)\n",
    "            \n",
    "        y_train = Framework.get_label_tensor(y_train)\n",
    "        y_train = y_train[train_mask]\n",
    "        \n",
    "        self.validate_input()\n",
    "        \n",
    "        aggregated_train_features_list = self.get_features(X_train, edge_index, train_mask, True)  \n",
    "        \n",
    "        trained_clfs = []\n",
    "        for i, aggregated_train_features in enumerate(aggregated_train_features_list):\n",
    "            clf = clone(self.clfs[i])\n",
    "            kwargs = kwargs_list[i] if kwargs_list and len(kwargs_list)>i is not None else {}\n",
    "            clf.fit(aggregated_train_features.cpu().numpy(), y_train,**kwargs)\n",
    "            trained_clfs.append(clf)\n",
    "        self.trained_clfs = trained_clfs\n",
    "        return trained_clfs    \n",
    "    \n",
    "    def predict_proba(self, X_test:torch.FloatTensor|numpy._typing.NDArray,\n",
    "                      edge_index:torch.LongTensor|numpy._typing.NDArray,\n",
    "                      test_mask:torch.BoolTensor|numpy._typing.NDArray|None,\n",
    "                      weights=None,\n",
    "                     kwargs_list = None):  \n",
    "        if test_mask is None:\n",
    "            test_mask = torch.ones(X_test.shape[0]).type(torch.bool)\n",
    "        aggregated_test_features_list = self.get_features(X_test, edge_index, test_mask)\n",
    "        \n",
    "        pred_probas = []\n",
    "        for i, clf in enumerate(self.trained_clfs):\n",
    "            aggregated_test_features = aggregated_test_features_list[i]\n",
    "            kwargs = kwargs_list[i] if kwargs_list is not None else {}\n",
    "            pred_proba = clf.predict_proba(aggregated_test_features.cpu().numpy(),**kwargs) if kwargs else clf.predict_proba(aggregated_test_features.cpu().numpy())\n",
    "            pred_probas.append(pred_proba)\n",
    "        final_pred_proba = np.average(np.asarray(pred_probas), weights=weights, axis=0)\n",
    "        return final_pred_proba\n",
    "        \n",
    "    \n",
    "    def predict(self,\n",
    "                X_test:torch.FloatTensor|numpy._typing.NDArray,\n",
    "                edge_index:torch.LongTensor|numpy._typing.NDArray,\n",
    "                test_mask:torch.BoolTensor|numpy._typing.NDArray|None,\n",
    "                 weights=None,\n",
    "                     kwargs_list = None):\n",
    "        return self.predict_proba(X_test, edge_index, test_mask, weights, kwargs_list).argmax(1)\n",
    "        \n",
    "\n",
    "    def validate_input(self):\n",
    "        pass\n",
    "            \n",
    "    @staticmethod\n",
    "    def get_feature_tensor(X:torch.FloatTensor|numpy._typing.NDArray) -> torch.FloatTensor|None:\n",
    "        if not torch.is_tensor(X):\n",
    "            try:\n",
    "                return torch.from_numpy(X).type(torch.float)\n",
    "            except:\n",
    "                raise Exception(\"Features input X must be numpy array or torch tensor!\")\n",
    "                return None \n",
    "        return X\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_label_tensor(y:torch.LongTensor|numpy._typing.NDArray) -> torch.LongTensor|None:\n",
    "        if not torch.is_tensor(y):\n",
    "            try:\n",
    "                return torch.from_numpy(y).type(torch.long)\n",
    "            except:\n",
    "                raise Exception(\"Label input y must be numpy array or torch tensor!\")\n",
    "                return None\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_mask_tensor(mask:torch.BoolTensor|numpy._typing.NDArray) -> torch.BoolTensor|None:\n",
    "        if not torch.is_tensor(mask):\n",
    "            try:\n",
    "                return torch.from_numpy(mask).type(torch.bool)\n",
    "            except:\n",
    "                raise Exception(\"Input mask must be numpy array or torch tensor!\")\n",
    "                return None\n",
    "        return mask\n",
    "            \n",
    "    @staticmethod\n",
    "    def get_edge_index_tensor(edge_index:torch.LongTensor|numpy._typing.NDArray) -> torch.LongTensor|None:\n",
    "        if not torch.is_tensor(edge_index):\n",
    "            try:\n",
    "                edge_index =  torch.from_numpy(edge_index).type(torch.long)\n",
    "                Framework.validate_edge_index(edge_index)\n",
    "                return edge_index\n",
    "            except:\n",
    "                raise Exception(\"Edge index must be numpy array or torch tensor\")\n",
    "                return None\n",
    "        return edge_index\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_edge_index(edge_index:torch.LongTensor) -> None:\n",
    "        if edge_index.shape[0] != 2:\n",
    "            raise Exception(\"Edge index must have the shape 2 x NumberOfEdges\")\n",
    "            # TODO: check max edge index and shape of features\n",
    "    \n",
    "    def shift_tensor_to_device(self,\n",
    "                               t:torch.FloatTensor) -> torch.FloatTensor:\n",
    "        if self.gpu_idx is not None:\n",
    "            return t.to(self.device) \n",
    "        return t\n",
    "    \n",
    "    def validate_grid_input(self, grid_params):\n",
    "        if len(grid_params) != 1 and self.use_feature_based_aggregation:\n",
    "            raise Exception(\"You need to provide grid parameter for the classifier!\")\n",
    "        if len(grid_params) != 2 and not self.use_feature_based_aggregation:\n",
    "            raise Exception(\"You need to provide two grid parameter, one for each classifier!\")\n",
    "        return\n",
    "    \n",
    "    def hyper_param_tuning(spaces, objectives, n_iter, X_train, y_train, X_val, y_val):\n",
    "        ## bayes optim\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2714bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora', split=\"public\")\n",
    "dataset.transform = T.NormalizeFeatures()\n",
    "\n",
    "X =  dataset[0].x \n",
    "y =  dataset[0].y \n",
    "\n",
    "test =  dataset[0].test_mask\n",
    "train = dataset[0].train_mask \n",
    "val =  dataset[0].val_mask\n",
    "\n",
    "edge_index = dataset[0].edge_index \n",
    "edge_index = add_self_loops(edge_index)[0]\n",
    "\n",
    "\n",
    "clf_1 = XGBClassifier( tree_method='hist',\n",
    "                      device=\"cuda\",\n",
    "                           n_estimators=1100,\n",
    "                           max_depth=2,\n",
    "                    random_state=42,\n",
    "                    eta=0.3,\n",
    "                    reg_lambda=0.001,\n",
    "                           min_child_weight = 1,\n",
    "                           max_delta_step= 3,\n",
    "                           sampling_method= \"uniform\")\n",
    "   \n",
    "clf_2 = XGBClassifier( tree_method='hist',\n",
    "                      device=\"cuda\",\n",
    "                           n_estimators=900,\n",
    "                           max_depth=2,\n",
    "                       random_state=42,\n",
    "                       reg_lambda=0.2953684210526316,\n",
    "                       eta=0.2733333333333333,\n",
    "                           min_child_weight = 2,\n",
    "                           max_delta_step= 4,\n",
    "                           sampling_method= \"uniform\",\n",
    "                      subsample=0.5)\n",
    "\n",
    "clf_3 = SVC(probability=True, C=100.0, kernel=\"linear\", degree=1)\n",
    "\n",
    "def user_function(kwargs):\n",
    "    return  kwargs[\"updated_features\"] + kwargs[\"summed_neighbors\"]\n",
    "\n",
    "user_functions = [user_function, user_function,  user_function]\n",
    "clfs = [clf_1, clf_2, clf_3]\n",
    "hops_list = [0, 3,  8]\n",
    "attention_configs = [ {'inter_layer_normalize': True,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.01,\n",
    "                     'dropout_attn': None}, \n",
    "                     {'inter_layer_normalize': True,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.01,\n",
    "                     'dropout_attn': None},\n",
    "                     {'inter_layer_normalize': True,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.001,\n",
    "                     'dropout_attn': None}\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef909b52-bba6-4a9a-a42a-45fdb9b2cca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13264])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98de673a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66339/1964427141.py:128: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:230.)\n",
      "  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce=\"add\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.73400\n",
      "[1]\tvalidation_0-mlogloss:1.63995\n",
      "[2]\tvalidation_0-mlogloss:1.54837\n",
      "[3]\tvalidation_0-mlogloss:1.48037\n",
      "[4]\tvalidation_0-mlogloss:1.45511\n",
      "[5]\tvalidation_0-mlogloss:1.42616\n",
      "[6]\tvalidation_0-mlogloss:1.40100\n",
      "[7]\tvalidation_0-mlogloss:1.37746\n",
      "[8]\tvalidation_0-mlogloss:1.37575\n",
      "[9]\tvalidation_0-mlogloss:1.36987\n",
      "[10]\tvalidation_0-mlogloss:1.36355\n",
      "[11]\tvalidation_0-mlogloss:1.35918\n",
      "[12]\tvalidation_0-mlogloss:1.34453\n",
      "[13]\tvalidation_0-mlogloss:1.34202\n",
      "[14]\tvalidation_0-mlogloss:1.33372\n",
      "[15]\tvalidation_0-mlogloss:1.33217\n",
      "[16]\tvalidation_0-mlogloss:1.32909\n",
      "[17]\tvalidation_0-mlogloss:1.32382\n",
      "[18]\tvalidation_0-mlogloss:1.32752\n",
      "[19]\tvalidation_0-mlogloss:1.32067\n",
      "[20]\tvalidation_0-mlogloss:1.31594\n",
      "[21]\tvalidation_0-mlogloss:1.32297\n",
      "[22]\tvalidation_0-mlogloss:1.32562\n",
      "[23]\tvalidation_0-mlogloss:1.32744\n",
      "[24]\tvalidation_0-mlogloss:1.32578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.66496\n",
      "[1]\tvalidation_0-mlogloss:1.47006\n",
      "[2]\tvalidation_0-mlogloss:1.35592\n",
      "[3]\tvalidation_0-mlogloss:1.25656\n",
      "[4]\tvalidation_0-mlogloss:1.16575\n",
      "[5]\tvalidation_0-mlogloss:1.10393\n",
      "[6]\tvalidation_0-mlogloss:1.04138\n",
      "[7]\tvalidation_0-mlogloss:0.99219\n",
      "[8]\tvalidation_0-mlogloss:0.94844\n",
      "[9]\tvalidation_0-mlogloss:0.89773\n",
      "[10]\tvalidation_0-mlogloss:0.87204\n",
      "[11]\tvalidation_0-mlogloss:0.85190\n",
      "[12]\tvalidation_0-mlogloss:0.82722\n",
      "[13]\tvalidation_0-mlogloss:0.82055\n",
      "[14]\tvalidation_0-mlogloss:0.80748\n",
      "[15]\tvalidation_0-mlogloss:0.79850\n",
      "[16]\tvalidation_0-mlogloss:0.79455\n",
      "[17]\tvalidation_0-mlogloss:0.79733\n",
      "[18]\tvalidation_0-mlogloss:0.78743\n",
      "[19]\tvalidation_0-mlogloss:0.78669\n",
      "[20]\tvalidation_0-mlogloss:0.76802\n",
      "[21]\tvalidation_0-mlogloss:0.76707\n",
      "[22]\tvalidation_0-mlogloss:0.76692\n",
      "[23]\tvalidation_0-mlogloss:0.76274\n",
      "[24]\tvalidation_0-mlogloss:0.76288\n",
      "[25]\tvalidation_0-mlogloss:0.75665\n",
      "[26]\tvalidation_0-mlogloss:0.75304\n",
      "[27]\tvalidation_0-mlogloss:0.74388\n",
      "[28]\tvalidation_0-mlogloss:0.73596\n",
      "[29]\tvalidation_0-mlogloss:0.73232\n",
      "[30]\tvalidation_0-mlogloss:0.73394\n",
      "[31]\tvalidation_0-mlogloss:0.73615\n",
      "[32]\tvalidation_0-mlogloss:0.73575\n",
      "[33]\tvalidation_0-mlogloss:0.73355\n",
      "0.9803621768951416\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "framework = Framework(user_functions, \n",
    "                     hops_list=hops_list, ## to obtain best for local neighborhood\n",
    "                     clfs=clfs,\n",
    "                     gpu_idx=0,\n",
    "                     handle_nan=0.0,\n",
    "                    attention_configs=attention_configs)\n",
    "val_0, val_3, val_8 = framework.get_features(X, edge_index,val)\n",
    "val_0, val_3, val_8 = val_0.cpu(), val_3.cpu(), val_8.cpu()\n",
    "kwargs_list=[{\"eval_set\":[(val_0, y[val])], \"early_stopping_rounds\":5}, {\"eval_set\":[(val_3, y[val])], \"early_stopping_rounds\":5}, {}]\n",
    "framework.fit(X, edge_index, y, train, kwargs_list)\n",
    "print(time.time() - start )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91737190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:35:48] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81\n",
      "0.835\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "pred = framework.predict(X, edge_index, test) \n",
    "pred_val = framework.predict(X, edge_index, val) \n",
    "y_test = y[test]\n",
    "y_val = y[val]\n",
    "print(accuracy_score(y_val, pred_val))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "756d080c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846a2759dbde40c2bdee9afd98b32565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a68502e1c48447196a03cf0e231d366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ded63d49ce46488d816a41ec1e31b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc69ce315ade4dc19a4e4f02363d1c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f40c2c48c49409da37c3b30903a4d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec7b325e4364e80860f694fa9eab81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "max_val = 0\n",
    "max_test = 0\n",
    "best_weights = None\n",
    "for weight_0 in tqdm(np.linspace(0,1, 5)):\n",
    "    for weight_1 in tqdm(np.linspace(0,1,5)):\n",
    "        pred = framework.predict(X, edge_index, test, weights=[weight_0, weight_1, 1-weight_0-weight_1]) \n",
    "        pred_val = framework.predict(X, edge_index, val, weights=[weight_0, weight_1, 1-weight_0-weight_1]) \n",
    "        y_test = y[test]\n",
    "        y_val = y[val]\n",
    "        acc_val = accuracy_score(y_val, pred_val)\n",
    "        acc_test = accuracy_score(y_test, pred)\n",
    "        \n",
    "        if acc_val >= max_val:\n",
    "            max_val = acc_val\n",
    "            max_test = acc_test\n",
    "            best_weights = [weight_0, weight_1, 1-weight_0-weight_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d8faabb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814\n",
      "0.83\n",
      "[0.25, 0.25, 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(max_val)\n",
    "print(max_test)\n",
    "print(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c95d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
