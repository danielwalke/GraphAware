{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9325db63",
   "metadata": {},
   "source": [
    "## SBC Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2938ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "abc84a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/git/graph_aware_ml/dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n",
      "/home/dwalke/git/graph_aware_ml/dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n",
      "/home/dwalke/git/graph_aware_ml/dataAnalysis/data/Filter.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Label'] = self.data['Diagnosis']\n"
     ]
    }
   ],
   "source": [
    "from dataAnalysis.DataAnalysis import DataAnalysis\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"./sbcdata.csv\", header=0)\n",
    "data_analysis = DataAnalysis(data, mimic_data = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3d882efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat((data_analysis.get_training_data(), data_analysis.get_testing_data()))\n",
    "max_Id = data[\"Id\"].unique().max()\n",
    "gw_data = data_analysis.get_gw_testing_data().copy(deep=True)\n",
    "gw_data = gw_data.assign(Id=lambda x: x.Id + max_Id)\n",
    "data = pd.concat((data, gw_data))\n",
    "data = data.sort_values([\"Id\", \"Time\"])\n",
    "data = data.reset_index(drop=True)\n",
    "popped_index = data.pop(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bbfa5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataAnalysis.Constants import SEX_CATEGORY_COLUMN_NAME, SEX_COLUMN_NAME, FEATURES\n",
    "data[SEX_CATEGORY_COLUMN_NAME] = data.loc[:, SEX_COLUMN_NAME] ==\"W\"\n",
    "\n",
    "data[SEX_CATEGORY_COLUMN_NAME] = data[SEX_CATEGORY_COLUMN_NAME].astype(\"int8\")\n",
    "data[\"Label\"] = data[\"Label\"] == \"Sepsis\"\n",
    "data[\"Label\"] = data[\"Label\"].astype(\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a1e4e",
   "metadata": {},
   "source": [
    "## Edge index construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "52567628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "def get_edge_index(dataset):\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "    source_edge_index = []\n",
    "    target_edge_index = []\n",
    "\n",
    "    for Id, group in dataset.groupby(\"Id\"):\n",
    "        indices = group.index\n",
    "        offset = indices[0]\n",
    "        num_nodes = len(indices)\n",
    "        edge_index = torch.zeros((2, sum(range(num_nodes + 1))), dtype=torch.long)+offset\n",
    "\n",
    "        ## Self edges\n",
    "        edge_index[:, 0:num_nodes] = (torch.arange(num_nodes) + offset).view(1, -1)\n",
    "        idx = num_nodes\n",
    "        for i in range(1, num_nodes):\n",
    "            edge_index[1, idx:idx + i] = i+offset\n",
    "            edge_index[0, idx:idx + i] = torch.arange(i)+offset\n",
    "            idx += i\n",
    "\n",
    "        source_edge_index.extend(edge_index[0, :].numpy().tolist())\n",
    "        target_edge_index.extend(edge_index[1, :].numpy().tolist())\n",
    "\n",
    "    edge_index = np.asarray([np.asarray(source_edge_index), np.asarray(target_edge_index)])\n",
    "    edge_index = torch.tensor(edge_index)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edge_index = get_edge_index(data[(data[\"Set\"] == \"Validation\") & (data[\"Center\"] == \"Leipzig\")])\n",
    "test_gw_edge_index = get_edge_index(data[(data[\"Set\"] == \"Validation\") & (data[\"Center\"] == \"Greifswald\")])\n",
    "train_edge_index = get_edge_index(data[data[\"Set\"] == \"Training\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beddbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataAnalysis.Constants import FEATURES, LABEL_COLUMN_NAME\n",
    "\n",
    "train_mask = data[\"Set\"] == \"Training\"\n",
    "X_train = data.loc[train_mask, FEATURES].values\n",
    "y_train = data.loc[train_mask, LABEL_COLUMN_NAME].values\n",
    "\n",
    "test_mask = (data[\"Set\"] == \"Validation\") & (data[\"Center\"] == \"Leipzig\")\n",
    "X_test = data.loc[test_mask, FEATURES].values\n",
    "y_test = data.loc[test_mask, LABEL_COLUMN_NAME].values\n",
    "\n",
    "test_gw_mask = (data[\"Set\"] == \"Validation\") & (data[\"Center\"] == \"Greifswald\")\n",
    "X_test_gw = data.loc[test_gw_mask, FEATURES].values\n",
    "y_test_gw = data.loc[test_gw_mask, LABEL_COLUMN_NAME].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_edge_index(edge_index):\n",
    "    rev_edge_index = torch.zeros_like(edge_index)\n",
    "    index = torch.LongTensor([1,0])\n",
    "    rev_edge_index[index] = edge_index\n",
    "    return rev_edge_index\n",
    "\n",
    "rev_train_edge_index = reverse_edge_index(train_edge_index)\n",
    "rev_test_edge_index = reverse_edge_index(test_edge_index)\n",
    "rev_test_gw_edge_index = reverse_edge_index(test_gw_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e45058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_undirected\n",
    "undir_train_edge_index = to_undirected(train_edge_index)\n",
    "undir_test_edge_index = to_undirected(test_edge_index)\n",
    "undir_test_gw_edge_index = to_undirected(test_gw_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def user_function(origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors):\n",
    "    return updated_features - sum_neighbors / num_neighbors\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = random_forest = RandomForestClassifier(class_weight={0: 0.0025, 1: 1}, max_leaf_nodes=79,\n",
    "                                             min_samples_leaf=0.0001,\n",
    "                                             min_samples_split=0.0055,\n",
    "                                             n_estimators=700, random_state=42, n_jobs=-1)\n",
    "# XGBClassifier(tree_method='gpu_hist', \n",
    "#                            scale_pos_weight = 70,\n",
    "#                            n_estimators=1000,\n",
    "#                            max_depth=2,\n",
    "#                            eta=0.1,\n",
    "#                            min_child_weight = 7,\n",
    "#                            max_delta_step= 7,\n",
    "#                            sampling_method= \"uniform\")\n",
    "\n",
    "clf_nh = random_forest = RandomForestClassifier(class_weight={0: 0.0025, 1: 1}, max_leaf_nodes=79,\n",
    "                                                min_samples_leaf=0.0001,\n",
    "                                                min_samples_split=0.0055,\n",
    "                                                n_estimators=500, random_state=42, n_jobs=-1)\n",
    "\n",
    "bst = XGBClassifier( tree_method='gpu_hist', \n",
    "                    n_estimators=1000,\n",
    "                    max_depth=2,\n",
    "                    eta=0.1,\n",
    "                    min_child_weight = 7,\n",
    "                    max_delta_step= 7,\n",
    "                    sampling_method= \"uniform\")\n",
    "\n",
    "bst_nh = XGBClassifier( tree_method='gpu_hist', \n",
    "                       n_estimators=1000,\n",
    "                       max_depth=2,\n",
    "                       eta=0.1,\n",
    "                       min_child_weight = 7,\n",
    "                       max_delta_step= 7,\n",
    "                       sampling_method= \"uniform\")\n",
    "feature_based_aggregation:Feature_based_aggregation = {\n",
    "    \"concat\": True,\n",
    "    \"combined_clf\": clf\n",
    "}\n",
    "\n",
    "classifier_based_aggregation:Classifier_based_aggregation = {\n",
    "    \"clf_list\": [clf, clf_nh]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bcd9402",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Framework.__init__() got an unexpected keyword argument 'eps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m framework \u001b[38;5;241m=\u001b[39m \u001b[43mFramework\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiff_of_updated_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_feature_based_aggregation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m          \u001b[49m\u001b[43mfeature_based_aggregation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature_based_aggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclassifier_based_aggregation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclassifier_based_aggregation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle_nan\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_pseudo_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m framework\u001b[38;5;241m.\u001b[39mfit(X_train, rev_train_edge_index, y_train, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Framework.__init__() got an unexpected keyword argument 'eps'"
     ]
    }
   ],
   "source": [
    "framework = Framework(\"diff_of_updated_mean\", hops=1, use_feature_based_aggregation = True,\n",
    "          feature_based_aggregation = feature_based_aggregation,\n",
    "            classifier_based_aggregation = classifier_based_aggregation, gpu_idx = 1, handle_nan = 0, normalize=False, use_pseudo_attention=False, eps = None)\n",
    "\n",
    "framework.fit(X_train, rev_train_edge_index, y_train, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b3d27cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.942675</td>\n",
       "      <td>0.045355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GW</th>\n",
       "      <td>0.952914</td>\n",
       "      <td>0.029799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AUROC     AUPRC\n",
       "L   0.942675  0.045355\n",
       "GW  0.952914  0.029799"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 5/6] START n_estimators=600............................................\n",
      "[CV 1/3; 5/6] END n_estimators=600; accuracy: (train=0.935, test=0.935) total time= 4.3min\n",
      "[CV 2/3; 5/6] START n_estimators=600............................................\n",
      "[CV 2/3; 5/6] END n_estimators=600; accuracy: (train=0.935, test=0.935) total time= 4.3min\n",
      "[CV 3/3; 5/6] START n_estimators=600............................................\n",
      "[CV 3/3; 5/6] END n_estimators=600; accuracy: (train=0.932, test=0.932) total time= 4.3min\n",
      "[CV 1/3; 6/6] START n_estimators=700............................................\n",
      "[CV 1/3; 6/6] END n_estimators=700; accuracy: (train=0.935, test=0.935) total time= 4.3min\n",
      "[CV 2/3; 6/6] START n_estimators=700............................................\n",
      "[CV 2/3; 6/6] END n_estimators=700; accuracy: (train=0.936, test=0.935) total time= 4.3min\n",
      "[CV 3/3; 6/6] START n_estimators=700............................................\n",
      "[CV 3/3; 6/6] END n_estimators=700; accuracy: (train=0.932, test=0.932) total time= 4.3min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score,average_precision_score\n",
    "y_score = framework.predict_proba(X_test, rev_test_edge_index, None)[:,1]\n",
    "y_score_gw = framework.predict_proba(X_test_gw, rev_test_gw_edge_index, None)[:,1]\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_score)\n",
    "auroc_gw = roc_auc_score(y_test_gw, y_score_gw)\n",
    "\n",
    "auc_precision_recall = average_precision_score(y_test, y_score)\n",
    "auc_precision_recall_gw = average_precision_score(y_test_gw, y_score_gw)\n",
    "\n",
    "pd.DataFrame([[auroc, auc_precision_recall], [auroc_gw, auc_precision_recall_gw]], columns=[\"AUROC\", \"AUPRC\"], index=[\"L\", \"GW\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe9ce88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:184: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return array[key] if axis == 0 else array[:, key]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight={0: 0.0025, 1: 1}, max_leaf_nodes=79,\n",
       "                       min_samples_leaf=0.0001, min_samples_split=0.0055,\n",
       "                       n_estimators=700, n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight={0: 0.0025, 1: 1}, max_leaf_nodes=79,\n",
       "                       min_samples_leaf=0.0001, min_samples_split=0.0055,\n",
       "                       n_estimators=700, n_jobs=-1, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight={0: 0.0025, 1: 1}, max_leaf_nodes=79,\n",
       "                       min_samples_leaf=0.0001, min_samples_split=0.0055,\n",
       "                       n_estimators=700, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 1/6] START n_estimators=200............................................\n",
      "[CV 2/3; 1/6] END n_estimators=200; accuracy: (train=0.935, test=0.935) total time= 2.3min\n",
      "[CV 3/3; 1/6] START n_estimators=200............................................\n",
      "[CV 3/3; 1/6] END n_estimators=200; accuracy: (train=0.931, test=0.931) total time= 2.4min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def get_best_estimator(model, param_grid):\n",
    "    grid = GridSearchCV(\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    estimator=model,\n",
    "    scoring=['accuracy'],\n",
    "    refit=\"accuracy\",\n",
    "    return_train_score=True,\n",
    "    param_grid=param_grid,\n",
    "    verbose= 10\n",
    "    )\n",
    "    grid.fit(X_train, rev_train_edge_index, y_train, None)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "param_grid={\n",
    "        'n_estimators': range(200, 800, 100),\n",
    "}\n",
    "framework.grid_search(X_train, rev_train_edge_index, y_train, None, [param_grid], n_jobs=-1,\n",
    "    cv=3,\n",
    "    scoring=['accuracy'],\n",
    "    refit=\"accuracy\",\n",
    "    return_train_score=True,\n",
    "    verbose= 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9a164",
   "metadata": {},
   "source": [
    "## Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e1e6c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.base import BaseEstimator\n",
    "from typing import TypedDict\n",
    "import numpy \n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class Feature_based_aggregation(TypedDict):\n",
    "    \"\"\"\n",
    "    A class for giving type hints for the dict feature_based_aggregation.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    concat:bool\n",
    "        A boolean whether to concat or add target node features and neighborhood features\n",
    "    combined_clf: BaseEstimator\n",
    "        A classifier (combined_clf) from sklearn applied to the aggregated features \n",
    "        (target node features and neighborhood features)\n",
    "    \"\"\"\n",
    "    combined_clf: BaseEstimator\n",
    "    concat: bool\n",
    "    n_estimators: int ## rather list of classifiers??\n",
    "        \n",
    "class Classifier_based_aggregation(TypedDict):\n",
    "    \"\"\"\n",
    "    A class for giving type hints for the dict classifier_based_aggregation.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    clf_list: tuple[BaseEstimator, BaseEstimator]\n",
    "        A tuple classifiers from sklearn - one applied to the target node features \n",
    "        and the other one to the neighborhood features\n",
    "    \"\"\"\n",
    "    clf_list: tuple[BaseEstimator, BaseEstimator]\n",
    "    n_estimators: int ## other data structure?,\n",
    "    weights: tuple[float, float]\n",
    "    #weight: float weights influence of self-awareness to neighborhood-awareness\n",
    "\n",
    "USER_FUNCTIONS = {\n",
    "    'sum': lambda origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors, hop: sum_neighbors,\n",
    "    'mean': lambda origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors, hop: sum_neighbors / num_neighbors,\n",
    "    'diff_of_origin_mean': lambda origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors, hop: origin_features - sum_neighbors / num_neighbors,\n",
    "    'diff_of_updated_mean': lambda origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors, hop: updated_features - sum_neighbors / num_neighbors,\n",
    "    'sum_of_origin_mean': lambda origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors, hop: origin_features + sum_neighbors / num_neighbors,\n",
    "    'sum_of_updated_mean': lambda origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors, hop: updated_features + sum_neighbors / num_neighbors,\n",
    "}\n",
    "\n",
    "## Verbesserungsvorschläge? -> besonders um Acc zu verbessern\n",
    "## heterogene Graphen?\n",
    "##TODO More input_validation, grid search method whoch accepts the same params\n",
    "class Framework:\n",
    "    \"\"\"\n",
    "    A class for giving Machine learning algorithms additional graph-awareness.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    user_function:def|str\n",
    "        a user-defined function applied to the aggregation of neighboring nodes or a string from USER_FUNCTIONS\n",
    "    hops:int\n",
    "        number of hops to aggregate\n",
    "    use_feature_based_aggregation:bool\n",
    "        Boolean whether to use feature-based aggregation or classifier-based aggregation\n",
    "    feature_based_aggregation:Feature_based_aggregation\n",
    "        dictionary containing a classifier (combined_clf) applied to the aggregated features\n",
    "        and a boolean (concat) indicating whether to concat or add up features from ttarget-nodes and it's neighbors\n",
    "    classifier_based_aggregation:Classifier_based_aggregation\n",
    "        dictionary containing a tuple of classifiers (clf_list), one applied to the features of the target node\n",
    "        and the other one to the features of the aggregated features of the neighbors\n",
    "    gpu_idx:int|None=None\n",
    "        Optional index parameter for using GPU device (None or an integer like 0 depending the GPU index)\n",
    "    handle_nan:float|None|int=None\n",
    "        Optional parameter whether to handle nan values after applying the user-defined function (e.g., dividing by 0)\n",
    "        Replaces all nan values with the given value\n",
    "    normalize:bool=True (BETA)\n",
    "        Optional boolean whether or not to normalize features? \n",
    "    use_pseudo_attention = True (BETA)\n",
    "        Optional boolean whether or not to apply a pseudo attention mechanism \n",
    "        (weighted aggregation based on cosine-similarity)\n",
    "    device:torch.DeviceObjType\n",
    "        Torch device for computation (cpu) or gpu if available and requested\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    make_tensors()\n",
    "        Checks correct type and format of input features and edge_index\n",
    "        And transfers possible numby arrays to torch tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    ## min max aggregation might also work as future idea (https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html) -> in Beta\n",
    "\n",
    "    def __init__(self, user_function, \n",
    "                 hops:int,\n",
    "                 use_feature_based_aggregation:bool,\n",
    "                 feature_based_aggregation:Feature_based_aggregation,\n",
    "                 classifier_based_aggregation:Classifier_based_aggregation,\n",
    "                 gpu_idx:int|None=None,\n",
    "                 handle_nan:float|None=None,\n",
    "                 normalize:bool=False,\n",
    "                 use_pseudo_attention = False,\n",
    "                 cosine_eps:float|None=None,\n",
    "                dropout_attn:float|None = None) -> None:\n",
    "        self.user_function = user_function\n",
    "        self.hops:int = hops\n",
    "        self.use_feature_based_aggregation:bool = use_feature_based_aggregation\n",
    "        self.feature_based_aggregation:Feature_based_aggregation = feature_based_aggregation\n",
    "        self.classifier_based_aggregation:Classifier_based_aggregation = classifier_based_aggregation\n",
    "        self.gpu_idx:int|None = gpu_idx\n",
    "        self.handle_nan:float|int|None = handle_nan\n",
    "        self.normalize:bool = normalize\n",
    "        self.use_pseudo_attention:bool = use_pseudo_attention\n",
    "        self.cosine_eps:float|None = cosine_eps\n",
    "        self.dropout_attn:float|None = dropout_attn\n",
    "        self.device:torch.DeviceObjType = torch.device(f\"cuda:{str(self.gpu_idx)}\") if self.gpu_idx is not None and torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    \n",
    "    def update_user_function(self):\n",
    "        if self.user_function in USER_FUNCTIONS:\n",
    "            self.user_function = USER_FUNCTIONS[self.user_function]\n",
    "        else:\n",
    "            raise Exception(f\"Only the following string values are valid inputs for the user function: {[key for key in USER_FUNCTIONS]}. You can also specify your own function for aggregatioon.\")\n",
    "    def get_features(self,\n",
    "                     X:torch.FloatTensor|numpy._typing.NDArray,\n",
    "                     edge_index:torch.LongTensor|numpy._typing.NDArray,\n",
    "                     mask:torch.BoolTensor|numpy._typing.NDArray,\n",
    "                    is_training:bool = False) -> tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        if mask is None:\n",
    "            mask = torch.ones(X.shape[0]).type(torch.bool)\n",
    "        if isinstance(self.user_function, str):\n",
    "            self.update_user_function()\n",
    "        ## To tensor\n",
    "        X = Framework.get_feature_tensor(X)\n",
    "        edge_index = Framework.get_edge_index_tensor(edge_index)\n",
    "        mask = Framework.get_mask_tensor(mask)\n",
    "        \n",
    "        ## To device\n",
    "        X = self.shift_tensor_to_device(X)\n",
    "        edge_index = self.shift_tensor_to_device(edge_index)\n",
    "        mask = self.shift_tensor_to_device(mask)\n",
    "        \n",
    "        if self.hops <= 0:\n",
    "            return (X[mask], None)\n",
    "        \n",
    "        ## Aggregate\n",
    "        neighbor_features = self.aggregate(X, edge_index, is_training)\n",
    "        \n",
    "        return (X[mask], neighbor_features[mask])\n",
    "    \n",
    "    def fit(self,\n",
    "            X_train:torch.FloatTensor|numpy._typing.NDArray,\n",
    "            edge_index:torch.LongTensor|numpy._typing.NDArray,\n",
    "            y_train:torch.LongTensor|numpy._typing.NDArray,\n",
    "            train_mask:torch.BoolTensor|numpy._typing.NDArray|None,\n",
    "            kwargs_list = None\n",
    "            ) -> BaseEstimator:   \n",
    "        if train_mask is None:\n",
    "            train_mask = torch.ones(X_train.shape[0]).type(torch.bool)\n",
    "        y_train = Framework.get_label_tensor(y_train)\n",
    "        y_train = y_train[train_mask]\n",
    "        \n",
    "        self.validate_input()\n",
    "        \n",
    "        X_train, neighbors = self.get_features(X_train, edge_index, train_mask, True)        \n",
    "        \n",
    "        if self.use_feature_based_aggregation and self.feature_based_aggregation is not None:\n",
    "            concat:bool = self.feature_based_aggregation[\"concat\"]\n",
    "            clf:BaseEstimator = self.feature_based_aggregation[\"combined_clf\"]\n",
    "            out_clfs:list[BaseEstimator] = []\n",
    "            if neighbors is not None:\n",
    "                X_train:torch.FloatTensor = torch.concat((X_train, neighbors), dim = 1) if concat else torch.add(X_train, neighbors)\n",
    "            for i in range(self.feature_based_aggregation[\"n_estimators\"]):\n",
    "                clf_clone = clone(clf)\n",
    "                if kwargs_list and len(kwargs_list) == 1:\n",
    "                    clf_clone.fit(X_train.cpu().numpy(), y_train,**kwargs_list[0])\n",
    "                else:\n",
    "                    clf_clone.fit(X_train.cpu().numpy(), y_train)\n",
    "                out_clfs.append(clf_clone)\n",
    "            self.feature_based_aggregation[\"trained_clfs\"] = out_clfs\n",
    "            return self.feature_based_aggregation[\"trained_clfs\"][0] if len(self.feature_based_aggregation[\"trained_clfs\"]) == 1 else self.feature_based_aggregation[\"trained_clf\"]\n",
    "        \n",
    "        if not self.use_feature_based_aggregation and self.classifier_based_aggregation is not None:\n",
    "            clf_list:list[BaseEstimator] = self.classifier_based_aggregation[\"clf_list\"]\n",
    "            out_clfs:list[tuple[BaseEstimator]] = []\n",
    "            for i in range(self.classifier_based_aggregation[\"n_estimators\"]):\n",
    "                clf_0_clone = clone(clf_list[0])\n",
    "                clf_1_clone = clone(clf_list[1])          \n",
    "                if kwargs_list and len(kwargs_list) >= 1:\n",
    "                    clf_0_clone.fit(X_train.cpu().numpy(), y_train,**kwargs_list[0])\n",
    "                else:\n",
    "                    clf_0_clone.fit(X_train.cpu().numpy(), y_train)\n",
    "                if neighbors is not None:\n",
    "                    if kwargs_list and len(kwargs_list) >= 1:\n",
    "                        clf_1_clone.fit(neighbors.cpu().numpy(), y_train,**kwargs_list[1])\n",
    "                    else:\n",
    "                        clf_1_clone.fit(neighbors.cpu().numpy(), y_train)\n",
    "                out_clfs.append((clf_0_clone, clf_1_clone))\n",
    "            self.classifier_based_aggregation[\"trained_clfs\"] = out_clfs\n",
    "            return self.classifier_based_aggregation[\"trained_clfs\"][0] if len(self.classifier_based_aggregation[\"trained_clfs\"]) == 1 else self.classifier_based_aggregation[\"trained_clfs\"]\n",
    "    \n",
    "    def predict_proba(self, X_test:torch.FloatTensor|numpy._typing.NDArray,\n",
    "                      edge_index:torch.LongTensor|numpy._typing.NDArray,\n",
    "                      test_mask:torch.BoolTensor|numpy._typing.NDArray|None,\n",
    "                     **kwargs):  \n",
    "        if test_mask is None:\n",
    "            test_mask = torch.ones(X_test.shape[0]).type(torch.bool)\n",
    "        X_test, neighbors = self.get_features(X_test, edge_index, test_mask)\n",
    "        \n",
    "        if self.use_feature_based_aggregation and self.feature_based_aggregation is not None:\n",
    "            concat:bool = self.feature_based_aggregation[\"concat\"]\n",
    "#             clf:BaseEstimator = self.feature_based_aggregation[\"combined_clf\"]\n",
    "            if neighbors is not None:\n",
    "                X_test:torch.FloatTensor = torch.concat((X_test, neighbors), dim = 1) if concat else torch.add(X_test, neighbors)\n",
    "            pred_proba = []\n",
    "            for clf in self.feature_based_aggregation[\"trained_clfs\"]:\n",
    "                pred_proba.append(clf.predict_proba(X_test.cpu().numpy(),**kwargs))\n",
    "            return np.mean(np.array(pred_proba), axis=0)\n",
    "        \n",
    "        if not self.use_feature_based_aggregation and self.classifier_based_aggregation is not None:\n",
    "#             clf_list:list[BaseEstimator] = self.classifier_based_aggregation[\"clf_list\"]\n",
    "            pred_proba = []\n",
    "            for clf_list in self.classifier_based_aggregation[\"trained_clfs\"]:\n",
    "                pred_proba_0 = clf_list[0].predict_proba(X_test.cpu().numpy(),**kwargs) \n",
    "                if \"weights\" not in classifier_based_aggregation or len(classifier_based_aggregation[\"weights\"]) != 2:\n",
    "                    classifier_based_aggregation[\"weights\"] = (.5, .5)\n",
    "                pred_proba.append(classifier_based_aggregation[\"weights\"][0] * pred_proba_0)\n",
    "                if neighbors is not None:\n",
    "                    pred_proba_1 = clf_list[1].predict_proba(neighbors.cpu().numpy(), **kwargs)\n",
    "                    pred_proba.append(classifier_based_aggregation[\"weights\"][1] * pred_proba_1)     \n",
    "            return np.mean(np.array(pred_proba), axis=0)\n",
    "        \n",
    "    \n",
    "    def predict(self,\n",
    "                X_test:torch.FloatTensor|numpy._typing.NDArray,\n",
    "                edge_index:torch.LongTensor|numpy._typing.NDArray,\n",
    "                test_mask:torch.BoolTensor|numpy._typing.NDArray|None, **kwargs):\n",
    "        return self.predict_proba(X_test, edge_index, test_mask, **kwargs).argmax(1)\n",
    "        \n",
    "\n",
    "    def validate_input(self):\n",
    "        pass\n",
    "            \n",
    "    @staticmethod\n",
    "    def get_feature_tensor(X:torch.FloatTensor|numpy._typing.NDArray) -> torch.FloatTensor|None:\n",
    "        if not torch.is_tensor(X):\n",
    "            try:\n",
    "                return torch.from_numpy(X).type(torch.float)\n",
    "            except:\n",
    "                raise Exception(\"Features input X must be numpy array or torch tensor!\")\n",
    "                return None \n",
    "        return X\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_label_tensor(y:torch.LongTensor|numpy._typing.NDArray) -> torch.LongTensor|None:\n",
    "        if not torch.is_tensor(y):\n",
    "            try:\n",
    "                return torch.from_numpy(y).type(torch.long)\n",
    "            except:\n",
    "                raise Exception(\"Label input y must be numpy array or torch tensor!\")\n",
    "                return None\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_mask_tensor(mask:torch.BoolTensor|numpy._typing.NDArray) -> torch.BoolTensor|None:\n",
    "        if not torch.is_tensor(mask):\n",
    "            try:\n",
    "                return torch.from_numpy(mask).type(torch.bool)\n",
    "            except:\n",
    "                raise Exception(\"Input mask must be numpy array or torch tensor!\")\n",
    "                return None\n",
    "        return mask\n",
    "            \n",
    "    @staticmethod\n",
    "    def get_edge_index_tensor(edge_index:torch.LongTensor|numpy._typing.NDArray) -> torch.LongTensor|None:\n",
    "        if not torch.is_tensor(edge_index):\n",
    "            try:\n",
    "                edge_index =  torch.from_numpy(edge_index).type(torch.long)\n",
    "                Framework.validate_edge_index(edge_index)\n",
    "                return edge_index\n",
    "            except:\n",
    "                raise Exception(\"Edge index must be numpy array or torch tensor\")\n",
    "                return None\n",
    "        return edge_index\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_edge_index(edge_index:torch.LongTensor) -> None:\n",
    "        if edge_index.shape[0] != 2:\n",
    "            raise Exception(\"Edge index must have the shape 2 x NumberOfEdges\")\n",
    "            # TODO: check max edge index and shape of features\n",
    "            \n",
    "    def apply_attention_mechanism(self, source_lift:torch.FloatTensor,\n",
    "                                  features_for_aggregation:torch.FloatTensor,\n",
    "                                  target:torch.LongTensor,\n",
    "                                 is_training:bool = False) -> torch.FloatTensor:\n",
    "        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        score = cos(source_lift, features_for_aggregation.index_select(0, target))\n",
    "        dropout_tens = None\n",
    "        \n",
    "        origin_scores = torch.clone(score)\n",
    "        if self.cosine_eps:\n",
    "            score[score < self.cosine_eps] = -torch.inf\n",
    "        if self.dropout_attn is not None and is_training:\n",
    "            dropout_tens = torch.FloatTensor(score.shape[0]).uniform_(0, 1)\n",
    "            score[dropout_tens < self.dropout_attn] = -torch.inf\n",
    "            print(origin_scores[dropout_tens < self.dropout_attn])\n",
    "        exp_score = torch.exp(score)\n",
    "        summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce=\"add\")\n",
    "        target_lifted_summed_exp_score = summed_exp_score.index_select(0, target)\n",
    "        normalized_scores = exp_score / target_lifted_summed_exp_score\n",
    "        source_lift = normalized_scores.unsqueeze(1) * source_lift\n",
    "        return source_lift\n",
    "    \n",
    "    def shift_tensor_to_device(self,\n",
    "                               t:torch.FloatTensor) -> torch.FloatTensor:\n",
    "        if self.gpu_idx is not None:\n",
    "            return t.to(self.device) \n",
    "        return t\n",
    "    \n",
    "    \n",
    "    def aggregate(self, X:torch.FloatTensor, edge_index:torch.LongTensor, is_training:bool=False) -> torch.FloatTensor: \n",
    "        features_for_aggregation:torch.FloatTensor = torch.clone(X)\n",
    "        for i in range(self.hops):\n",
    "            if self.normalize:\n",
    "                features_for_aggregation = torch.nn.functional.normalize(features_for_aggregation, dim = 0)\n",
    "            source_lift = features_for_aggregation.index_select(0, edge_index[0])\n",
    "            target = edge_index[1]\n",
    "            \n",
    "            if self.use_pseudo_attention:\n",
    "                source_lift = self.apply_attention_mechanism(source_lift, features_for_aggregation, target, is_training)\n",
    "            \n",
    "            summed_neighbors = torch.zeros_like(features_for_aggregation, device=self.device).scatter_(0, target.unsqueeze(0).repeat(features_for_aggregation.shape[1], 1).t(), source_lift, reduce=\"add\")\n",
    "            multiplied_neighbors = torch.ones_like(features_for_aggregation, device=self.device).scatter_(0, target.unsqueeze(0).repeat(features_for_aggregation.shape[1], 1).t(), source_lift, reduce=\"multiply\")\n",
    "\n",
    "            num_source_neighbors = torch.zeros(features_for_aggregation.shape[0], dtype=torch.float, device=self.device)\n",
    "            num_source_neighbors.scatter_(0, target, torch.ones_like(target, dtype=torch.float, device=self.device), reduce=\"add\")\n",
    "            num_source_neighbors = num_source_neighbors.unsqueeze(-1)\n",
    "\n",
    "            out = self.user_function(X,features_for_aggregation,\n",
    "                                     summed_neighbors, multiplied_neighbors, num_source_neighbors, i)\n",
    "            \n",
    "            if self.handle_nan is not None:\n",
    "                out = torch.nan_to_num(out, nan=self.handle_nan)\n",
    "            features_for_aggregation = out\n",
    "        return features_for_aggregation\n",
    "    \n",
    "    def validate_grid_input(self, grid_params):\n",
    "        if len(grid_params) != 1 and self.use_feature_based_aggregation:\n",
    "            raise Exception(\"You need to provide grid parameter for the classifier!\")\n",
    "        if len(grid_params) != 2 and not self.use_feature_based_aggregation:\n",
    "            raise Exception(\"You need to provide two grid parameter, one for each classifier!\")\n",
    "        return\n",
    "    \n",
    "    def grid_search(self,\n",
    "            X_train:torch.FloatTensor|numpy._typing.NDArray,\n",
    "            edge_index:torch.LongTensor|numpy._typing.NDArray,\n",
    "            y_train:torch.LongTensor|numpy._typing.NDArray,\n",
    "            train_mask:torch.BoolTensor|numpy._typing.NDArray|None,\n",
    "            grid_params:list,\n",
    "            **grid_kwargs\n",
    "            ) -> BaseEstimator:        \n",
    "        if train_mask is None:\n",
    "            train_mask = torch.ones(X_train.shape[0]).type(torch.bool)\n",
    "        y_train = Framework.get_label_tensor(y_train)\n",
    "        y_train = y_train[train_mask]\n",
    "        \n",
    "        self.validate_grid_input(grid_params)\n",
    "        \n",
    "        X_train, neighbors = self.get_features(X_train, edge_index, train_mask)        \n",
    "        \n",
    "        if self.use_feature_based_aggregation and self.feature_based_aggregation is not None:\n",
    "            concat:bool = self.feature_based_aggregation[\"concat\"]\n",
    "            clf:BaseEstimator = self.feature_based_aggregation[\"combined_clf\"]\n",
    "            if neighbors is not None:\n",
    "                X_train:torch.FloatTensor = torch.concat((X_train, neighbors), dim = 1) if concat else torch.add(X_train, neighbors)\n",
    "            grid = GridSearchCV(\n",
    "                estimator=clf,\n",
    "                param_grid=grid_params[0],\n",
    "                **grid_kwargs\n",
    "            )\n",
    "            grid.fit(X_train.cpu().numpy(), y_train)\n",
    "            self.feature_based_aggregation[\"combined_clf\"] = grid.best_estimator_\n",
    "            return self.feature_based_aggregation[\"combined_clf\"]\n",
    "        \n",
    "        if not self.use_feature_based_aggregation and self.classifier_based_aggregation is not None:\n",
    "            clf_list:list[BaseEstimator] = self.classifier_based_aggregation[\"clf_list\"]\n",
    "            optimzed_clfs = []\n",
    "            grid_self = GridSearchCV(\n",
    "                estimator=clf_list[0],\n",
    "                param_grid=grid_params[0],\n",
    "                **grid_kwargs\n",
    "            )\n",
    "            grid_self.fit(X_train.cpu().numpy(), y_train)\n",
    "            optimzed_clfs.append(grid_self.best_estimator_)\n",
    "            if neighbors is not None:\n",
    "                grid_neighbors = GridSearchCV(\n",
    "                    estimator=clf_list[1],\n",
    "                    param_grid=grid_params[1],\n",
    "                    **grid_kwargs\n",
    "                )\n",
    "                grid_neighbors.fit(neighbors.cpu().numpy(), y_train)\n",
    "                optimzed_clfs.append(grid_neighbors.best_estimator_)\n",
    "            self.classifier_based_aggregation[\"clf_list\"] = optimzed_clfs\n",
    "            return self.classifier_based_aggregation[\"clf_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8cca695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora', split=\"public\") ## public\n",
    "dataset.transform = T.NormalizeFeatures()\n",
    "\n",
    "PSEUDO_EPOCHS = 1\n",
    "\n",
    "X =  dataset[0].x #node_features #dataset[0].x.repeat(PSEUDO_EPOCHS, 1) #node_features #dataset[0].x.repeat(PSEUDO_EPOCHS, 1)\n",
    "y =  dataset[0].y #node_labels #dataset[0].y.repeat(PSEUDO_EPOCHS) #node_labels #dataset[0].y.repeat(PSEUDO_EPOCHS)\n",
    "\n",
    "test =  dataset[0].test_mask #test_indices #dataset[0].test_mask #test_indices #dataset[0].test_mask.repeat(PSEUDO_EPOCHS)\n",
    "# test[dataset[0].y.shape[0]:] = False\n",
    "# print(test.sum())\n",
    "train = dataset[0].train_mask #train_indices #dataset[0].train_mask #train_indices #dataset[0].train_mask.repeat(PSEUDO_EPOCHS)\n",
    "val =  dataset[0].val_mask\n",
    "\n",
    "\n",
    "# X = dataset[0].x\n",
    "# y = dataset[0].y\n",
    "\n",
    "# test = dataset[0].test_mask\n",
    "# train = ~test #dataset[0].train_mask\n",
    "# val = dataset[0].val_mask\n",
    "\n",
    "# y_train = y[train]\n",
    "# y_test = y[test]\n",
    "\n",
    "edge_index = dataset[0].edge_index \n",
    "edge_index = add_self_loops(edge_index)[0]\n",
    "\n",
    "# edges_indices = []\n",
    "# for i in range(PSEUDO_EPOCHS):\n",
    "#     edges_indices.append(dataset[0].edge_index+i*dataset[0].x.shape[0])\n",
    "# edge_index = torch.cat(edges_indices, 1)\n",
    "\n",
    "bst = XGBClassifier( tree_method='gpu_hist', \n",
    "                           n_estimators=1100,\n",
    "                           max_depth=2,\n",
    "                    random_state=42,\n",
    "                    eta=0.3,\n",
    "                    reg_lambda=0.001,\n",
    "                           min_child_weight = 1,\n",
    "                           max_delta_step= 3,\n",
    "                           sampling_method= \"uniform\")\n",
    "   \n",
    "bst_nh = XGBClassifier( tree_method='gpu_hist', \n",
    "                           n_estimators=900,\n",
    "                           max_depth=2,\n",
    "                       random_state=42,\n",
    "                       reg_lambda=0.2953684210526316,\n",
    "                       eta=0.2733333333333333,\n",
    "                           min_child_weight = 2,\n",
    "                           max_delta_step= 4,\n",
    "                           sampling_method= \"uniform\",\n",
    "                      subsample=0.5)\n",
    "\n",
    "# bst = SGDClassifier(random_state=42, loss=\"log_loss\", eta0=1, learning_rate=\"constant\", penalty=\"l2\", alpha=0.0002) # XGBClassifier(booster=\"gblinear\") #RandomForestClassifier()\n",
    "   \n",
    "# bst_nh = SGDClassifier(random_state=42, loss=\"log_loss\", eta0=1, learning_rate=\"constant\", penalty=\"l2\", alpha=0.0002) #XGBClassifier(booster=\"gblinear\") #RandomForestClassifier()\n",
    "\n",
    "# bst = SVC(probability=True, C=100, kernel=\"linear\", degree=1)\n",
    "   \n",
    "# bst_nh = SVC(probability=True, C=100, kernel=\"linear\", degree=1)\n",
    "\n",
    "feature_based_aggregation:Feature_based_aggregation = {\n",
    "    \"concat\": True,\n",
    "    \"combined_clf\": clf,\n",
    "    \"n_estimators\": 1,\n",
    "    \n",
    "}\n",
    "weight = .5 #0.475\n",
    "classifier_based_aggregation:Classifier_based_aggregation = {\n",
    "    \"clf_list\": [bst, bst_nh],\n",
    "    \"n_estimators\": 1,\n",
    "    \"weights\": [weight, 1-weight]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a4d85ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.73400\n",
      "[1]\tvalidation_0-mlogloss:1.63995\n",
      "[2]\tvalidation_0-mlogloss:1.54837\n",
      "[3]\tvalidation_0-mlogloss:1.48037\n",
      "[4]\tvalidation_0-mlogloss:1.45511\n",
      "[5]\tvalidation_0-mlogloss:1.42616\n",
      "[6]\tvalidation_0-mlogloss:1.40100\n",
      "[7]\tvalidation_0-mlogloss:1.37746\n",
      "[8]\tvalidation_0-mlogloss:1.37575\n",
      "[9]\tvalidation_0-mlogloss:1.36987\n",
      "[10]\tvalidation_0-mlogloss:1.36355\n",
      "[11]\tvalidation_0-mlogloss:1.35918\n",
      "[12]\tvalidation_0-mlogloss:1.34453\n",
      "[13]\tvalidation_0-mlogloss:1.34202\n",
      "[14]\tvalidation_0-mlogloss:1.33372\n",
      "[15]\tvalidation_0-mlogloss:1.33217\n",
      "[16]\tvalidation_0-mlogloss:1.32909\n",
      "[17]\tvalidation_0-mlogloss:1.32382\n",
      "[18]\tvalidation_0-mlogloss:1.32752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:19:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19]\tvalidation_0-mlogloss:1.32067\n",
      "[20]\tvalidation_0-mlogloss:1.31594\n",
      "[21]\tvalidation_0-mlogloss:1.32297\n",
      "[22]\tvalidation_0-mlogloss:1.32562\n",
      "[0]\tvalidation_0-mlogloss:1.66496\n",
      "[1]\tvalidation_0-mlogloss:1.47006\n",
      "[2]\tvalidation_0-mlogloss:1.35592\n",
      "[3]\tvalidation_0-mlogloss:1.25656\n",
      "[4]\tvalidation_0-mlogloss:1.16575\n",
      "[5]\tvalidation_0-mlogloss:1.10393\n",
      "[6]\tvalidation_0-mlogloss:1.04138\n",
      "[7]\tvalidation_0-mlogloss:0.99219\n",
      "[8]\tvalidation_0-mlogloss:0.94844\n",
      "[9]\tvalidation_0-mlogloss:0.89773\n",
      "[10]\tvalidation_0-mlogloss:0.87204\n",
      "[11]\tvalidation_0-mlogloss:0.85190\n",
      "[12]\tvalidation_0-mlogloss:0.82722\n",
      "[13]\tvalidation_0-mlogloss:0.82055\n",
      "[14]\tvalidation_0-mlogloss:0.80748\n",
      "[15]\tvalidation_0-mlogloss:0.79850\n",
      "[16]\tvalidation_0-mlogloss:0.79455\n",
      "[17]\tvalidation_0-mlogloss:0.79733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:19:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18]\tvalidation_0-mlogloss:0.78743\n",
      "[19]\tvalidation_0-mlogloss:0.78669\n",
      "[20]\tvalidation_0-mlogloss:0.76802\n",
      "[21]\tvalidation_0-mlogloss:0.76707\n",
      "[22]\tvalidation_0-mlogloss:0.76692\n",
      "[23]\tvalidation_0-mlogloss:0.76274\n",
      "[24]\tvalidation_0-mlogloss:0.76288\n",
      "[25]\tvalidation_0-mlogloss:0.75665\n",
      "[26]\tvalidation_0-mlogloss:0.75304\n",
      "[27]\tvalidation_0-mlogloss:0.74388\n",
      "[28]\tvalidation_0-mlogloss:0.73596\n",
      "[29]\tvalidation_0-mlogloss:0.73232\n",
      "[30]\tvalidation_0-mlogloss:0.73394\n",
      "[31]\tvalidation_0-mlogloss:0.73615\n",
      "[32]\tvalidation_0-mlogloss:0.73575\n",
      "1.5777363777160645\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import time\n",
    "def user_function(origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors, hop):\n",
    "    return  updated_features + sum_neighbors #/ torch.max(sum_neighbors, 0)[0]\n",
    "\n",
    "framework_xgb = Framework(user_function, hops=3, use_feature_based_aggregation = False,\n",
    "              feature_based_aggregation = feature_based_aggregation,\n",
    "                classifier_based_aggregation = classifier_based_aggregation, gpu_idx = 1, handle_nan = 0, normalize=True, use_pseudo_attention=True,\n",
    "                    cosine_eps = .01 , dropout_attn=None)\n",
    "\n",
    "start = time.time()\n",
    "self_val, neighbors_val = framework_xgb.get_features(X, edge_index,val)\n",
    "self_val, neighbors_val = self_val.cpu(), neighbors_val.cpu()\n",
    "kwargs_list=[{\n",
    "    \"eval_set\":[(self_val, y[val])], \"early_stopping_rounds\":3\n",
    "}, {\"eval_set\":[(neighbors_val, y[val])], \"early_stopping_rounds\":3}]\n",
    "framework_xgb.fit(X, edge_index, y, train, kwargs_list=kwargs_list) #, kwargs_list=kwargs_list\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4f7eb182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:19:10] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78\n",
      "0.808\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "pred = framework_xgb.predict(X, edge_index, test) \n",
    "pred_val = framework_xgb.predict(X, edge_index, val) \n",
    "y_test = y[test]\n",
    "y_val = y[val]\n",
    "print(accuracy_score(y_val, pred_val))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "954687d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2784135341644287\n",
      "0.726\n",
      "0.698\n"
     ]
    }
   ],
   "source": [
    "bst = SVC(probability=True, C=.7, kernel=\"linear\", degree=1)\n",
    "   \n",
    "bst_nh = SVC(probability=True, C=.7, kernel=\"linear\", degree=1)\n",
    "\n",
    "feature_based_aggregation:Feature_based_aggregation = {\n",
    "    \"concat\": True,\n",
    "    \"combined_clf\": clf,\n",
    "    \"n_estimators\": 1,\n",
    "    \n",
    "}\n",
    "weight = .5 #0.475\n",
    "classifier_based_aggregation:Classifier_based_aggregation = {\n",
    "    \"clf_list\": [bst, bst_nh],\n",
    "    \"n_estimators\": 1,\n",
    "    \"weights\": [weight, 1-weight]\n",
    "}\n",
    "\n",
    "framework_svc = Framework(user_function, hops=3, use_feature_based_aggregation = False,\n",
    "              feature_based_aggregation = feature_based_aggregation,\n",
    "                classifier_based_aggregation = classifier_based_aggregation, gpu_idx = 1, handle_nan = 0, normalize=True, use_pseudo_attention=True,\n",
    "                    cosine_eps = .01 , dropout_attn=None)\n",
    "\n",
    "start = time.time()\n",
    "framework_svc.fit(X, edge_index, y, train) \n",
    "print(time.time() - start)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "pred = framework_svc.predict(X, edge_index, test) \n",
    "pred_val = framework_svc.predict(X, edge_index, val) \n",
    "y_test = y[test]\n",
    "y_val = y[val]\n",
    "print(accuracy_score(y_val, pred_val))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "17b1c195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798\n",
      "0.826\n"
     ]
    }
   ],
   "source": [
    "pred_val = (framework_svc.predict_proba(X, edge_index, val) +  framework_xgb.predict_proba(X, edge_index, val)).argmax(1)\n",
    "print(accuracy_score(y_val, pred_val))\n",
    "pred = (.4*framework_svc.predict_proba(X, edge_index, test) +  .6*framework_xgb.predict_proba(X, edge_index, test)).argmax(1)\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dac0f630",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This Framework instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m features, feature_names \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m0\u001b[39m,)], [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m     10\u001b[0m deciles \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)}\n\u001b[0;32m---> 11\u001b[0m pd_results \u001b[38;5;241m=\u001b[39m \u001b[43mpartial_dependence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maverage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_resolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m display \u001b[38;5;241m=\u001b[39m PartialDependenceDisplay(\n\u001b[1;32m     14\u001b[0m     [pd_results], features\u001b[38;5;241m=\u001b[39mfeatures, feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[1;32m     15\u001b[0m     target_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, deciles\u001b[38;5;241m=\u001b[39mdeciles\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m display\u001b[38;5;241m.\u001b[39mplot()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/inspection/_partial_dependence.py:574\u001b[0m, in \u001b[0;36mpartial_dependence\u001b[0;34m(estimator, X, features, sample_weight, categorical_features, feature_names, response_method, percentiles, grid_resolution, method, kind)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    354\u001b[0m     {\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m     kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    386\u001b[0m ):\n\u001b[1;32m    387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Partial dependence of ``features``.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m    Partial dependence of a feature (or a set of features) corresponds to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;124;03m    (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 574\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_classifier(estimator) \u001b[38;5;129;01mor\u001b[39;00m is_regressor(estimator)):\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a fitted regressor or classifier.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:1462\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[1;32m   1461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This Framework instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.inspection import partial_dependence\n",
    "\n",
    "clf = framework_svc.fit(X, edge_index, y, train)[0]#GradientBoostingRegressor(n_estimators=10).fit(X, y)\n",
    "features, feature_names = [(0,)], [f\"Features #{i}\" for i in range(X.shape[1])]\n",
    "deciles = {0: np.linspace(0, 1, num=5)}\n",
    "pd_results = partial_dependence(\n",
    "    clf, X.numpy(), features=0, kind=\"average\", grid_resolution=100)\n",
    "display = PartialDependenceDisplay(\n",
    "    [pd_results], features=features, feature_names=feature_names,\n",
    "    target_idx=0, deciles=deciles\n",
    ")\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf19809",
   "metadata": {},
   "source": [
    "## Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13360ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework = Framework(user_function, hops=3, use_feature_based_aggregation = False,\n",
    "              feature_based_aggregation = feature_based_aggregation,\n",
    "                classifier_based_aggregation = classifier_based_aggregation, gpu_idx = 1, handle_nan = 0, normalize=True, use_pseudo_attention=True,\n",
    "                    cosine_eps = .01 , dropout_attn=None)\n",
    "\n",
    "self_val, neighbors_val = framework.get_features(X, edge_index,val)\n",
    "self_val, neighbors_val = self_val.cpu(), neighbors_val.cpu()\n",
    "self_train, neighbors_train = framework.get_features(X, edge_index,train)\n",
    "self_train, neighbors_train = self_train.cpu(), neighbors_train.cpu()\n",
    "\n",
    "self_test, neighbors_test = framework.get_features(X, edge_index,test)\n",
    "self_test, neighbors_test = self_test.cpu(), neighbors_test.cpu()\n",
    "\n",
    "X_train_self = self_train\n",
    "X_train_neigh = neighbors_train\n",
    "y_train = y[train]\n",
    "X_val_self = self_val\n",
    "X_val_neigh = neighbors_val\n",
    "y_val = y[val]\n",
    "X_test_self = self_test\n",
    "X_test_neigh = neighbors_test\n",
    "y_test = y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a08bc7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best set of hyperparameters self:  {'alpha': 0.05829725506606954, 'booster': 'dart', 'colsample_bylevel': 0.717819726384605, 'colsample_bynode': 0.701294796439535, 'colsample_bytree': 0.8341038094607527, 'eta': 0.03104742267586813, 'max_delta_step': 3, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 9, 'rate_drop': 0.30909818686663004, 'reg_lambda': 0.03289950239042971, 'skip_drop': 0.21073731947232097, 'subsample': 0.8199534198900966}\n",
      "Best set of hyperparameters neighbor:  {'alpha': 0.07148295824648293, 'booster': 2, 'colsample_bylevel': 0.889831867304163, 'colsample_bynode': 0.6806418352162596, 'colsample_bytree': 0.9358447943019634, 'eta': 0.03622255613770491, 'max_delta_step': 3, 'max_depth': 2, 'min_child_weight': 1, 'n_estimators': 7, 'rate_drop': 0.7327269501508407, 'reg_lambda': 0.12796042172483413, 'skip_drop': 0.5709088020105434, 'subsample': 0.7593303490564857}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp,STATUS_OK\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "booster_self = [\"gbtree\", \"dart\"]\n",
    "booster_neigh = [\"gbtree\",  \"dart\"]\n",
    "\n",
    "n_estimators_self = [5, 50, 65, 70, 75, 80, 90, 100,110, 120, 150, 200, 400]\n",
    "n_estimators_neigh = [5, 50, 75, 100, 125, 150,160,170, 175,180, 200]\n",
    "\n",
    "max_depth_self = [None, 1, 2, 3, 4, 8]\n",
    "max_depth_neigh = [None,2, 3, 4, 5,6,7,8,9,10]\n",
    "\n",
    "max_delta_step_self = [None, 1, 2, 3, 4, 8]\n",
    "max_delta_step_neigh = [None, 1, 2, 3, 4, 8]\n",
    "\n",
    "min_child_weight_self = [None, 1, 2, 3, 4, 8]\n",
    "min_child_weight_neigh = [None, 1, 2, 3, 4, 8]\n",
    "# Define the hyperparameter space\n",
    "space_self = {\n",
    "    'booster': hp.choice('booster',booster_self),\n",
    "    'n_estimators': hp.choice('n_estimators',n_estimators_self),\n",
    "    'max_depth': hp.choice('max_depth',max_depth_self),\n",
    "    'eta': hp.loguniform('eta', -5, -1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'max_delta_step': hp.choice('max_delta_step',max_delta_step_self),\n",
    "    'min_child_weight': hp.choice('min_child_weight',min_child_weight_self),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.02, 0.1),\n",
    "    'alpha': hp.uniform('alpha', 0, 0.1),\n",
    "   'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1.0),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.7, 1.0),\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.7, 1.0),\n",
    "    'rate_drop': hp.uniform('rate_drop', 0.0, 1.0),\n",
    "    'skip_drop': hp.uniform('skip_drop', 0.0, 1.0),\n",
    "}\n",
    "\n",
    "space_neigh = {\n",
    "    'booster': hp.choice('booster',booster_neigh),\n",
    "    'n_estimators': hp.choice('n_estimators',n_estimators_neigh),\n",
    "    'max_depth': hp.choice('max_depth',max_depth_neigh),\n",
    "    'eta': hp.loguniform('eta', -5, -1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'max_delta_step': hp.choice('max_delta_step',max_delta_step_neigh),\n",
    "    'min_child_weight': hp.choice('min_child_weight',min_child_weight_neigh),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.01, 0.15),\n",
    "    'alpha': hp.uniform('alpha', 0, 0.15),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.6, 1),\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.6, 1),\n",
    "    'rate_drop': hp.uniform('rate_drop', 0.0, 1.0),\n",
    "    'skip_drop': hp.uniform('skip_drop', 0.0, 1.0),\n",
    "}\n",
    "\n",
    "                        \n",
    "# Define the objective function to minimize\n",
    "def objective_self(params):\n",
    "    xgb_model = XGBClassifier(tree_method='gpu_hist',random_state=42, **params)\n",
    "    xgb_model.fit(X_train_self, y_train)\n",
    "    y_pred = xgb_model.predict(X_val_self)\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Define the objective function to minimize\n",
    "def objective_neigh(params):\n",
    "    xgb_model = XGBClassifier(tree_method='gpu_hist',random_state=42, **params)\n",
    "    xgb_model.fit(X_train_neigh, y_train)\n",
    "    y_pred = xgb_model.predict(X_val_neigh)\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Perform the optimization\n",
    "# best_params_self = fmin(objective_self, space_self, algo=tpe.suggest, max_evals=200)\n",
    "# best_params_neigh = fmin(objective_neigh, space_neigh, algo=tpe.suggest, max_evals=200)\n",
    "print(\"Best set of hyperparameters self: \", best_params_self)\n",
    "print(\"Best set of hyperparameters neighbor: \", best_params_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac672e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_self = {'alpha': 0.05829725506606954, 'booster': 0, 'colsample_bylevel': 0.717819726384605, 'colsample_bynode': 0.701294796439535, 'colsample_bytree': 0.8341038094607527, 'eta': 0.03104742267586813, 'max_delta_step': 3, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 9, 'rate_drop': 0.30909818686663004, 'reg_lambda': 0.03289950239042971, 'skip_drop': 0.21073731947232097, 'subsample': 0.8199534198900966}\n",
    "best_params_neigh = {'alpha': 0.07148295824648293, 'booster': 2, 'colsample_bylevel': 0.889831867304163, 'colsample_bynode': 0.6806418352162596, 'colsample_bytree': 0.9358447943019634, 'eta': 0.03622255613770491, 'max_delta_step': 3, 'max_depth': 2, 'min_child_weight': 1, 'n_estimators': 7, 'rate_drop': 0.7327269501508407, 'reg_lambda': 0.12796042172483413, 'skip_drop': 0.5709088020105434, 'subsample': 0.7593303490564857}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4175d20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.788\n",
      "0.804\n"
     ]
    }
   ],
   "source": [
    "best_params_self[\"booster\"] = \"dart\"\n",
    "best_params_self[\"n_estimators\"] = n_estimators_self[best_params_self[\"n_estimators\"]]\n",
    "best_params_self[\"max_depth\"] = max_depth_self[best_params_self[\"max_depth\"]]\n",
    "best_params_self[\"max_delta_step\"] = max_delta_step_self[best_params_self[\"max_delta_step\"]]\n",
    "best_params_self[\"min_child_weight\"] = min_child_weight_self[best_params_self[\"min_child_weight\"]]\n",
    "\n",
    "best_params_neigh[\"booster\"] = \"dart\"\n",
    "best_params_neigh[\"n_estimators\"] = n_estimators_neigh[best_params_neigh[\"n_estimators\"]]\n",
    "best_params_neigh[\"max_depth\"] = max_depth_neigh[best_params_neigh[\"max_depth\"]]\n",
    "best_params_neigh[\"max_delta_step\"] = max_delta_step_neigh[best_params_neigh[\"max_delta_step\"]]\n",
    "best_params_neigh[\"min_child_weight\"] = min_child_weight_neigh[best_params_neigh[\"min_child_weight\"]]\n",
    "\n",
    "xgb_model_self = XGBClassifier(tree_method='hist',device=\"cuda:0\", **best_params_self)\n",
    "xgb_model_neigh = XGBClassifier(tree_method='hist',device=\"cuda:0\", **best_params_neigh)\n",
    "\n",
    "xgb_model_self.fit(X_train_self, y_train)\n",
    "xgb_model_neigh.fit(X_train_neigh, y_train)\n",
    "\n",
    "\n",
    "y_pred_proba_self_val = xgb_model_self.predict_proba(X_val_self)\n",
    "y_pred_proba_neigh_val = xgb_model_neigh.predict_proba(X_val_neigh)\n",
    "y_pred_proba_self_test = xgb_model_self.predict_proba(X_test_self)\n",
    "y_pred_proba_neigh_test = xgb_model_neigh.predict_proba(X_test_neigh)\n",
    "\n",
    "weight = 0.475\n",
    "y_pred_proba_val = weight* y_pred_proba_self_val + (1-weight)*y_pred_proba_neigh_val\n",
    "y_pred_proba_test = weight* y_pred_proba_self_test + (1-weight)*y_pred_proba_neigh_test\n",
    "y_pred_val = y_pred_proba_val.argmax(1)\n",
    "y_pred_test = y_pred_proba_test.argmax(1)\n",
    "\n",
    "print(accuracy_score(y_val, y_pred_val))\n",
    "print(accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da2d76ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.821"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba_test_0 = framework_svc.predict_proba(X, edge_index, test) \n",
    "accuracy_score(y_test, (y_pred_proba_test_0 + y_pred_proba_test).argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9176d545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.831"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba_test_0 = framework_lin.predict_proba(X, edge_index, test) \n",
    "accuracy_score(y_test, (y_pred_proba_test_0 + y_pred_proba_test).argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92278220",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "1) Combination of different classifiers (e.g., SVC with XGBoost trees)\n",
    "2) Hops decay\n",
    "3) Combination of diifferent aggregators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c51067a",
   "metadata": {},
   "source": [
    "## Hyperopt linear xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fb4df13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.10trial/s, best loss: -0.524]\n",
      "100%|███████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.83trial/s, best loss: -0.712]\n",
      "Best set of hyperparameters self lin:  {'alpha': 3.144119410867499e-05, 'feature_selector': 0, 'n_estimators': 0, 'reg_lambda': 0.07465657953014855, 'updater': 0}\n",
      "Best set of hyperparameters neighbor lin:  {'alpha': 8.439797309137466e-05, 'feature_selector': 0, 'n_estimators': 10, 'reg_lambda': 0.052744670914722055, 'updater': 0}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp,STATUS_OK\n",
    "\n",
    "\n",
    "n_estimators_self_lin = [5, 50, 65, 70, 75, 80, 90, 100,110, 120, 150, 200, 400]\n",
    "n_estimators_neigh_lin = [5, 50, 75, 100, 125, 150,160,170, 175,180, 200]\n",
    "\n",
    "updater_self = [\"shotgun\"]\n",
    "updater_neigh = [\"shotgun\"]\n",
    "\n",
    "feature_selector_self = [\"cyclic\"]\n",
    "feature_selector_neigh = [\"cyclic\"]\n",
    "\n",
    "# Define the hyperparameter space\n",
    "space_self_lin = {\n",
    "    'n_estimators': hp.choice('n_estimators',n_estimators_self_lin),\n",
    "    'updater': hp.choice('updater',updater_self),\n",
    "    'feature_selector': hp.choice('feature_selector',feature_selector_self),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.02, 0.1),\n",
    "    'alpha': hp.uniform('alpha', 0, 0.1),\n",
    "}\n",
    "\n",
    "space_neigh_lin = {\n",
    "    'n_estimators': hp.choice('n_estimators',n_estimators_neigh_lin),\n",
    "     'updater': hp.choice('updater',updater_neigh),\n",
    "    'feature_selector': hp.choice('feature_selector',feature_selector_neigh),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.01, 0.15),\n",
    "    'alpha': hp.uniform('alpha', 0, 0.15),\n",
    "}\n",
    "\n",
    "                        \n",
    "# Define the objective function to minimize\n",
    "def objective_self_lin(params):\n",
    "    xgb_model = XGBClassifier(  booster=\"gblinear\", random_state=42, **params)\n",
    "    xgb_model.fit(X_train_self, y_train)\n",
    "    y_pred = xgb_model.predict(X_val_self)\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Define the objective function to minimize\n",
    "def objective_neigh_lin(params):\n",
    "    xgb_model = XGBClassifier(booster=\"gblinear\", random_state=42, **params)\n",
    "    xgb_model.fit(X_train_neigh, y_train)\n",
    "    y_pred = xgb_model.predict(X_val_neigh)\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Perform the optimization\n",
    "best_params_self_lin = fmin(objective_self_lin, space_self_lin, algo=tpe.suggest, max_evals=100)\n",
    "best_params_neigh_lin = fmin(objective_neigh_lin, space_neigh_lin, algo=tpe.suggest, max_evals=100)\n",
    "print(\"Best set of hyperparameters self lin: \", best_params_self_lin)\n",
    "print(\"Best set of hyperparameters neighbor lin: \", best_params_neigh_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7381e538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:09:55] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"device\" } are not used.\n",
      "\n",
      "[10:09:55] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"device\" } are not used.\n",
      "\n",
      "0.72\n",
      "0.713\n"
     ]
    }
   ],
   "source": [
    "best_params_self_lin[\"n_estimators\"] = n_estimators_self_lin[best_params_self_lin[\"n_estimators\"]]\n",
    "best_params_self_lin[\"updater\"] = updater_self[best_params_self_lin[\"updater\"]]\n",
    "best_params_self_lin[\"feature_selector\"] = feature_selector_self[best_params_self_lin[\"feature_selector\"]]\n",
    "\n",
    "best_params_neigh_lin[\"n_estimators\"] = n_estimators_neigh_lin[best_params_neigh_lin[\"n_estimators\"]]\n",
    "best_params_neigh_lin[\"updater\"] = updater_neigh[best_params_neigh_lin[\"updater\"]]\n",
    "best_params_neigh_lin[\"feature_selector\"] = feature_selector_neigh[best_params_neigh_lin[\"feature_selector\"]]\n",
    "\n",
    "xgb_model_self_lin = XGBClassifier(device=\"gpu\", booster=\"gblinear\", random_state=42,**best_params_self_lin)\n",
    "xgb_model_neigh_lin = XGBClassifier(device=\"gpu\", booster=\"gblinear\", random_state=42, **best_params_neigh_lin)\n",
    "\n",
    "xgb_model_self_lin.fit(X_train_self, y_train)\n",
    "xgb_model_neigh_lin.fit(X_train_neigh, y_train)\n",
    "\n",
    "\n",
    "y_pred_proba_self_val_lin = xgb_model_self_lin.predict_proba(X_val_self)\n",
    "y_pred_proba_neigh_val_lin = xgb_model_neigh_lin.predict_proba(X_val_neigh)\n",
    "y_pred_proba_self_test_lin = xgb_model_self_lin.predict_proba(X_test_self)\n",
    "y_pred_proba_neigh_test_lin = xgb_model_neigh_lin.predict_proba(X_test_neigh)\n",
    "\n",
    "weight = 0.475\n",
    "y_pred_proba_val_lin = weight* y_pred_proba_self_val_lin + (1-weight)*y_pred_proba_neigh_val_lin\n",
    "y_pred_proba_test_lin = weight* y_pred_proba_self_test_lin + (1-weight)*y_pred_proba_neigh_test_lin\n",
    "y_pred_val_lin = y_pred_proba_val_lin.argmax(1)\n",
    "y_pred_test_lin = y_pred_proba_test_lin.argmax(1)\n",
    "\n",
    "print(accuracy_score(y_val, y_pred_val_lin))\n",
    "print(accuracy_score(y_test, y_pred_test_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77b3bbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.802"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, (y_pred_proba_test + y_pred_proba_test_lin).argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2965f67",
   "metadata": {},
   "source": [
    "## Hyperopt SGD as linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6a9d987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.12trial/s, best loss: -0.6]\n",
      "100%|█████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.19trial/s, best loss: -0.6]\n",
      "Best set of hyperparameters self:  {'alpha': 0.00016234708404326437, 'eta0': 0.009712344844625765, 'l1_ratio': 0.9066656763340981, 'loss': 1, 'n_iter_no_change': 0, 'penalty': 2}\n",
      "Best set of hyperparameters neighbor:  {'alpha': 0.00015955127756278622, 'eta0': 0.07263964319593422, 'l1_ratio': 0.8220050106364165, 'loss': 1, 'n_iter_no_change': 1, 'penalty': 2}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp,STATUS_OK\n",
    "\n",
    "loss_self = [\"modified_huber\", \"log_loss\"]\n",
    "loss_neigh = [\"modified_huber\", \"log_loss\"]\n",
    "\n",
    "penalty_self = [\"l2\", \"l1\", \"elasticnet\"]\n",
    "penalty_neigh = [\"l2\", \"l1\", \"elasticnet\"]\n",
    "\n",
    "n_iter_no_change_self = [5, 10, 20]\n",
    "n_iter_no_change_neigh = [5, 10, 20]\n",
    "\n",
    "# Define the hyperparameter space\n",
    "space_self_sgd = {\n",
    "    'loss': hp.choice('loss',loss_self),\n",
    "    'penalty': hp.choice('penalty',penalty_self),\n",
    "    'n_iter_no_change': hp.choice('n_iter_no_change',n_iter_no_change_self),\n",
    "    'eta0': hp.loguniform('eta0', -5, -1),\n",
    "    'alpha': hp.uniform('alpha', 0.00008, 0.00018),\n",
    "    'l1_ratio':hp.uniform('l1_ratio', 0.0, 1.0),\n",
    "}\n",
    "\n",
    "space_neigh_sgd = {\n",
    "    'loss': hp.choice('loss',loss_neigh),\n",
    "    'penalty': hp.choice('penalty',penalty_neigh),\n",
    "    'n_iter_no_change': hp.choice('n_iter_no_change',n_iter_no_change_neigh),\n",
    "    'eta0': hp.loguniform('eta0', -5, -1),\n",
    "    'alpha': hp.uniform('alpha', 0.00008, 0.00018),\n",
    "    'l1_ratio':hp.uniform('l1_ratio', 0.0, 1.0),\n",
    "}\n",
    "\n",
    "                        \n",
    "# Define the objective function to minimize\n",
    "def objective_self_sgd(params):\n",
    "    model = SGDClassifier(random_state=42, n_jobs=-1,tol=None, **params)\n",
    "    model.fit(X_train_self, y_train)\n",
    "    y_pred = model.predict(X_val_self)\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Define the objective function to minimize\n",
    "def objective_neigh_sgd(params):\n",
    "    model = SGDClassifier(random_state=42, n_jobs=-1,tol=None, **params)\n",
    "    model.fit(X_train_self, y_train)\n",
    "    y_pred = model.predict(X_val_self)\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Perform the optimization\n",
    "best_params_self_sgd = fmin(objective_self_sgd, space_self_sgd, algo=tpe.suggest, max_evals=100)\n",
    "best_params_neigh_sgd = fmin(objective_neigh_sgd, space_neigh_sgd, algo=tpe.suggest, max_evals=100)\n",
    "print(\"Best set of hyperparameters self: \", best_params_self_sgd)\n",
    "print(\"Best set of hyperparameters neighbor: \", best_params_neigh_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23c84a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.708\n",
      "0.706\n"
     ]
    }
   ],
   "source": [
    "best_params_self_sgd[\"loss\"] = loss_self[best_params_self_sgd[\"loss\"]]\n",
    "best_params_self_sgd[\"penalty\"] = penalty_self[best_params_self_sgd[\"penalty\"]]\n",
    "best_params_self_sgd[\"n_iter_no_change\"] = n_iter_no_change_self[best_params_self_sgd[\"n_iter_no_change\"]]\n",
    "\n",
    "best_params_neigh_sgd[\"loss\"] = loss_neigh[best_params_neigh_sgd[\"loss\"]]\n",
    "best_params_neigh_sgd[\"penalty\"] = penalty_neigh[best_params_neigh_sgd[\"penalty\"]]\n",
    "best_params_neigh_sgd[\"n_iter_no_change\"] = n_iter_no_change_neigh[best_params_neigh_sgd[\"n_iter_no_change\"]]\n",
    "\n",
    "sgd_model_self = SGDClassifier(random_state=42, n_jobs=-1,tol=None, **best_params_self_sgd)\n",
    "sgd_model_neigh = SGDClassifier(random_state=42, n_jobs=-1,tol=None, **best_params_neigh_sgd)\n",
    "\n",
    "sgd_model_self.fit(X_train_self, y_train)\n",
    "sgd_model_neigh.fit(X_train_neigh, y_train)\n",
    "\n",
    "\n",
    "y_pred_proba_self_val_sgd = sgd_model_self.predict_proba(X_val_self)\n",
    "y_pred_proba_neigh_val_sgd = sgd_model_neigh.predict_proba(X_val_neigh)\n",
    "y_pred_proba_self_test_sgd = sgd_model_self.predict_proba(X_test_self)\n",
    "y_pred_proba_neigh_test_sgd = sgd_model_neigh.predict_proba(X_test_neigh)\n",
    "\n",
    "weight = 0.475\n",
    "y_pred_proba_val_sgd = weight* y_pred_proba_self_val_sgd + (1-weight)*y_pred_proba_neigh_val_sgd\n",
    "y_pred_proba_test_sgd = weight* y_pred_proba_self_test_sgd + (1-weight)*y_pred_proba_neigh_test_sgd\n",
    "y_pred_val_sgd = y_pred_proba_val_sgd.argmax(1)\n",
    "y_pred_test_sgd = y_pred_proba_test_sgd.argmax(1)\n",
    "\n",
    "print(accuracy_score(y_val, y_pred_val_sgd))\n",
    "print(accuracy_score(y_test, y_pred_test_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af0b8021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.777"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, (y_pred_proba_test + y_pred_proba_test_sgd).argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "9642261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Todo linear sgd pipeline\n",
    "## final eval\n",
    "## stacking/bagging/boosting algorithms in framework- multi classifiiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "700d0f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"gat_pred_0834.csv\")\n",
    "gat_pred = df[\"pred\"].values\n",
    "gat_true = df[\"true\"].values\n",
    "np.equal(gat_true, y_test).sum() == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4ae15692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [ True, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False,  True,  ..., False, False, False],\n",
       "        [False, False,  True,  ..., False, False, False]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_self[~np.equal(y_pred_test, gat_pred)] != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "043fe8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2054e-03, 2.1152e-04, 9.5923e-04,  ..., 2.3565e-03, 4.7782e-04,\n",
       "         1.1903e-05],\n",
       "        [1.1781e-04, 1.4961e-04, 1.2001e-04,  ..., 0.0000e+00, 5.0621e-05,\n",
       "         8.4189e-06],\n",
       "        [2.9530e-01, 1.8666e-04, 9.4587e-04,  ..., 8.8360e-03, 3.5020e-04,\n",
       "         1.4027e-04],\n",
       "        ...,\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 2.1688e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 4.7500e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_neigh[~np.equal(y_pred_test, gat_pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651fe7e",
   "metadata": {},
   "source": [
    "## Old experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "451ae41e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mself\u001b[39m, neighbors \u001b[38;5;241m=\u001b[39m framework\u001b[38;5;241m.\u001b[39mget_features(X, edge_index, \u001b[43mtrain_indices\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m self_val, neighbors_val \u001b[38;5;241m=\u001b[39m framework\u001b[38;5;241m.\u001b[39mget_features(X, edge_index,val)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m, neighbors, self_val, neighbors_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), neighbors\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),self_val\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), neighbors_val\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_indices' is not defined"
     ]
    }
   ],
   "source": [
    "self, neighbors = framework.get_features(X, edge_index, train_indices, True)\n",
    "self_val, neighbors_val = framework.get_features(X, edge_index,val)\n",
    "self, neighbors, self_val, neighbors_val = self.cpu().numpy(), neighbors.cpu().numpy(),self_val.cpu().numpy(), neighbors_val.cpu().numpy()\n",
    "bst.fit(self, y[train], eval_set=[(self_val, y[val])], early_stopping_rounds=5)\n",
    "bst_nh.fit(neighbors, y[train], eval_set=[(neighbors_val, y[val])], early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90d7f9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "tensor(0.7910)\n",
      "0.109\n",
      "tensor(0.7930)\n",
      "0.20800000000000002\n",
      "tensor(0.7970)\n",
      "0.30700000000000005\n",
      "tensor(0.8000)\n",
      "0.406\n",
      "tensor(0.8090)\n",
      "0.505\n",
      "tensor(0.8090)\n",
      "0.6040000000000001\n",
      "tensor(0.7830)\n",
      "0.7030000000000001\n",
      "tensor(0.7360)\n",
      "0.802\n",
      "tensor(0.6820)\n",
      "0.901\n",
      "tensor(0.6130)\n",
      "1.0\n",
      "tensor(0.5560)\n"
     ]
    }
   ],
   "source": [
    "for i in np.linspace(0.01, 1 , 11, endpoint=True):\n",
    "    print(i)\n",
    "    self_test, neighbors_test = framework.get_features(X, edge_index,test_indices)\n",
    "    self_test, neighbors_test = self_test.cpu().numpy(), neighbors_test.cpu().numpy()\n",
    "    pred = (bst.predict_proba(self_test)*i + (1-i)*bst_nh.predict_proba(neighbors_test)).argmax(-1) \n",
    "    y_test = y[test_indices]\n",
    "    print(np.equal(pred, y_test).sum() / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acd1cfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dwalke/miniconda3/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mframework\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mgrid_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreg_lambda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_estimators\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#1100\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin_child_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#2\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_delta_step\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#4\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#0.3\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_depth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#2\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreg_lambda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.2953684210526316\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_estimators\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m900\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#700\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin_child_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#2\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_delta_step\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#4\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.2733333333333333\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#0.375025\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_depth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#2\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 386\u001b[0m, in \u001b[0;36mFramework.grid_search\u001b[0;34m(self, X_train, edge_index, y_train, train_mask, grid_params, **grid_kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m optimzed_clfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    381\u001b[0m grid_self \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m    382\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mclf_list[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    383\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mgrid_params[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgrid_kwargs\n\u001b[1;32m    385\u001b[0m )\n\u001b[0;32m--> 386\u001b[0m \u001b[43mgrid_self\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m optimzed_clfs\u001b[38;5;241m.\u001b[39mappend(grid_self\u001b[38;5;241m.\u001b[39mbest_estimator_)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m neighbors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1419\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/joblib/parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/joblib/parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1586\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1587\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/joblib/parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1698\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1699\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "framework.grid_search(X, edge_index, y, val_indices, \n",
    "                      grid_params=[{'reg_lambda': [0.001],\n",
    "                                    'n_estimators':[1100],#1100\n",
    "                                   'min_child_weight':[1],#2\n",
    "                                   'max_delta_step': [3],#4\n",
    "                                   'eta': [0.3], #0.3\n",
    "                                   'max_depth':[2]},#2\n",
    "                                   {'reg_lambda': [0.2953684210526316],\n",
    "                                    'n_estimators':[900],#700\n",
    "                                   'min_child_weight': [2],#2\n",
    "                                   'max_delta_step': [4],#4\n",
    "                                   'eta': [0.2733333333333333],#0.375025\n",
    "                                   'max_depth':[2]\n",
    "                                   }],#2\n",
    "                                  scoring=\"accuracy\", n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a10b583",
   "metadata": {},
   "source": [
    "## Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "061f77fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:1')\n",
      "tensor([[4.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:1')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 1., 0., 0.],\n",
       "         [1., 0., 1., 0.],\n",
       "         [1., 0., 0., 1.],\n",
       "         [0., 1., 1., 0.],\n",
       "         [0., 1., 0., 1.]], device='cuda:1'),\n",
       " tensor([[6., 5., 3., 6.],\n",
       "         [4., 1., 4., 4.],\n",
       "         [2., 6., 2., 1.],\n",
       "         [4., 0., 3., 2.],\n",
       "         [2., 2., 2., 3.],\n",
       "         [1., 3., 0., 2.],\n",
       "         [1., 0., 1., 0.],\n",
       "         [1., 0., 0., 1.],\n",
       "         [0., 1., 1., 0.],\n",
       "         [0., 1., 0., 1.]], device='cuda:1'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## small test case\n",
    "\n",
    "def user_function(origin_features, updated_features, sum_neighbors, mul_neighbors, num_neighbors):\n",
    "    return updated_features + sum_neighbors\n",
    "\n",
    "\n",
    "# edge_index = torch.tensor([[0,1,2,3], [0,0,0, 2]], dtype=torch.long)\n",
    "# X_f = torch.tensor([[0,1,2], [2,4,8],[4,8,16], [8,16,32]], dtype=torch.float)\n",
    "\n",
    "edge_index = torch.tensor([[1,2,3,4,4,5,6,7,7,7,8,8,9],\n",
    "                           [0,0,1,1,0,2,3,3,0,4,4,2,5]], dtype=torch.long)\n",
    "X_f = torch.tensor([[0,0,0,0], [1,0,0,0],[0,1,0,0], [0,0,1,0], [0,0,0,1], [1,1,0,0], [1,0,1,0], [1,0,0,1], [0,1,1,0], [0,1,0,1]], dtype=torch.float)\n",
    "\n",
    "test_framework = Framework(user_function, hops=2, use_feature_based_aggregation = False,\n",
    "              feature_based_aggregation = feature_based_aggregation,\n",
    "                classifier_based_aggregation = classifier_based_aggregation, gpu_idx = 1, handle_nan = 0, normalize=False, use_pseudo_attention=False)\n",
    "test_framework.get_features(X_f, edge_index, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c213dc",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Results of SBC similar to \"Edges are all you need\" without normalize and without pseudo-attention\n",
    "- Iterative aggregation through hops similar to GNN layer\n",
    "- Graph-awareness in ML Algorithms: with zero hops or just hop the acc drops a lot\n",
    "- Extremely fast computation wiith node lifting and torch scatter which can utilize a GPU\n",
    "- Even without SparsePCA high acc although number of features is now higher than number of nodes -> important for Ronalds Dataset (would still recommend implement dimensionality reduction in framework - before or after? aggregation part) -> potential reason: sparsity of cora features\n",
    "- solely \"feature engineering/manipulation\" (I do not change algorithmic details) -> compatible with scikit learn, xgboost\n",
    "- integration in Rahuls tools in future?\n",
    "- new RQ?: How many samples are really required for training when I use high regularization params, dropouts, subsamples etc. -> + TODO for me in this context: read mopre about, generalizations, and \"grooking\" (https://arxiv.org/abs/2309.02390)\n",
    "\n",
    "- Frage an Robert: Hat Ronald vllt doch viele Samples obwohl wenige Patienten? Mehrere Messungen mit Label für einen Patienten?\n",
    "\n",
    "### Problems with Cora - datasets\n",
    "-> potentially aim is rather to be just competetive with others than better\n",
    "1) Full split (Sometimes used, sometime even differences there sota 87-90%) -> 88 % hard to become better\n",
    "2) Public split (Couldnt reproduce their value with GAT) -> 78 % ?  \n",
    "3) Aleksa gordic split, other edge index and normalized features (up to 83 % with his implementation) -> 81 %\n",
    "\n",
    "Questions: \n",
    "- Other benchmarks than only biological or medical data? If so which one?\n",
    "- Do we want to perform better?\n",
    "- Do we want to perform better when having all samples or less samples?\n",
    "- Do we want to set up own benchmarks or want to use benchmarks from the papers?\n",
    "- test other algorithms than just xgboost and random forest?\n",
    "- GAT uses masks -> so the aggregation is still done on all nodes?\n",
    "\n",
    "=> Then I can focus hyperparameter tuning + having a goal to benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0f655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
