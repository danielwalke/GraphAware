{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f39ca8-9364-4636-b9be-5b25e76d0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "citeseer_dataset = Planetoid(root = \"./data\", name = \"CiteSeer\")\n",
    "cora_dataset = Planetoid(root = \"./data\", name = \"Cora\")\n",
    "pubmed_dataset = Planetoid(root = \"./data\", name = \"PubMed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3328f65-6046-4cbf-b832-2d0c3f9d7cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "\n",
    "def pre_process(dataset):\n",
    "    dataset.transform = T.NormalizeFeatures()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2249dbee-1352-418c-a819-3b9ddb9c695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORA = \"Cora\"\n",
    "PUBMED = \"PubMed\"\n",
    "CITESEER = \"Citeseer\"\n",
    "\n",
    "name_to_dataset = dict({})\n",
    "name_to_dataset[CORA] = pre_process(cora_dataset)\n",
    "name_to_dataset[PUBMED] = pre_process(pubmed_dataset)\n",
    "name_to_dataset[CITESEER] = pre_process(citeseer_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39dd65e5-06aa-4bcc-a426-34a059ee3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_sets = dict({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bde21fd-a919-4ba3-a78d-d98cc295c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "def add_set(set_name):\n",
    "    global name_to_sets, name_to_dataset\n",
    "    name_to_sets[set_name] = dict({})\n",
    "    \n",
    "    dataset = name_to_dataset[set_name]\n",
    "    X =  dataset[0].x \n",
    "    y =  dataset[0].y \n",
    "    \n",
    "    test =  dataset[0].test_mask\n",
    "    train = dataset[0].train_mask \n",
    "    val =  dataset[0].val_mask\n",
    "    \n",
    "    edge_index = add_self_loops(dataset[0].edge_index)[0]\n",
    "\n",
    "    name_to_sets[set_name][\"X\"] = X\n",
    "    name_to_sets[set_name][\"y\"] = y\n",
    "    name_to_sets[set_name][\"test\"] = test\n",
    "    name_to_sets[set_name][\"train\"] = train\n",
    "    name_to_sets[set_name][\"val\"] = val\n",
    "    name_to_sets[set_name][\"edge_index\"] = edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b207df0-bb50-487d-8e68-6ecddeae3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sets():\n",
    "    for set_name in name_to_dataset.keys():\n",
    "        add_set(set_name)\n",
    "create_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a235c35-d268-4ff9-8f22-6ead891a9544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(120)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_to_dataset[CITESEER].train_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a9455ce-600d-45e7-a27f-d068864c6322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp,STATUS_OK, SparkTrials, space_eval \n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "from EnsembleFramework import Framework\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "class Data():\n",
    "    def __init__(self, X, y, edge_index):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.edge_index = edge_index\n",
    "        self.train = None\n",
    "        self.val = None\n",
    "        self.test = None\n",
    "        self.X_train = None\n",
    "        self.X_val = None\n",
    "        self.X_test = None\n",
    "    \n",
    "    def set_train(self, train):\n",
    "        self.train = train\n",
    "\n",
    "    def set_test(self, test):\n",
    "        self.test = test\n",
    "\n",
    "    def set_val(self, val):\n",
    "        self.val = val\n",
    "\n",
    "    def set_X_train(self, X):\n",
    "        self.X_train = X\n",
    "\n",
    "    def set_X_val(self, X):\n",
    "        self.X_val = X\n",
    "\n",
    "    def set_X_test(self, X):\n",
    "        self.X_test = X\n",
    "\n",
    "class SparkTune():\n",
    "    def __init__(self, data,clf, evals = 10):\n",
    "        self.evals = evals\n",
    "        self.data = data\n",
    "        self.clf = clf\n",
    "        \n",
    "    def objective(self, params):\n",
    "        model = self.clf(**params)\n",
    "        model.fit(self.data.X_train, self.data.y[self.data.train])\n",
    "        y_pred = model.predict(self.data.X_val)\n",
    "        score = accuracy_score(self.data.y[self.data.val], y_pred)\n",
    "        return {'loss': -score, 'status': STATUS_OK}\n",
    "    \n",
    "    def search(self, space):\n",
    "        spark_trials = SparkTrials()\n",
    "        best_params = fmin(self.objective, space, algo=tpe.suggest, max_evals=self.evals, trials=spark_trials)\n",
    "        return best_params\n",
    "    \n",
    "\n",
    "def get_data(set_name):\n",
    "    dataset = name_to_sets[set_name]\n",
    "    data = Data(dataset[\"X\"], dataset[\"y\"], dataset[\"edge_index\"])\n",
    "    data.set_test(dataset[\"test\"])\n",
    "    data.set_val(dataset[\"val\"])\n",
    "    data.set_train(dataset[\"train\"])\n",
    "    return data\n",
    "    \n",
    "def search_hop_clf_attention_config(set_name,evals, hop, clf, user_function, attention_config, space):\n",
    "    data:Data = get_data(set_name)\n",
    "    framework = Framework([user_function], \n",
    "                     hops_list=[hop],\n",
    "                     clfs=[],\n",
    "                     gpu_idx=0,\n",
    "                     handle_nan=0.0,\n",
    "                    attention_configs=[attention_config])\n",
    "    data.set_X_train(framework.get_features(data.X, data.edge_index, data.train)[0].cpu())\n",
    "    data.set_X_val(framework.get_features(data.X, data.edge_index, data.val)[0].cpu())\n",
    "    data.set_X_test(framework.get_features(data.X, data.edge_index, data.test)[0].cpu())\n",
    "    \n",
    "    sparkTune = SparkTune(data, clf, evals = evals)\n",
    "    params = sparkTune.search(space)\n",
    "    \n",
    "    params = space_eval(space, params)\n",
    "\n",
    "    model = clf(**params)\n",
    "    kwargs={\"eval_set\":[(data.X_val, data.y[data.val])], \"early_stopping_rounds\":5} if model.__class__.__name__ == 'XGBClassifier' else {}\n",
    "    \n",
    "    model.fit(data.X_train,data.y[data.train],**kwargs)\n",
    "    train_pred = model.predict(data.X_train)\n",
    "    val_pred = model.predict(data.X_val)\n",
    "    test_pred = model.predict(data.X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(data.y[data.train], train_pred)\n",
    "    val_acc = accuracy_score(data.y[data.val], val_pred)\n",
    "    test_acc = accuracy_score(data.y[data.test], test_pred)\n",
    "    search_dict = dict({})\n",
    "    search_dict[\"train_acc\"] = train_acc\n",
    "    search_dict[\"val_acc\"] = val_acc\n",
    "    search_dict[\"test_acc\"] = test_acc\n",
    "    search_dict[\"model\"] = model\n",
    "    search_dict[\"user_function\"] = user_function\n",
    "    return search_dict\n",
    "    \n",
    "def search(set_name,evals, clfs, hops, user_functions, clfs_space, attention_configs):\n",
    "    store = dict({})\n",
    "    for clf in tqdm(clfs):\n",
    "        clf_name = clf().__class__.__name__\n",
    "        space = clfs_space[clf_name]\n",
    "        store[clf_name] = dict({})\n",
    "        for hop in tqdm(hops):\n",
    "            best_search_dict = None\n",
    "            best_val = float(\"-inf\")\n",
    "            for attention_config in tqdm(attention_configs):\n",
    "                for user_function in user_functions:\n",
    "                    print(user_function)\n",
    "                    search_dict = search_hop_clf_attention_config(set_name,evals, hop, clf, user_function, attention_config, space)\n",
    "                    if search_dict[\"val_acc\"] >= best_val:\n",
    "                        best_val = search_dict[\"val_acc\"]\n",
    "                        best_search_dict = search_dict\n",
    "                        best_search_dict[\"attention_config\"] = attention_config\n",
    "            store[clf_name][hop] = best_search_dict\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a99fd-af4f-4f72-ab9d-9719ef84db11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trial task 240 failed, exception is [Errno 28] No space left on device.\n",
      " None\n",
      "trial task 241 failed, exception is [Errno 28] No space left on device.\n",
      " None\n",
      "trial task 242 failed, exception is [Errno 28] No space left on device.\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from AutoTune2 import AutoSearch\n",
    "from hyperopt import fmin, tpe, hp,STATUS_OK, SparkTrials, space_eval \n",
    "\n",
    "penalty = [\"l1\",\"l2\", None, \"elasticnet\"]\n",
    "penalty = [None]\n",
    "max_iter = [2**i for i in range(6, 15)]\n",
    "\n",
    "lr_choices = {\n",
    "    'penalty': penalty,\n",
    "    'max_iter': max_iter,\n",
    "    \n",
    "}\n",
    "\n",
    "space_lr = {\n",
    "    **{key: hp.choice(key, value) for key, value in lr_choices.items()},\n",
    "    'tol': hp.loguniform('tol', -11, -3),\n",
    "    'C': hp.uniform('C', 0.0, 20),\n",
    "    # 'l1_ratio': hp.uniform('l1_ratio', 0.0, 1.0)\n",
    "}\n",
    "\n",
    "\n",
    "def norm_user_function(kwargs):\n",
    "    return  normalize(kwargs[\"original_features\"] + kwargs[\"summed_neighbors\"], p=2.0, dim = 1)\n",
    "    \n",
    "def user_function(kwargs):\n",
    "    return  kwargs[\"original_features\"] + kwargs[\"summed_neighbors\"]\n",
    "    \n",
    "clfs = [LogisticRegression]\n",
    "hops = [0,3,8]\n",
    "clfs_space = dict({})\n",
    "clfs_space[\"LogisticRegression\"] = space_lr\n",
    "attention_configs = [None,{'inter_layer_normalize': False,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.01,\n",
    "                     'dropout_attn': None}, \n",
    "                     {'inter_layer_normalize': True,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.01,\n",
    "                     'dropout_attn': None},\n",
    "                     {'inter_layer_normalize': True,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.001,\n",
    "                     'dropout_attn': None}]\n",
    "user_functions = [norm_user_function, user_function]\n",
    "\n",
    "searcher = AutoSearch(name_to_sets[PUBMED], max_evals=300, pred_metric = accuracy_score)\n",
    "store = searcher.search(clfs, clfs_space, hops=[0,3,8])\n",
    "# pubmed_store = search(CORA,clfs, hops, user_functions, clfs_space, attention_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ffb4e-eeeb-4d43-8176-2756b58fb083",
   "metadata": {},
   "outputs": [],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3dba0e8-64b4-47fc-a013-a079934d4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EnsembleFramework import Framework\n",
    "\n",
    "hops_list=[3]\n",
    "name_to_model = dict({})\n",
    "def fit_dataset(set_name,user_functions=user_functions, hops_list= hops_list, clfs = clfs, attention_configs= attention_configs):\n",
    "    dataset = name_to_sets[set_name]\n",
    "    y = dataset[\"y\"]\n",
    "    \n",
    "    framework = Framework(user_functions, \n",
    "                     hops_list=hops_list, ## to obtain best for local neighborhood\n",
    "                     clfs=clfs,\n",
    "                     gpu_idx=0,\n",
    "                     handle_nan=0.0,\n",
    "                    attention_configs=attention_configs)\n",
    "    vals = framework.get_features(dataset[\"X\"], dataset[\"edge_index\"], dataset[\"val\"])\n",
    "    vals = [val.cpu() for val in vals]\n",
    "    print([clf.__class__.__name__ for clf in clfs])\n",
    "    kwargs_list=[{\"eval_set\":[(vals[i], y[dataset[\"val\"]])], \"early_stopping_rounds\":5} if clf.__class__.__name__ == 'XGBClassifier' else {} for i, clf in enumerate(clfs)]\n",
    "    print(kwargs_list)\n",
    "    framework.fit(dataset[\"X\"], dataset[\"edge_index\"], y, dataset[\"train\"], kwargs_list)\n",
    "    name_to_model[set_name] = framework\n",
    "    return framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78a265c4-b42c-4f6c-87a3-048377575ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def predict_dataset(set_name,framework):\n",
    "    dataset = name_to_sets[set_name]\n",
    "    \n",
    "    y = dataset[\"y\"]\n",
    "    framework = name_to_model[set_name]\n",
    "    pred = framework.predict(dataset[\"X\"], dataset[\"edge_index\"], dataset[\"test\"]) \n",
    "    pred_val = framework.predict(dataset[\"X\"], dataset[\"edge_index\"], dataset[\"val\"]) \n",
    "    y_test = y[dataset[\"test\"]]\n",
    "    y_val = y[dataset[\"val\"]]\n",
    "    print(set_name)\n",
    "    print(accuracy_score(y_val, pred_val))\n",
    "    print(accuracy_score(y_test, pred))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "889588e1-2e0d-4b9b-89aa-efd9fd75a815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LogisticRegression', 'LogisticRegression', 'LogisticRegression']\n",
      "[{}, {}, {}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/home/dwalke/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PubMed\n",
      "0.822\n",
      "0.79\n"
     ]
    }
   ],
   "source": [
    "store = store\n",
    "user_functions = [store[\"LogisticRegression\"][0][\"user_function\"], store[\"LogisticRegression\"][3][\"user_function\"], store[\"LogisticRegression\"][8][\"user_function\"]]\n",
    "clfs = [store[\"LogisticRegression\"][0][\"model\"], store[\"LogisticRegression\"][3][\"model\"], store[\"LogisticRegression\"][8][\"model\"]]\n",
    "attention_configs = [store[\"LogisticRegression\"][0][\"attention_config\"], store[\"LogisticRegression\"][3][\"attention_config\"], store[\"LogisticRegression\"][8][\"attention_config\"]]\n",
    "\n",
    "# user_functions = [store[\"LogisticRegression\"][3][\"user_function\"]]\n",
    "# clfs = [store[\"LogisticRegression\"][3][\"model\"]]\n",
    "# attention_configs = [store[\"LogisticRegression\"][3][\"attention_config\"]]\n",
    "framework = fit_dataset(PUBMED, user_functions=user_functions, hops_list= [3, 8], clfs = clfs, attention_configs= attention_configs)\n",
    "predict_dataset(PUBMED, framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "c5d5308e-9368-47c9-8a18-f75990e6fdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': {0: {'train_acc': 1.0,\n",
       "   'val_acc': 0.594,\n",
       "   'test_acc': 0.615,\n",
       "   'model': LogisticRegression(C=3.9057765563512103, l1_ratio=0.3841472539205063,\n",
       "                      max_iter=512, tol=0.00037394547447174774),\n",
       "   'user_function': <function __main__.user_function(kwargs)>,\n",
       "   'attention_config': {'inter_layer_normalize': True,\n",
       "    'use_pseudo_attention': True,\n",
       "    'cosine_eps': 0.01,\n",
       "    'dropout_attn': None}},\n",
       "  3: {'train_acc': 0.95,\n",
       "   'val_acc': 0.716,\n",
       "   'test_acc': 0.721,\n",
       "   'model': LogisticRegression(C=3.161729301367482, l1_ratio=0.41400664400322995,\n",
       "                      max_iter=128, tol=0.0022154364103027916),\n",
       "   'user_function': <function __main__.user_function(kwargs)>,\n",
       "   'attention_config': {'inter_layer_normalize': False,\n",
       "    'use_pseudo_attention': True,\n",
       "    'cosine_eps': 0.01,\n",
       "    'dropout_attn': None}},\n",
       "  8: {'train_acc': 0.9583333333333334,\n",
       "   'val_acc': 0.738,\n",
       "   'test_acc': 0.721,\n",
       "   'model': LogisticRegression(C=4.484384767955908, l1_ratio=0.31672332033998696,\n",
       "                      max_iter=16384, tol=0.000418404361811845),\n",
       "   'user_function': <function __main__.user_function(kwargs)>,\n",
       "   'attention_config': {'inter_layer_normalize': False,\n",
       "    'use_pseudo_attention': True,\n",
       "    'cosine_eps': 0.01,\n",
       "    'dropout_attn': None}}}}"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citeseer_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "2c4a69c9-43d5-4129-83b6-cb9631fa37e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LogisticRegression', 'LogisticRegression', 'LogisticRegression']\n",
      "[{}, {}, {}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/home/dwalke/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/home/dwalke/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citeseer\n",
      "0.738\n",
      "0.72\n"
     ]
    }
   ],
   "source": [
    "store = citeseer_store\n",
    "user_functions = [store[\"LogisticRegression\"][0][\"user_function\"], store[\"LogisticRegression\"][3][\"user_function\"], store[\"LogisticRegression\"][8][\"user_function\"]]\n",
    "clfs = [store[\"LogisticRegression\"][0][\"model\"], store[\"LogisticRegression\"][3][\"model\"], store[\"LogisticRegression\"][8][\"model\"]]\n",
    "attention_configs = [store[\"LogisticRegression\"][0][\"attention_config\"], store[\"LogisticRegression\"][3][\"attention_config\"], store[\"LogisticRegression\"][8][\"attention_config\"]]\n",
    "framework = fit_dataset(CITESEER,user_functions=user_functions, hops_list= [0,3,8], clfs = clfs, attention_configs= attention_configs)\n",
    "predict_dataset(CITESEER, framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b44c2bc4-4ac8-4884-8e48-b33baa158ca7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m permutation_importance\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mnew_train_features\u001b[49m)):\n\u001b[1;32m      4\u001b[0m     r \u001b[38;5;241m=\u001b[39m permutation_importance(model, new_val_features[model_i]\u001b[38;5;241m.\u001b[39mcpu(), val_set\u001b[38;5;241m.\u001b[39my,\n\u001b[1;32m      5\u001b[0m                             n_repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m      6\u001b[0m                             random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(r\u001b[38;5;241m.\u001b[39mimportances_mean[i])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_train_features' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "for model_i in range(len(new_train_features)):\n",
    "    r = permutation_importance(model, new_val_features[model_i].cpu(), val_set.y,\n",
    "                            n_repeats=30,\n",
    "                            random_state=0)\n",
    "    print(r.importances_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccf3be86-c2e8-4378-bae4-52e1965dc19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['XGBClassifier', 'XGBClassifier', 'SVC']\n",
      "[{'eval_set': [(tensor([[0.0000, 0.0000, 0.0159,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0133,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0533,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1080, 0.0000, 0.0075,  ..., 0.0000, 0.0000, 0.0000]]), tensor([2, 2, 1, 2, 1, 1, 2, 1, 0, 1, 0, 2, 0, 2, 1, 2, 2, 0, 0, 0, 0, 0, 1, 2,\n",
      "        1, 2, 1, 2, 1, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 1, 1, 0, 2, 2, 2, 1, 0, 2,\n",
      "        2, 2, 0, 2, 1, 1, 1, 0, 2, 2, 1, 1, 1, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 2,\n",
      "        0, 0, 1, 2, 0, 2, 1, 0, 1, 0, 0, 2, 2, 2, 2, 1, 2, 2, 1, 0, 2, 0, 1, 0,\n",
      "        2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 0, 2, 2, 2, 1, 2,\n",
      "        1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 2, 2, 2, 2, 1, 1,\n",
      "        1, 2, 0, 0, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 2, 2, 2, 2,\n",
      "        2, 2, 0, 1, 1, 0, 0, 1, 2, 2, 2, 2, 0, 1, 2, 1, 0, 1, 2, 2, 0, 2, 1, 2,\n",
      "        0, 1, 1, 0, 2, 0, 2, 2, 2, 0, 0, 1, 2, 2, 1, 1, 2, 0, 2, 2, 2, 1, 2, 1,\n",
      "        1, 1, 2, 1, 1, 2, 2, 2, 1, 0, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 0, 0, 2, 1,\n",
      "        1, 0, 1, 2, 1, 0, 2, 1, 2, 0, 1, 2, 2, 1, 1, 2, 2, 2, 0, 1, 2, 1, 2, 0,\n",
      "        1, 1, 2, 1, 0, 1, 2, 2, 1, 2, 0, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1,\n",
      "        2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 0, 0, 2, 1, 2, 2, 1, 1, 1,\n",
      "        0, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 0, 1, 0, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2,\n",
      "        2, 1, 0, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 0, 2, 1, 2, 0, 1, 0, 1, 1, 2, 2,\n",
      "        2, 2, 2, 1, 2, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 2, 2, 2,\n",
      "        2, 1, 1, 1, 1, 0, 2, 1, 1, 0, 0, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1,\n",
      "        2, 2, 0, 2, 2, 2, 2, 2, 0, 1, 1, 2, 0, 2, 0, 2, 0, 2, 1, 1, 2, 2, 1, 2,\n",
      "        0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 1, 2, 2, 2, 2, 0, 2, 0, 1, 1, 2, 1, 2, 2,\n",
      "        0, 0, 1, 0, 2, 1, 1, 2, 1, 2, 1, 0, 0, 0, 1, 0, 2, 2, 1, 2, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 1, 2, 1, 1, 2, 0, 0, 2, 2, 0, 2, 1, 1, 2, 2, 0]))], 'early_stopping_rounds': 5}, {'eval_set': [(tensor([[2.1490e-05, 9.7189e-03, 1.8505e-02,  ..., 1.5455e-06, 4.8649e-05,\n",
      "         1.2679e-04],\n",
      "        [0.0000e+00, 1.9430e-03, 2.8307e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [3.0513e-05, 6.7452e-03, 1.0019e-02,  ..., 1.2331e-03, 7.3271e-06,\n",
      "         1.3540e-05],\n",
      "        ...,\n",
      "        [5.2733e-03, 2.4546e-04, 1.0026e-01,  ..., 3.9593e-06, 0.0000e+00,\n",
      "         6.7279e-06],\n",
      "        [0.0000e+00, 1.5395e-02, 1.9374e-03,  ..., 1.8637e-03, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [4.3240e-02, 2.3419e-03, 1.6838e-02,  ..., 3.0422e-04, 0.0000e+00,\n",
      "         0.0000e+00]]), tensor([2, 2, 1, 2, 1, 1, 2, 1, 0, 1, 0, 2, 0, 2, 1, 2, 2, 0, 0, 0, 0, 0, 1, 2,\n",
      "        1, 2, 1, 2, 1, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 1, 1, 0, 2, 2, 2, 1, 0, 2,\n",
      "        2, 2, 0, 2, 1, 1, 1, 0, 2, 2, 1, 1, 1, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 2,\n",
      "        0, 0, 1, 2, 0, 2, 1, 0, 1, 0, 0, 2, 2, 2, 2, 1, 2, 2, 1, 0, 2, 0, 1, 0,\n",
      "        2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 0, 2, 2, 2, 1, 2,\n",
      "        1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 2, 2, 2, 2, 1, 1,\n",
      "        1, 2, 0, 0, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 2, 2, 2, 2,\n",
      "        2, 2, 0, 1, 1, 0, 0, 1, 2, 2, 2, 2, 0, 1, 2, 1, 0, 1, 2, 2, 0, 2, 1, 2,\n",
      "        0, 1, 1, 0, 2, 0, 2, 2, 2, 0, 0, 1, 2, 2, 1, 1, 2, 0, 2, 2, 2, 1, 2, 1,\n",
      "        1, 1, 2, 1, 1, 2, 2, 2, 1, 0, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 0, 0, 2, 1,\n",
      "        1, 0, 1, 2, 1, 0, 2, 1, 2, 0, 1, 2, 2, 1, 1, 2, 2, 2, 0, 1, 2, 1, 2, 0,\n",
      "        1, 1, 2, 1, 0, 1, 2, 2, 1, 2, 0, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1,\n",
      "        2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 0, 0, 2, 1, 2, 2, 1, 1, 1,\n",
      "        0, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 0, 1, 0, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2,\n",
      "        2, 1, 0, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 0, 2, 1, 2, 0, 1, 0, 1, 1, 2, 2,\n",
      "        2, 2, 2, 1, 2, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 2, 2, 2,\n",
      "        2, 1, 1, 1, 1, 0, 2, 1, 1, 0, 0, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1,\n",
      "        2, 2, 0, 2, 2, 2, 2, 2, 0, 1, 1, 2, 0, 2, 0, 2, 0, 2, 1, 1, 2, 2, 1, 2,\n",
      "        0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 1, 2, 2, 2, 2, 0, 2, 0, 1, 1, 2, 1, 2, 2,\n",
      "        0, 0, 1, 0, 2, 1, 1, 2, 1, 2, 1, 0, 0, 0, 1, 0, 2, 2, 1, 2, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 1, 2, 1, 1, 2, 0, 0, 2, 2, 0, 2, 1, 1, 2, 2, 0]))], 'early_stopping_rounds': 5}, {}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:0.98416\n",
      "[1]\tvalidation_0-mlogloss:0.92272\n",
      "[2]\tvalidation_0-mlogloss:0.88664\n",
      "[3]\tvalidation_0-mlogloss:0.84306\n",
      "[4]\tvalidation_0-mlogloss:0.82333\n",
      "[5]\tvalidation_0-mlogloss:0.81140\n",
      "[6]\tvalidation_0-mlogloss:0.78062\n",
      "[7]\tvalidation_0-mlogloss:0.77776\n",
      "[8]\tvalidation_0-mlogloss:0.76773\n",
      "[9]\tvalidation_0-mlogloss:0.75974\n",
      "[10]\tvalidation_0-mlogloss:0.74930\n",
      "[11]\tvalidation_0-mlogloss:0.75331\n",
      "[12]\tvalidation_0-mlogloss:0.75462\n",
      "[13]\tvalidation_0-mlogloss:0.73632\n",
      "[14]\tvalidation_0-mlogloss:0.73620\n",
      "[15]\tvalidation_0-mlogloss:0.72862\n",
      "[16]\tvalidation_0-mlogloss:0.73010\n",
      "[17]\tvalidation_0-mlogloss:0.73777\n",
      "[18]\tvalidation_0-mlogloss:0.74450\n",
      "[19]\tvalidation_0-mlogloss:0.74320\n",
      "[0]\tvalidation_0-mlogloss:1.01912\n",
      "[1]\tvalidation_0-mlogloss:0.89476\n",
      "[2]\tvalidation_0-mlogloss:0.81348\n",
      "[3]\tvalidation_0-mlogloss:0.75490\n",
      "[4]\tvalidation_0-mlogloss:0.70351\n",
      "[5]\tvalidation_0-mlogloss:0.67500\n",
      "[6]\tvalidation_0-mlogloss:0.65043\n",
      "[7]\tvalidation_0-mlogloss:0.64049\n",
      "[8]\tvalidation_0-mlogloss:0.62562\n",
      "[9]\tvalidation_0-mlogloss:0.61488\n",
      "[10]\tvalidation_0-mlogloss:0.60998\n",
      "[11]\tvalidation_0-mlogloss:0.58454\n",
      "[12]\tvalidation_0-mlogloss:0.56873\n",
      "[13]\tvalidation_0-mlogloss:0.56365\n",
      "[14]\tvalidation_0-mlogloss:0.56055\n",
      "[15]\tvalidation_0-mlogloss:0.55886\n",
      "[16]\tvalidation_0-mlogloss:0.55700\n",
      "[17]\tvalidation_0-mlogloss:0.56072\n",
      "[18]\tvalidation_0-mlogloss:0.54642\n",
      "[19]\tvalidation_0-mlogloss:0.54481\n",
      "[20]\tvalidation_0-mlogloss:0.54138\n",
      "[21]\tvalidation_0-mlogloss:0.54117\n",
      "[22]\tvalidation_0-mlogloss:0.54380\n",
      "[23]\tvalidation_0-mlogloss:0.54849\n",
      "[24]\tvalidation_0-mlogloss:0.54521\n",
      "[25]\tvalidation_0-mlogloss:0.54306\n",
      "[26]\tvalidation_0-mlogloss:0.54106\n",
      "[27]\tvalidation_0-mlogloss:0.54245\n",
      "[28]\tvalidation_0-mlogloss:0.54069\n",
      "[29]\tvalidation_0-mlogloss:0.54014\n",
      "[30]\tvalidation_0-mlogloss:0.54141\n",
      "[31]\tvalidation_0-mlogloss:0.54237\n",
      "[32]\tvalidation_0-mlogloss:0.54139\n",
      "[33]\tvalidation_0-mlogloss:0.54128\n",
      "PubMed\n",
      "0.81\n",
      "0.814\n"
     ]
    }
   ],
   "source": [
    "fit_dataset(PUBMED)\n",
    "predict_dataset(PUBMED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
