{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52ad701-071b-4c9f-9bc8-a0da062c9c8e",
   "metadata": {},
   "source": [
    "# Graph Aware evaluation on Core, CiteSeer and PubMed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45afa977-54bf-45de-be73-5b1f5fffc97c",
   "metadata": {},
   "source": [
    "## Load font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6361a3a5-c7cd-478d-ba66-9acacb684235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/share/fonts/truetype/msttcorefonts/Arial.ttf\n"
     ]
    }
   ],
   "source": [
    "from pylab import *\n",
    "from math import sin\n",
    "rc('text', usetex = False)\n",
    "la = matplotlib.font_manager.FontManager()\n",
    "lu = matplotlib.font_manager.FontProperties(family = 'Arial')\n",
    "print(la.findfont(lu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9426710-16b8-4651-a531-33eb87bcc074",
   "metadata": {},
   "source": [
    "## Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f39ca8-9364-4636-b9be-5b25e76d0d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Cora': {'X': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       "  'y': tensor([3, 4, 4,  ..., 3, 3, 3]),\n",
       "  'test': tensor([False, False, False,  ...,  True,  True,  True]),\n",
       "  'train': tensor([ True,  True,  True,  ..., False, False, False]),\n",
       "  'val': tensor([False, False, False,  ..., False, False, False]),\n",
       "  'edge_index': tensor([[   0,    0,    0,  ..., 2705, 2706, 2707],\n",
       "          [ 633, 1862, 2582,  ..., 2705, 2706, 2707]])},\n",
       " 'PubMed': {'X': tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0554, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0114, 0.0047,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0531, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       "  'y': tensor([1, 1, 0,  ..., 2, 0, 2]),\n",
       "  'test': tensor([False, False, False,  ...,  True,  True,  True]),\n",
       "  'train': tensor([ True,  True,  True,  ..., False, False, False]),\n",
       "  'val': tensor([False, False, False,  ..., False, False, False]),\n",
       "  'edge_index': tensor([[    0,     0,     0,  ..., 19714, 19715, 19716],\n",
       "          [ 1378,  1544,  6092,  ..., 19714, 19715, 19716]])},\n",
       " 'Citeseer': {'X': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       "  'y': tensor([3, 1, 5,  ..., 3, 1, 5]),\n",
       "  'test': tensor([False, False, False,  ...,  True,  True,  True]),\n",
       "  'train': tensor([ True,  True,  True,  ..., False, False, False]),\n",
       "  'val': tensor([False, False, False,  ..., False, False, False]),\n",
       "  'edge_index': tensor([[   0,    1,    1,  ..., 3324, 3325, 3326],\n",
       "          [ 628,  158,  486,  ..., 3324, 3325, 3326]])}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "citeseer_dataset = Planetoid(root = \"./data\", name = \"CiteSeer\")\n",
    "cora_dataset = Planetoid(root = \"./data\", name = \"Cora\")\n",
    "pubmed_dataset = Planetoid(root = \"./data\", name = \"PubMed\")\n",
    "\n",
    "def pre_process(dataset):\n",
    "    dataset.transform = T.NormalizeFeatures()\n",
    "    return dataset\n",
    "\n",
    "def add_set(set_name):\n",
    "    global name_to_sets, name_to_dataset\n",
    "    name_to_sets[set_name] = dict({})\n",
    "    \n",
    "    dataset = name_to_dataset[set_name]\n",
    "    X =  dataset[0].x \n",
    "    y =  dataset[0].y \n",
    "    \n",
    "    test =  dataset[0].test_mask\n",
    "    train = dataset[0].train_mask \n",
    "    val =  dataset[0].val_mask\n",
    "    \n",
    "    edge_index = add_self_loops(dataset[0].edge_index)[0]\n",
    "\n",
    "    name_to_sets[set_name][\"X\"] = X\n",
    "    name_to_sets[set_name][\"y\"] = y\n",
    "    name_to_sets[set_name][\"test\"] = test\n",
    "    name_to_sets[set_name][\"train\"] = train\n",
    "    name_to_sets[set_name][\"val\"] = val\n",
    "    name_to_sets[set_name][\"edge_index\"] = edge_index\n",
    "\n",
    "def create_sets():\n",
    "    for set_name in name_to_dataset.keys():\n",
    "        add_set(set_name)\n",
    "\n",
    "\n",
    "CORA = \"Cora\"\n",
    "PUBMED = \"PubMed\"\n",
    "CITESEER = \"Citeseer\"\n",
    "\n",
    "name_to_dataset = dict({})\n",
    "name_to_dataset[CORA] = pre_process(cora_dataset)\n",
    "name_to_dataset[PUBMED] = pre_process(pubmed_dataset)\n",
    "name_to_dataset[CITESEER] = pre_process(citeseer_dataset)\n",
    "name_to_sets = dict({})\n",
    "create_sets()\n",
    "name_to_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "724bb15a-0830-441f-9903-9b2341cd12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp,STATUS_OK, SparkTrials, space_eval \n",
    "from sklearn.model_selection import KFold\n",
    "from EnsembleFramework import Framework\n",
    "\n",
    "def user_function(kwargs):\n",
    "    return  kwargs[\"original_features\"] + kwargs[\"summed_neighbors\"]\n",
    "    \n",
    "DEF_USER_FUNCTIONS = [user_function]\n",
    "DEF_ATTENTION_CONFIGS= [{'inter_layer_normalize': False,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.01,\n",
    "                     'dropout_attn': None}]\n",
    "def index_to_mask(rows, index_array):\n",
    "    mask_array = np.zeros(rows, dtype=int)\n",
    "    mask_array[index_array] = 1\n",
    "    return mask_array.astype(np.bool_)\n",
    "\n",
    "class AutoTuneCVTransductive:\n",
    "    def __init__(self,clf, k, outer_train_index, evaluate, hop, max_evals = 100, parallelism = 1, n_jobs = 1):\n",
    "        self.n_jobs = n_jobs\n",
    "        self.max_evals = max_evals\n",
    "        self.parallelism = parallelism\n",
    "        self.data = None\n",
    "        self.clf = clf\n",
    "        self.outer_train_index = outer_train_index\n",
    "        self.scores = np.zeros(k)\n",
    "        self.kf = KFold(n_splits=k)\n",
    "        self.evaluate = evaluate\n",
    "        self.hop = hop\n",
    "\n",
    "    def objective(self, params):        \n",
    "        outer_train_mask = index_to_mask(self.data.X.shape[0], self.outer_train_index)\n",
    "        for i, (train_index, test_index) in enumerate(self.kf.split(self.data.X[outer_train_mask])): ##self.outer_train_index\n",
    "\n",
    "            inner_train_mask = index_to_mask(self.data.X.shape[0], self.outer_train_index[train_index])\n",
    "            inner_test_mask = index_to_mask(self.data.X.shape[0], self.outer_train_index[test_index])\n",
    "\n",
    "            user_function = params.pop('user_function', DEF_USER_FUNCTIONS[0])\n",
    "            attention_config = params.pop('attention_config', DEF_ATTENTION_CONFIGS[0])\n",
    "            model = self.clf(**params)\n",
    "            \n",
    "            framework = Framework(user_functions=[user_function], \n",
    "                             hops_list=[self.hop],\n",
    "                             clfs=[model],\n",
    "                             gpu_idx=0,\n",
    "                             handle_nan=0.0,\n",
    "                            attention_configs=[attention_config])\n",
    "            framework.fit(self.data.X, self.data.edge_index,\n",
    "                          self.data.y, inner_train_mask, kwargs_multi_clf_list = [{\"n_jobs\":self.n_jobs}])\n",
    "            \n",
    "            pred_proba = framework.predict_proba(self.data.X, self.data.edge_index, inner_test_mask)\n",
    "            score = self.evaluate(self.data.y[inner_test_mask], pred_proba)\n",
    "            self.scores[i] = score\n",
    "        return {'loss': -self.scores.mean(), 'status': STATUS_OK}\n",
    "    \n",
    "    def forward(self, graph, space):\n",
    "        self.data = graph\n",
    "        spark_trials = SparkTrials(parallelism = self.parallelism)\n",
    "        best_params = fmin(self.objective, space, algo=tpe.suggest, max_evals=self.max_evals, trials=spark_trials, verbose = False)\n",
    "        return space_eval(space, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cb81171-b5a3-4406-8e13-c460ecf3bb03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from hyperopt import hp\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn.functional import normalize\n",
    "from hyperopt import space_eval\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class Data():\n",
    "    def __init__(self, X, y, edge_index):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"\"\"Custom Data Object X: {self.X.shape},y: {self.y.shape}, edge_index: {self.edge_index.shape}\"\"\"\n",
    "        \n",
    "class NestedCVTransductive:\n",
    "    def __init__(self, clf,space,evaluate_fun, graph = name_to_sets[\"Cora\"], hops = [0,1,2], n_jobs = 1, max_evals = 100, parallelism = 1, iterations = 20):\n",
    "        self.clf = clf\n",
    "        self.hops = hops\n",
    "        self.space = space\n",
    "        self.best_clf = None\n",
    "        self.graph = NestedCVTransductive.parse_data(graph)\n",
    "        self.n_jobs = n_jobs\n",
    "        self.evaluate = evaluate_fun\n",
    "        self.scores = None\n",
    "        self.auto_tune_cvs = []\n",
    "        self.max_evals = max_evals\n",
    "        self.parallelism = parallelism\n",
    "        self.iterations = iterations\n",
    "        self.scores_iters = []\n",
    "\n",
    "    def forward(self, k, k_inner):\n",
    "        kf_outer = KFold(n_splits=k)\n",
    "        \n",
    "        self.scores = np.zeros(k)\n",
    "        for i, (train_index, test_index) in tqdm(enumerate(kf_outer.split(self.graph.X))):\n",
    "            user_functions = []\n",
    "            attention_configs = []\n",
    "            models = []\n",
    "            for hop in self.hops:\n",
    "                auto_tune_cv = AutoTuneCVTransductive(self.clf, k_inner, train_index, self.evaluate, hop = hop, max_evals = self.max_evals,  parallelism =self.parallelism, n_jobs = self.n_jobs)\n",
    "                best_params = auto_tune_cv.forward(self.graph, self.space)\n",
    "                user_function = best_params.pop('user_function', DEF_USER_FUNCTIONS[0])\n",
    "                attention_config = best_params.pop('attention_config', DEF_ATTENTION_CONFIGS[0])\n",
    "                model = self.clf(**best_params)\n",
    "                attention_configs.append(attention_config)\n",
    "                user_functions.append(user_function)\n",
    "                models.append(model)\n",
    "                self.auto_tune_cvs.append(auto_tune_cv)\n",
    "\n",
    "            ## TODO: Here iterate N Times for random state initiaization\n",
    "            scores = np.zeros(self.iterations)\n",
    "            for iter_idx in range(self.iterations):\n",
    "                framework = Framework(user_functions=user_functions, \n",
    "                                 hops_list=self.hops,\n",
    "                                 clfs=models,\n",
    "                                 gpu_idx=0,\n",
    "                                 handle_nan=0.0,\n",
    "                                attention_configs=attention_configs)\n",
    "                train_mask = index_to_mask(self.graph.X.shape[0], train_index)\n",
    "                framework.fit(self.graph.X, self.graph.edge_index,\n",
    "                              self.graph.y, train_mask, kwargs_multi_clf_list = [{\"n_jobs\":self.n_jobs}])\n",
    "                \n",
    "                test_mask = index_to_mask(self.graph.X.shape[0], test_index)\n",
    "                # assert test_mask != train_mask, \"Assertion failed\"\n",
    "                pred_proba = framework.predict_proba(self.graph.X, self.graph.edge_index, test_mask) \n",
    "                scores[iter_idx] = self.evaluate(self.graph.y[test_mask], pred_proba)\n",
    "                print(scores)\n",
    "            self.scores[i] = scores.mean()\n",
    "            self.scores_iters.append(scores)\n",
    "            clear_output(wait=True)\n",
    "        return self.scores.mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_data(dataset):\n",
    "        data = Data(dataset[\"X\"], dataset[\"y\"], dataset[\"edge_index\"])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ce008-c038-40ac-914a-239a94f3f0c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/dwalke/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/dwalke/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/dwalke/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/dwalke/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Stage 15305:>              (0 + 1) / 1][Stage 15306:>              (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "def norm_user_function(kwargs):\n",
    "    return  normalize(kwargs[\"original_features\"] + kwargs[\"summed_neighbors\"], p=2.0, dim = 1)\n",
    "    \n",
    "def user_function(kwargs):\n",
    "    return  kwargs[\"original_features\"] + kwargs[\"summed_neighbors\"]\n",
    "    \n",
    "lr_choices = {\n",
    "    'penalty': [\"l2\"],\n",
    "    'max_iter': [2**i for i in range(6, 15)],\n",
    "    'user_function': [norm_user_function, user_function],\n",
    "    'attention_config':  [None,{'inter_layer_normalize': False,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.01,\n",
    "                     'dropout_attn': None}, \n",
    "                     {'inter_layer_normalize': True,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.01,\n",
    "                     'dropout_attn': None},\n",
    "                     {'inter_layer_normalize': True,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.001,\n",
    "                     'dropout_attn': None}],\n",
    "}\n",
    "\n",
    "space_lr = {\n",
    "    **{key: hp.choice(key, value) for key, value in lr_choices.items()},\n",
    "    'tol': hp.loguniform('tol', -5, -3),\n",
    "    'C': hp.uniform('C', 0.0, 10)\n",
    "}\n",
    "\n",
    "def evaluate_fun(y, pred_proba):\n",
    "    return accuracy_score(y, pred_proba.argmax(1))\n",
    "\n",
    "\n",
    "nested_cv_transd_new = NestedCVTransductive(LogisticRegression,space_lr,evaluate_fun, n_jobs = 2,parallelism=24, hops = [0, 3, 8], iterations= 1, max_evals=50)\n",
    "from IPython.utils import io\n",
    "\n",
    "with io.capture_output() as captured:\n",
    "    nested_cv_transd_new.forward(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980bd26-ef9a-4dd1-a2dd-eb2230515d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce81790e-dcf0-44e2-a9ba-4655f5fb144f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8934497354497354, 0.06880104689754228)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_cv_transd.scores.mean(), nested_cv_transd.scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd2fd0d2-d243-4e31-a963-be633bc70538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89285714, 0.82142857, 0.82142857, 0.89285714, 0.96428571,\n",
       "       0.96428571, 0.96428571, 0.96428571, 0.88888889, 0.92592593,\n",
       "       0.88888889, 0.88888889, 0.88888889, 0.88888889, 1.        ,\n",
       "       0.74074074, 0.92592593, 0.77777778, 0.85185185, 0.81481481,\n",
       "       0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.85185185,\n",
       "       0.85185185, 0.85185185, 0.77777778, 0.88888889, 0.92592593,\n",
       "       0.81481481, 0.74074074, 0.85185185, 1.        , 0.92592593,\n",
       "       0.92592593, 0.92592593, 0.88888889, 0.92592593, 0.77777778,\n",
       "       0.92592593, 0.96296296, 1.        , 0.96296296, 0.85185185,\n",
       "       0.81481481, 0.96296296, 0.88888889, 0.88888889, 0.81481481,\n",
       "       0.88888889, 0.88888889, 0.96296296, 0.92592593, 1.        ,\n",
       "       0.88888889, 0.96296296, 0.96296296, 0.85185185, 0.92592593,\n",
       "       0.92592593, 0.85185185, 0.96296296, 0.92592593, 0.96296296,\n",
       "       0.92592593, 0.88888889, 0.92592593, 0.88888889, 1.        ,\n",
       "       0.74074074, 0.92592593, 0.96296296, 0.77777778, 0.66666667,\n",
       "       0.85185185, 0.85185185, 0.88888889, 0.88888889, 0.88888889,\n",
       "       0.81481481, 0.81481481, 0.92592593, 0.92592593, 0.92592593,\n",
       "       0.85185185, 0.96296296, 0.92592593, 0.77777778, 0.74074074,\n",
       "       0.96296296, 0.85185185, 1.        , 0.96296296, 1.        ,\n",
       "       0.92592593, 0.85185185, 0.92592593, 0.91111111, 1.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_cv_transd.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "944a6053-daa3-42e3-98af-0c3c217c999f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.89285714, 0.89285714, 0.89285714, 0.89285714, 0.89285714]),\n",
       " array([0.82142857, 0.82142857, 0.82142857, 0.82142857, 0.82142857]),\n",
       " array([0.82142857, 0.82142857, 0.82142857, 0.82142857, 0.82142857]),\n",
       " array([0.89285714, 0.89285714, 0.89285714, 0.89285714, 0.89285714]),\n",
       " array([0.96428571, 0.96428571, 0.96428571, 0.96428571, 0.96428571]),\n",
       " array([0.96428571, 0.96428571, 0.96428571, 0.96428571, 0.96428571]),\n",
       " array([0.96428571, 0.96428571, 0.96428571, 0.96428571, 0.96428571]),\n",
       " array([0.96428571, 0.96428571, 0.96428571, 0.96428571, 0.96428571]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([1., 1., 1., 1., 1.]),\n",
       " array([0.74074074, 0.74074074, 0.74074074, 0.74074074, 0.74074074]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778]),\n",
       " array([0.85185185, 0.85185185, 0.85185185, 0.85185185, 0.85185185]),\n",
       " array([0.81481481, 0.81481481, 0.81481481, 0.81481481, 0.81481481]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.85185185, 0.85185185, 0.85185185, 0.85185185, 0.85185185]),\n",
       " array([0.85185185, 0.85185185, 0.85185185, 0.85185185, 0.85185185]),\n",
       " array([0.85185185, 0.85185185, 0.85185185, 0.85185185, 0.85185185]),\n",
       " array([0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.81481481, 0.81481481, 0.81481481, 0.81481481, 0.81481481]),\n",
       " array([0.74074074, 0.74074074, 0.74074074, 0.74074074, 0.74074074]),\n",
       " array([0.85185185, 0.85185185, 0.85185185, 0.85185185, 0.85185185]),\n",
       " array([1., 1., 1., 1., 1.]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296]),\n",
       " array([1., 1., 1., 1., 1.]),\n",
       " array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296]),\n",
       " array([0.85185185, 0.85185185, 0.85185185, 0.85185185, 0.85185185]),\n",
       " array([0.81481481, 0.81481481, 0.81481481, 0.81481481, 0.81481481]),\n",
       " array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.81481481, 0.81481481, 0.81481481, 0.81481481, 0.81481481]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([1., 1., 1., 1., 1.]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296]),\n",
       " array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296]),\n",
       " array([0.85185185, 0.85185185, 0.85185185, 0.85185185, 0.85185185]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.85185185, 0.85185185, 0.85185185, 0.85185185, 0.85185185]),\n",
       " array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([1., 1., 1., 1., 1.]),\n",
       " array([0.74074074, 0.74074074, 0.74074074, 0.74074074, 0.74074074]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296]),\n",
       " array([0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778]),\n",
       " array([0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.66666667]),\n",
       " array([0.85185185, 0.85185185, 0.85185185, 0.85185185, 0.85185185]),\n",
       " array([0.85185185, 0.85185185, 0.85185185, 0.85185185, 0.85185185]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]),\n",
       " array([0.81481481, 0.81481481, 0.81481481, 0.81481481, 0.81481481]),\n",
       " array([0.81481481, 0.81481481, 0.81481481, 0.81481481, 0.81481481]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.85185185, 0.85185185, 0.85185185, 0.85185185, 0.85185185]),\n",
       " array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778]),\n",
       " array([0.74074074, 0.74074074, 0.74074074, 0.74074074, 0.74074074]),\n",
       " array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296]),\n",
       " array([0.85185185, 0.85185185, 0.85185185, 0.85185185, 0.85185185]),\n",
       " array([1., 1., 1., 1., 1.]),\n",
       " array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296]),\n",
       " array([1., 1., 1., 1., 1.]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.85185185, 0.85185185, 0.85185185, 0.85185185, 0.85185185]),\n",
       " array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593]),\n",
       " array([0.92592593, 0.88888889, 0.88888889, 0.92592593, 0.92592593]),\n",
       " array([1., 1., 1., 1., 1.])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_cv_transd.scores_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "030518a5-e9ec-4704-89e4-424886d71677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_cv_transd.auto_tune_cvs[10].scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e617389a-d7e9-4344-a953-7effac79197b",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a99fd-af4f-4f72-ab9d-9719ef84db11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from hyperopt import hp\n",
    "from AutoTune2 import AutoSearch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "lr_choices = {\n",
    "    'penalty': [\"l2\"],\n",
    "    'max_iter': [2**i for i in range(6, 15)],\n",
    "    \n",
    "}\n",
    "\n",
    "space_lr = {\n",
    "    **{key: hp.choice(key, value) for key, value in lr_choices.items()},\n",
    "    'tol': hp.loguniform('tol', -11, -3),\n",
    "    'C': hp.uniform('C', 0.0, 10)\n",
    "}\n",
    "\n",
    "def norm_user_function(kwargs):\n",
    "    return  normalize(kwargs[\"original_features\"] + kwargs[\"summed_neighbors\"], p=2.0, dim = 1)\n",
    "    \n",
    "def user_function(kwargs):\n",
    "    return  kwargs[\"original_features\"] + kwargs[\"summed_neighbors\"]\n",
    "    \n",
    "\n",
    "hops = [0,3,8]\n",
    "clfs = [LogisticRegression]\n",
    "clfs_space = dict({})\n",
    "clfs_space[\"LogisticRegression\"] = space_lr\n",
    "attention_configs = [None,{'inter_layer_normalize': False,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.01,\n",
    "                     'dropout_attn': None}, \n",
    "                     {'inter_layer_normalize': True,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.01,\n",
    "                     'dropout_attn': None},\n",
    "                     {'inter_layer_normalize': True,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.001,\n",
    "                     'dropout_attn': None}]\n",
    "user_functions = [norm_user_function, user_function]\n",
    "searcher = AutoSearch(name_to_sets[CORA], max_evals=500, pred_metric = accuracy_score, parallelism=50)\n",
    "# store = searcher.search(clfs, clfs_space, hops=hops, user_functions= user_functions,\n",
    "#                         attention_configs = attention_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae299c02-84b6-4b82-b698-3f2d50bd6c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from hyperopt import hp\n",
    "from Auto_Tune_Comb import AutoSearch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn.functional import normalize\n",
    "lr_choices = {\n",
    "    'penalty': [\"l2\"],\n",
    "    'max_iter': [2**i for i in range(6, 15)],\n",
    "    \n",
    "}\n",
    "\n",
    "space_lr = {\n",
    "    **{key: hp.choice(key, value) for key, value in lr_choices.items()},\n",
    "    'tol': hp.loguniform('tol', -11, -3),\n",
    "    'C': hp.uniform('C', 0.0, 100)\n",
    "}\n",
    "\n",
    "lr_choices_1 = {\n",
    "    'penalty': [\"l2\"],\n",
    "    'max_iter': [2**i for i in range(6, 15)],\n",
    "    \n",
    "}\n",
    "\n",
    "space_lr_1 = {\n",
    "    **{key: hp.choice(f\"{key}_1\", value) for key, value in lr_choices_1.items()},\n",
    "    'tol': hp.loguniform('tol_1', -11, -3),\n",
    "    'C': hp.uniform('C_1', 0.0, 100)\n",
    "}\n",
    "\n",
    "lr_choices_2 = {\n",
    "    'penalty': [\"l2\"],\n",
    "    'max_iter': [2**i for i in range(6, 15)],\n",
    "    \n",
    "}\n",
    "\n",
    "space_lr_2 = {\n",
    "    **{key: hp.choice(f\"{key}_2\", value) for key, value in lr_choices_2.items()},\n",
    "    'tol': hp.loguniform('tol_2', -11, -3),\n",
    "    'C': hp.uniform('C_2', 0.0, 100)\n",
    "}\n",
    "\n",
    "def norm_user_function(kwargs):\n",
    "    return  normalize(kwargs[\"original_features\"] + kwargs[\"summed_neighbors\"], p=2.0, dim = 1)\n",
    "    \n",
    "def user_function(kwargs):\n",
    "    return  kwargs[\"original_features\"] + kwargs[\"summed_neighbors\"]\n",
    "    \n",
    "\n",
    "hops_list = [[0,3, 8]]\n",
    "clfs = [LogisticRegression]\n",
    "clfs_space = dict({})\n",
    "clfs_space[\"LogisticRegression\"] = [space_lr, space_lr_1,space_lr_2]\n",
    "attention_configs = [{'inter_layer_normalize': True,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.01,\n",
    "                     'dropout_attn': None}]\n",
    "user_functions = [user_function]\n",
    "searcher = AutoSearch(name_to_sets[PUBMED], max_evals=300, pred_metric = accuracy_score, parallelism=50)\n",
    "# store_cora = searcher.search(clfs, clfs_space, hops_list=hops_list, user_functions= user_functions,\n",
    "#                         attention_configs = attention_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80209899-bb5c-4b68-af73-969fbd966d95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from hyperopt import hp\n",
    "from Auto_Tune_Comb import AutoSearch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "lr_choices = {\n",
    "    'penalty': [\"l2\"],\n",
    "    'max_iter': [2**i for i in range(6, 15)],\n",
    "    \n",
    "}\n",
    "\n",
    "space_lr = {\n",
    "    **{key: hp.choice(key, value) for key, value in lr_choices.items()},\n",
    "    'tol': hp.loguniform('tol', -11, -3),\n",
    "    'C': hp.uniform('C', 0.0, 10)\n",
    "}\n",
    "\n",
    "def norm_user_function(kwargs):\n",
    "    return  normalize(kwargs[\"original_features\"] + kwargs[\"summed_neighbors\"], p=2.0, dim = 1)\n",
    "    \n",
    "def user_function(kwargs):\n",
    "    return  kwargs[\"original_features\"] + kwargs[\"summed_neighbors\"]\n",
    "    \n",
    "\n",
    "hops_list = [[0,3,6]]\n",
    "clfs = [LogisticRegression]\n",
    "clfs_space = dict({})\n",
    "clfs_space[\"LogisticRegression\"] = space_lr\n",
    "attention_configs = [{'inter_layer_normalize': False,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.01,\n",
    "                     'dropout_attn': None}, \n",
    "                     {'inter_layer_normalize': True,\n",
    "                     'use_pseudo_attention':True,\n",
    "                     'cosine_eps':.01,\n",
    "                     'dropout_attn': None}]\n",
    "user_functions = [user_function]\n",
    "searcher = AutoSearch(name_to_sets[CITESEER], max_evals=100, pred_metric = accuracy_score, parallelism=50)\n",
    "# store = searcher.search(clfs, clfs_space, hops_list=hops_list, user_functions= user_functions,\n",
    "#                         attention_configs = attention_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152bacf-9124-4f31-921c-e6cfc65844a5",
   "metadata": {},
   "source": [
    "## Generic fit function for GraphAware training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81b55b-cf71-4a91-adcb-f6fad70dffd5",
   "metadata": {},
   "source": [
    "## Dictionary for storing trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b02d09-8a18-47d2-bc37-7afb0e47ba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_model = dict({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dba0e8-64b4-47fc-a013-a079934d4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EnsembleFramework import Framework\n",
    "import time\n",
    "\n",
    "def fit_dataset(set_name,user_functions=[], hops_list= [3], clfs = [], attention_configs= []):\n",
    "    dataset = name_to_sets[set_name]\n",
    "    y = dataset[\"y\"]\n",
    "    \n",
    "    start = time.time()\n",
    "    framework = Framework(user_functions, \n",
    "                     hops_list=hops_list, ## to obtain best for local neighborhood\n",
    "                     clfs=clfs,\n",
    "                     gpu_idx=0,\n",
    "                     handle_nan=0.0,\n",
    "                    attention_configs=attention_configs)\n",
    "    vals = framework.get_features(dataset[\"X\"], dataset[\"edge_index\"], dataset[\"val\"])\n",
    "    vals = [val.cpu() for val in vals]\n",
    "    kwargs_list=[{\"eval_set\":[(vals[i], y[dataset[\"val\"]])], \"early_stopping_rounds\":5} if clf.__class__.__name__ == 'XGBClassifier' else {} for i, clf in enumerate(clfs)]\n",
    "    framework.fit(dataset[\"X\"], dataset[\"edge_index\"], y, dataset[\"train\"], kwargs_list)\n",
    "    end = time.time()-start\n",
    "    name_to_model[set_name] = framework\n",
    "    return framework, end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d54b4-487c-4bf8-9be2-accaa57bd46f",
   "metadata": {},
   "source": [
    "## Generic predict function for GraphAware evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a265c4-b42c-4f6c-87a3-048377575ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def predict_dataset(set_name,framework):\n",
    "    dataset = name_to_sets[set_name]\n",
    "    \n",
    "    y = dataset[\"y\"]\n",
    "    framework = name_to_model[set_name]\n",
    "    pred = framework.predict(dataset[\"X\"], dataset[\"edge_index\"], dataset[\"test\"]) \n",
    "    pred_val = framework.predict(dataset[\"X\"], dataset[\"edge_index\"], dataset[\"val\"]) \n",
    "    y_test = y[dataset[\"test\"]]\n",
    "    y_val = y[dataset[\"val\"]]\n",
    "    return {\n",
    "        \"test_acc\": accuracy_score(y_test, pred),\n",
    "        \"val_acc\": accuracy_score(y_val, pred_val)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec54227-0829-4d25-9b2b-543ff1630b06",
   "metadata": {},
   "source": [
    "## Citeseer Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb7163-63e7-4589-9de6-2c56962ea1e2",
   "metadata": {},
   "source": [
    "### CiteSeer Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d5308e-9368-47c9-8a18-f75990e6fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from AutoTune2 import upd_user_function, norm_user_function, user_function\n",
    "citeseer_store = {'LogisticRegression': {0: {'train_acc': 1.0,\n",
    "   'val_acc': 0.594,\n",
    "   'test_acc': 0.615,\n",
    "   'model': LogisticRegression(C=3.9057765563512103,\n",
    "                      max_iter=512, tol=0.00037394547447174774),\n",
    "   'user_function': user_function,\n",
    "   'attention_config': {'inter_layer_normalize': True,\n",
    "    'use_pseudo_attention': True,\n",
    "    'cosine_eps': 0.01,\n",
    "    'dropout_attn': None}},\n",
    "  3: {'train_acc': 0.95,\n",
    "   'val_acc': 0.716,\n",
    "   'test_acc': 0.721,\n",
    "   'model': LogisticRegression(C=3.161729301367482, \n",
    "                      max_iter=128, tol=0.0022154364103027916),\n",
    "   'user_function': user_function,\n",
    "   'attention_config': {'inter_layer_normalize': False,\n",
    "    'use_pseudo_attention': True,\n",
    "    'cosine_eps': 0.01,\n",
    "    'dropout_attn': None}},\n",
    "  8: {'train_acc': 0.9583333333333334,\n",
    "   'val_acc': 0.738,\n",
    "   'test_acc': 0.721,\n",
    "   'model': LogisticRegression(C=4.484384767955908,\n",
    "                      max_iter=16384, tol=0.000418404361811845),\n",
    "   'user_function': user_function,\n",
    "   'attention_config': {'inter_layer_normalize': False,\n",
    "    'use_pseudo_attention': True,\n",
    "    'cosine_eps': 0.01,\n",
    "    'dropout_attn': None}}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e6281-3911-4352-9ed8-4f5cfc76db4f",
   "metadata": {},
   "source": [
    "### GraphAware on CiteSeer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a69c9-43d5-4129-83b6-cb9631fa37e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "store = citeseer_store\n",
    "user_functions = [store[\"LogisticRegression\"][0][\"user_function\"], store[\"LogisticRegression\"][3][\"user_function\"], store[\"LogisticRegression\"][8][\"user_function\"]]\n",
    "clfs = [store[\"LogisticRegression\"][0][\"model\"], store[\"LogisticRegression\"][3][\"model\"], store[\"LogisticRegression\"][8][\"model\"]]\n",
    "attention_configs = [store[\"LogisticRegression\"][0][\"attention_config\"], store[\"LogisticRegression\"][3][\"attention_config\"], store[\"LogisticRegression\"][8][\"attention_config\"]]\n",
    "times = []\n",
    "accs = []\n",
    "for i in range(1):\n",
    "    framework, end_time = fit_dataset(CITESEER,user_functions=user_functions, hops_list= [0,3,8], clfs = clfs, attention_configs= attention_configs)\n",
    "    acc_dict = predict_dataset(CITESEER, framework)\n",
    "    times.append(end_time)\n",
    "    accs.append(acc_dict[\"test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfd4c0-2eb5-4237-90a0-95c87149dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(f\"Accuracy of CiteSeer {np.array(accs).mean()} +- {np.array(accs).std()}; Required training time: {np.array(times).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c48d4-e7e9-4127-ab3c-c9c007402599",
   "metadata": {},
   "source": [
    "## PubMed Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55ce4d-9202-4216-b16b-ee85a0d9f1bd",
   "metadata": {},
   "source": [
    "### PubMed Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539cca25-9d2c-4a90-bc25-5c0723c5bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoTune2 import upd_user_function, norm_user_function\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pubmed_store = {'LogisticRegression': {0: {'train_acc': 0.9833333333333333,\n",
    "   'val_acc': 0.742,\n",
    "   'test_acc': 0.737,\n",
    "   'model': LogisticRegression(C=19.012946053218332, max_iter=4096,\n",
    "                      tol=0.011693958862167635),\n",
    "   'user_function': upd_user_function,\n",
    "   'attention_config': {'inter_layer_normalize': True,\n",
    "    'use_pseudo_attention': True,\n",
    "    'cosine_eps': 0.001,\n",
    "    'dropout_attn': None}},\n",
    "  3: {'train_acc': 0.9833333333333333,\n",
    "   'val_acc': 0.812,\n",
    "   'test_acc': 0.802,\n",
    "   'model': LogisticRegression(C=2.3417904147486635, max_iter=4096,\n",
    "                      tol=1.877242326314165e-05),\n",
    "   'user_function': norm_user_function,\n",
    "   'attention_config': {'inter_layer_normalize': False,\n",
    "    'use_pseudo_attention': True,\n",
    "    'cosine_eps': 0.01,\n",
    "    'dropout_attn': None}},\n",
    "  8: {'train_acc': 0.9666666666666667,\n",
    "   'val_acc': 0.826,\n",
    "   'test_acc': 0.793,\n",
    "   'model': LogisticRegression(C=8.816857543671555, max_iter=256, tol=0.0003864950814262107),\n",
    "   'user_function': norm_user_function,\n",
    "   'attention_config': None}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaa5914-5307-4900-accc-2d334a741fd8",
   "metadata": {},
   "source": [
    "### GraphAware on PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318ac98-4b11-4f83-b72c-26424523021c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "store = pubmed_store\n",
    "user_functions = [store[\"LogisticRegression\"][0][\"user_function\"], store[\"LogisticRegression\"][3][\"user_function\"], store[\"LogisticRegression\"][8][\"user_function\"]]\n",
    "clfs = [store[\"LogisticRegression\"][0][\"model\"], store[\"LogisticRegression\"][3][\"model\"], store[\"LogisticRegression\"][8][\"model\"]]\n",
    "attention_configs = [store[\"LogisticRegression\"][0][\"attention_config\"], store[\"LogisticRegression\"][3][\"attention_config\"], store[\"LogisticRegression\"][8][\"attention_config\"]]\n",
    "times = []\n",
    "accs = []\n",
    "for i in tqdm(range(1)):\n",
    "    framework, end_time = fit_dataset(PUBMED,user_functions=user_functions, hops_list= [0,3,8], clfs = clfs, attention_configs= attention_configs)\n",
    "    acc_dict = predict_dataset(PUBMED, framework)\n",
    "    times.append(end_time)\n",
    "    accs.append(acc_dict[\"test_acc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae86b1f1-b7c7-4481-a516-9af2a7319f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(f\"Accuracy of PuBMed {np.array(accs).mean()} +- {np.array(accs).std()}; Required training time: {np.array(times).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7b1a4-ba15-4da0-9188-a39b19b24d8a",
   "metadata": {},
   "source": [
    "## Cora Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d685f74-4f0c-47ff-b700-f6307cb316a0",
   "metadata": {},
   "source": [
    "### Cora Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ceacc-852c-4bd7-91df-71e63ac0dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from AutoTune2 import user_function, norm_user_function\n",
    "cora_store = {'LogisticRegression': {0: {'train_acc': 0.9928571428571429,\n",
    "   'val_acc': 0.586,\n",
    "   'test_acc': 0.598,\n",
    "   'model': LogisticRegression(C=9.89464848441749, l1_ratio=0.1083794378326286,\n",
    "                      max_iter=8192, tol=3.9088643651368724e-05),\n",
    "   'user_function': user_function,\n",
    "   'attention_config': {'inter_layer_normalize': True,\n",
    "    'use_pseudo_attention': True,\n",
    "    'cosine_eps': 0.001,\n",
    "    'dropout_attn': None}},\n",
    "  3: {'train_acc': 0.9928571428571429,\n",
    "   'val_acc': 0.808,\n",
    "   'test_acc': 0.82,\n",
    "   'model': LogisticRegression(C=4.9289832447362025, l1_ratio=0.1761472021705791,\n",
    "                      max_iter=8192, tol=0.017104071425396022),\n",
    "   'user_function': norm_user_function,\n",
    "   'attention_config': None},\n",
    "  8: {'train_acc': 0.9928571428571429,\n",
    "   'val_acc': 0.808,\n",
    "   'test_acc': 0.817,\n",
    "   'model': LogisticRegression(C=3.726771337407598, l1_ratio=0.032154684509317244,\n",
    "                      max_iter=128, tol=7.557643967339885e-05),\n",
    "   'user_function': norm_user_function,\n",
    "   'attention_config': {'inter_layer_normalize': True,\n",
    "    'use_pseudo_attention': True,\n",
    "    'cosine_eps': 0.001,\n",
    "    'dropout_attn': None}}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea531a8-444a-4134-b172-b1d0fc1fd4f9",
   "metadata": {},
   "source": [
    "### GraphAware on Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244557af-8d5c-4abe-bc40-ce3bd3a461ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "store = cora_store\n",
    "user_functions = [store[\"LogisticRegression\"][0][\"user_function\"], store[\"LogisticRegression\"][3][\"user_function\"], store[\"LogisticRegression\"][8][\"user_function\"]]\n",
    "clfs = [store[\"LogisticRegression\"][0][\"model\"], store[\"LogisticRegression\"][3][\"model\"], store[\"LogisticRegression\"][8][\"model\"]]\n",
    "attention_configs = [store[\"LogisticRegression\"][0][\"attention_config\"], store[\"LogisticRegression\"][3][\"attention_config\"], store[\"LogisticRegression\"][8][\"attention_config\"]]\n",
    "\n",
    "times = []\n",
    "accs = []\n",
    "for i in range(1):\n",
    "    framework, end_time = fit_dataset(CORA,user_functions=user_functions, hops_list= [0,3,8], clfs = clfs, attention_configs= attention_configs)\n",
    "    acc_dict = predict_dataset(CORA, framework)\n",
    "    print(acc_dict)\n",
    "    times.append(end_time)\n",
    "    accs.append(acc_dict[\"test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d2c11-8082-42f6-a013-8eb2cad42c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(f\"Accuracy of Cora {np.array(accs).mean()} +- {np.array(accs).std()}; Required training time: {np.array(times).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7653e6f-4a72-4ef0-9b0d-3e6fca8ef221",
   "metadata": {},
   "source": [
    "## Feature importance for CORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295879d0-b9b9-45f3-ba13-74e1c2d38530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "font = {'family' : 'Arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "framework.plot_feature_importances(mark_top_n_peaks = 3, file_name=\"feature_importance_cora\", dpi = 100 , font_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd397a7e-644a-4986-ae3f-81ae5fc06896",
   "metadata": {},
   "source": [
    "## T-SNE Plot for Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aadd99-dabd-4a8a-874f-8239e18cbf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = name_to_sets[CORA]\n",
    "y = dataset[\"y\"]\n",
    "X = dataset[\"X\"]\n",
    "edge_index = dataset[\"edge_index\"]\n",
    "label_to_color_map = colors = {\n",
    "    0: (1.0, 0.0, 0.0),       # Red\n",
    "    1: (0.0, 0.5, 0.0),       # Green\n",
    "    2: (0.0, 0.0, 1.0),       # Blue\n",
    "    3: (1.0, 0.65, 0.0),      # Orange\n",
    "    4: (0.5, 0.0, 0.5),       # Purple\n",
    "    5: (0.0, 1.0, 1.0),       # Cyan\n",
    "    6: (1.0, 1.0, 0.0)        # Yellow\n",
    "}\n",
    "\n",
    "framework.plot_tsne(X, edge_index, y, label_to_color_map = label_to_color_map, dpi = 100, file_name=\"tsne_graphaware\", fig_size=(15,10), font_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4725d3a-fe49-42eb-abee-6dab1aab6557",
   "metadata": {},
   "source": [
    "## TSNE only with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e8d19f-7216-40de-acbd-12d591faf796",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = cora_store\n",
    "user_functions = [None]\n",
    "clfs = [store[\"LogisticRegression\"][0][\"model\"]]\n",
    "attention_configs = [None]\n",
    "\n",
    "framework_without_neighbors, end_time = fit_dataset(CORA,user_functions=user_functions, hops_list= [0], clfs = clfs, attention_configs= attention_configs)\n",
    "acc_dict = predict_dataset(CORA, framework_without_neighbors)\n",
    "print(acc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf5dde7-8289-41ad-9f37-fd039b1de5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = name_to_sets[CORA]\n",
    "y = dataset[\"y\"]\n",
    "X = dataset[\"X\"]\n",
    "edge_index = dataset[\"edge_index\"]\n",
    "label_to_color_map = colors = {\n",
    "    0: (1.0, 0.0, 0.0),       # Red\n",
    "    1: (0.0, 0.5, 0.0),       # Green\n",
    "    2: (0.0, 0.0, 1.0),       # Blue\n",
    "    3: (1.0, 0.65, 0.0),      # Orange\n",
    "    4: (0.5, 0.0, 0.5),       # Purple\n",
    "    5: (0.0, 1.0, 1.0),       # Cyan\n",
    "    6: (1.0, 1.0, 0.0)        # Yellow\n",
    "}\n",
    "\n",
    "framework_without_neighbors.plot_tsne(X, edge_index, y, label_to_color_map = label_to_color_map, dpi = 100, file_name=\"tsne_without_neighbors\", fig_size=(15,10), font_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b602188d-9696-43ed-91ee-84d466e15b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
