/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/08/06 14:21:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/08/06 14:21:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:23:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 1:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:38:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 1:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1]                                                                                [Stage 2:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:51:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 2:>                                                          (0 + 1) / 1][Stage 2:>                                                          (0 + 1) / 1][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:54:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 3:>                                                          (0 + 1) / 1][Stage 3:>                                                          (0 + 1) / 1]                                                                                [Stage 4:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 4:>                                                          (0 + 1) / 1][Stage 4:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:59:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 4:>                                                          (0 + 1) / 1][Stage 4:>                                                          (0 + 1) / 1][Stage 4:>                                                          (0 + 1) / 1][Stage 4:>                                                          (0 + 1) / 1][Stage 4:>                                                          (0 + 1) / 1][Stage 4:>                                                          (0 + 1) / 1][Stage 4:>                                                          (0 + 1) / 1][Stage 4:>                                                          (0 + 1) / 1][Stage 4:>                                                          (0 + 1) / 1][Stage 4:>                                                          (0 + 1) / 1][Stage 4:>                                                          (0 + 1) / 1]                                                                                [Stage 5:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:10:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 5:>                                                          (0 + 1) / 1][Stage 5:>                                                          (0 + 1) / 1]                                                                                [Stage 6:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:16:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1]                                                                                [Stage 7:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:35:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1]                                                                                [Stage 8:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 8:>                                                          (0 + 1) / 1][Stage 8:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:56:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 8:>                                                          (0 + 1) / 1][Stage 8:>                                                          (0 + 1) / 1][Stage 8:>                                                          (0 + 1) / 1][Stage 8:>                                                          (0 + 1) / 1][Stage 8:>                                                          (0 + 1) / 1][Stage 8:>                                                          (0 + 1) / 1][Stage 8:>                                                          (0 + 1) / 1]                                                                                [Stage 9:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 9:>                                                          (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:05:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 9:>                                                          (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1]                                                                                [Stage 10:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 10:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:11:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 10:>                                                         (0 + 1) / 1][Stage 10:>                                                         (0 + 1) / 1][Stage 10:>                                                         (0 + 1) / 1][Stage 10:>                                                         (0 + 1) / 1][Stage 10:>                                                         (0 + 1) / 1]                                                                                [Stage 11:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:16:33] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 11:>                                                         (0 + 1) / 1]                                                                                [Stage 12:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:18:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 12:>                                                         (0 + 1) / 1][Stage 12:>                                                         (0 + 1) / 1]                                                                                [Stage 13:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 13:>                                                         (0 + 1) / 1][Stage 13:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:22:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 13:>                                                         (0 + 1) / 1][Stage 13:>                                                         (0 + 1) / 1][Stage 13:>                                                         (0 + 1) / 1][Stage 13:>                                                         (0 + 1) / 1][Stage 13:>                                                         (0 + 1) / 1][Stage 13:>                                                         (0 + 1) / 1][Stage 13:>                                                         (0 + 1) / 1][Stage 13:>                                                         (0 + 1) / 1]                                                                                [Stage 14:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 14:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:32:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 14:>                                                         (0 + 1) / 1][Stage 14:>                                                         (0 + 1) / 1][Stage 14:>                                                         (0 + 1) / 1][Stage 14:>                                                         (0 + 1) / 1][Stage 14:>                                                         (0 + 1) / 1][Stage 14:>                                                         (0 + 1) / 1]                                                                                [Stage 15:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 15:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:40:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 15:>                                                         (0 + 1) / 1][Stage 15:>                                                         (0 + 1) / 1][Stage 15:>                                                         (0 + 1) / 1][Stage 15:>                                                         (0 + 1) / 1][Stage 15:>                                                         (0 + 1) / 1][Stage 15:>                                                         (0 + 1) / 1]                                                                                [Stage 16:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:47:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 16:>                                                         (0 + 1) / 1][Stage 16:>                                                         (0 + 1) / 1][Stage 16:>                                                         (0 + 1) / 1]                                                                                [Stage 17:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 17:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:50:36] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 17:>                                                         (0 + 1) / 1][Stage 17:>                                                         (0 + 1) / 1][Stage 17:>                                                         (0 + 1) / 1][Stage 17:>                                                         (0 + 1) / 1]                                                                                [Stage 18:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 18:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:56:31] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 18:>                                                         (0 + 1) / 1][Stage 18:>                                                         (0 + 1) / 1][Stage 18:>                                                         (0 + 1) / 1][Stage 18:>                                                         (0 + 1) / 1][Stage 18:>                                                         (0 + 1) / 1][Stage 18:>                                                         (0 + 1) / 1][Stage 18:>                                                         (0 + 1) / 1]                                                                                [Stage 19:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:03:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 19:>                                                         (0 + 1) / 1][Stage 19:>                                                         (0 + 1) / 1]                                                                                [Stage 20:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 20:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:07:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 20:>                                                         (0 + 1) / 1][Stage 20:>                                                         (0 + 1) / 1][Stage 20:>                                                         (0 + 1) / 1][Stage 20:>                                                         (0 + 1) / 1][Stage 20:>                                                         (0 + 1) / 1][Stage 20:>                                                         (0 + 1) / 1]                                                                                [Stage 21:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:18:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1][Stage 21:>                                                         (0 + 1) / 1]                                                                                [Stage 22:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1][Stage 22:>                                                         (0 + 1) / 1]                                                                                [Stage 23:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:02:32] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1][Stage 23:>                                                         (0 + 1) / 1]                                                                                [Stage 24:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 24:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:23:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 24:>                                                         (0 + 1) / 1][Stage 24:>                                                         (0 + 1) / 1][Stage 24:>                                                         (0 + 1) / 1][Stage 24:>                                                         (0 + 1) / 1][Stage 24:>                                                         (0 + 1) / 1][Stage 24:>                                                         (0 + 1) / 1][Stage 24:>                                                         (0 + 1) / 1][Stage 24:>                                                         (0 + 1) / 1]                                                                                [Stage 25:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:35:52] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1][Stage 25:>                                                         (0 + 1) / 1]                                                                                [Stage 26:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 26:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:53:29] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 26:>                                                         (0 + 1) / 1][Stage 26:>                                                         (0 + 1) / 1][Stage 26:>                                                         (0 + 1) / 1][Stage 26:>                                                         (0 + 1) / 1][Stage 26:>                                                         (0 + 1) / 1][Stage 26:>                                                         (0 + 1) / 1][Stage 26:>                                                         (0 + 1) / 1]                                                                                [Stage 27:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:03:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1]                                                                                [Stage 28:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 28:>                                                         (0 + 1) / 1][Stage 28:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:18:43] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 28:>                                                         (0 + 1) / 1][Stage 28:>                                                         (0 + 1) / 1][Stage 28:>                                                         (0 + 1) / 1][Stage 28:>                                                         (0 + 1) / 1][Stage 28:>                                                         (0 + 1) / 1][Stage 28:>                                                         (0 + 1) / 1][Stage 28:>                                                         (0 + 1) / 1][Stage 28:>                                                         (0 + 1) / 1][Stage 28:>                                                         (0 + 1) / 1][Stage 28:>                                                         (0 + 1) / 1]                                                                                [Stage 29:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 29:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:29:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 29:>                                                         (0 + 1) / 1][Stage 29:>                                                         (0 + 1) / 1][Stage 29:>                                                         (0 + 1) / 1][Stage 29:>                                                         (0 + 1) / 1]                                                                                [Stage 30:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:37:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1][Stage 30:>                                                         (0 + 1) / 1]                                                                                [Stage 31:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 31:>                                                         (0 + 1) / 1][Stage 31:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:52:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 31:>                                                         (0 + 1) / 1][Stage 31:>                                                         (0 + 1) / 1][Stage 31:>                                                         (0 + 1) / 1][Stage 31:>                                                         (0 + 1) / 1][Stage 31:>                                                         (0 + 1) / 1][Stage 31:>                                                         (0 + 1) / 1][Stage 31:>                                                         (0 + 1) / 1]                                                                                [Stage 32:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:03:48] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1][Stage 32:>                                                         (0 + 1) / 1]                                                                                [Stage 33:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 33:>                                                         (0 + 1) / 1][Stage 33:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:20:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 33:>                                                         (0 + 1) / 1][Stage 33:>                                                         (0 + 1) / 1][Stage 33:>                                                         (0 + 1) / 1][Stage 33:>                                                         (0 + 1) / 1][Stage 33:>                                                         (0 + 1) / 1][Stage 33:>                                                         (0 + 1) / 1][Stage 33:>                                                         (0 + 1) / 1][Stage 33:>                                                         (0 + 1) / 1][Stage 33:>                                                         (0 + 1) / 1][Stage 33:>                                                         (0 + 1) / 1][Stage 33:>                                                         (0 + 1) / 1][Stage 33:>                                                         (0 + 1) / 1]                                                                                [Stage 34:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:35:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1][Stage 34:>                                                         (0 + 1) / 1]                                                                                [Stage 35:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 35:>                                                         (0 + 1) / 1][Stage 35:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:51:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 35:>                                                         (0 + 1) / 1][Stage 35:>                                                         (0 + 1) / 1][Stage 35:>                                                         (0 + 1) / 1][Stage 35:>                                                         (0 + 1) / 1][Stage 35:>                                                         (0 + 1) / 1][Stage 35:>                                                         (0 + 1) / 1][Stage 35:>                                                         (0 + 1) / 1][Stage 35:>                                                         (0 + 1) / 1]                                                                                [Stage 36:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:03:43] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1][Stage 36:>                                                         (0 + 1) / 1]                                                                                [Stage 37:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:22:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1][Stage 37:>                                                         (0 + 1) / 1]                                                                                [Stage 38:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 38:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:44:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 38:>                                                         (0 + 1) / 1][Stage 38:>                                                         (0 + 1) / 1][Stage 38:>                                                         (0 + 1) / 1][Stage 38:>                                                         (0 + 1) / 1][Stage 38:>                                                         (0 + 1) / 1][Stage 38:>                                                         (0 + 1) / 1][Stage 38:>                                                         (0 + 1) / 1]                                                                                [Stage 39:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:52:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 39:>                                                         (0 + 1) / 1][Stage 39:>                                                         (0 + 1) / 1][Stage 39:>                                                         (0 + 1) / 1]                                                                                [Stage 40:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 40:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:55:58] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 40:>                                                         (0 + 1) / 1][Stage 40:>                                                         (0 + 1) / 1][Stage 40:>                                                         (0 + 1) / 1][Stage 40:>                                                         (0 + 1) / 1]                                                                                [Stage 41:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:04:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1][Stage 41:>                                                         (0 + 1) / 1]                                                                                [Stage 42:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 42:>                                                         (0 + 1) / 1][Stage 42:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:26:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 42:>                                                         (0 + 1) / 1][Stage 42:>                                                         (0 + 1) / 1][Stage 42:>                                                         (0 + 1) / 1][Stage 42:>                                                         (0 + 1) / 1][Stage 42:>                                                         (0 + 1) / 1][Stage 42:>                                                         (0 + 1) / 1][Stage 42:>                                                         (0 + 1) / 1][Stage 42:>                                                         (0 + 1) / 1][Stage 42:>                                                         (0 + 1) / 1][Stage 42:>                                                         (0 + 1) / 1][Stage 42:>                                                         (0 + 1) / 1]                                                                                [Stage 43:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:41:20] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1][Stage 43:>                                                         (0 + 1) / 1]                                                                                [Stage 44:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 44:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:00:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 44:>                                                         (0 + 1) / 1][Stage 44:>                                                         (0 + 1) / 1][Stage 44:>                                                         (0 + 1) / 1][Stage 44:>                                                         (0 + 1) / 1][Stage 44:>                                                         (0 + 1) / 1][Stage 44:>                                                         (0 + 1) / 1][Stage 44:>                                                         (0 + 1) / 1][Stage 44:>                                                         (0 + 1) / 1]                                                                                [Stage 45:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 45:>                                                         (0 + 1) / 1][Stage 45:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:10:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 45:>                                                         (0 + 1) / 1][Stage 45:>                                                         (0 + 1) / 1][Stage 45:>                                                         (0 + 1) / 1][Stage 45:>                                                         (0 + 1) / 1][Stage 45:>                                                         (0 + 1) / 1][Stage 45:>                                                         (0 + 1) / 1][Stage 45:>                                                         (0 + 1) / 1][Stage 45:>                                                         (0 + 1) / 1][Stage 45:>                                                         (0 + 1) / 1]                                                                                [Stage 46:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 46:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:21:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 46:>                                                         (0 + 1) / 1][Stage 46:>                                                         (0 + 1) / 1][Stage 46:>                                                         (0 + 1) / 1][Stage 46:>                                                         (0 + 1) / 1][Stage 46:>                                                         (0 + 1) / 1][Stage 46:>                                                         (0 + 1) / 1][Stage 46:>                                                         (0 + 1) / 1][Stage 46:>                                                         (0 + 1) / 1]                                                                                [Stage 47:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 47:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:29:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 47:>                                                         (0 + 1) / 1][Stage 47:>                                                         (0 + 1) / 1][Stage 47:>                                                         (0 + 1) / 1]                                                                                [Stage 48:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:36:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1][Stage 48:>                                                         (0 + 1) / 1]                                                                                [Stage 49:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 49:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:51:47] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 49:>                                                         (0 + 1) / 1][Stage 49:>                                                         (0 + 1) / 1][Stage 49:>                                                         (0 + 1) / 1][Stage 49:>                                                         (0 + 1) / 1][Stage 49:>                                                         (0 + 1) / 1][Stage 49:>                                                         (0 + 1) / 1]                                                                                [Stage 50:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:58:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 50:>                                                         (0 + 1) / 1]                                                                                [Stage 51:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 51:>                                                         (0 + 1) / 1][Stage 51:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:01:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 51:>                                                         (0 + 1) / 1][Stage 51:>                                                         (0 + 1) / 1][Stage 51:>                                                         (0 + 1) / 1][Stage 51:>                                                         (0 + 1) / 1][Stage 51:>                                                         (0 + 1) / 1][Stage 51:>                                                         (0 + 1) / 1][Stage 51:>                                                         (0 + 1) / 1][Stage 51:>                                                         (0 + 1) / 1][Stage 51:>                                                         (0 + 1) / 1]                                                                                [Stage 52:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:15:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1][Stage 52:>                                                         (0 + 1) / 1]                                                                                [Stage 53:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 53:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:33:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 53:>                                                         (0 + 1) / 1][Stage 53:>                                                         (0 + 1) / 1][Stage 53:>                                                         (0 + 1) / 1][Stage 53:>                                                         (0 + 1) / 1][Stage 53:>                                                         (0 + 1) / 1][Stage 53:>                                                         (0 + 1) / 1]                                                                                [Stage 54:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 54:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:40:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 54:>                                                         (0 + 1) / 1][Stage 54:>                                                         (0 + 1) / 1][Stage 54:>                                                         (0 + 1) / 1][Stage 54:>                                                         (0 + 1) / 1][Stage 54:>                                                         (0 + 1) / 1][Stage 54:>                                                         (0 + 1) / 1]                                                                                [Stage 55:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:47:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 55:>                                                         (0 + 1) / 1][Stage 55:>                                                         (0 + 1) / 1][Stage 55:>                                                         (0 + 1) / 1][Stage 55:>                                                         (0 + 1) / 1]                                                                                [Stage 56:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 56:>                                                         (0 + 1) / 1][Stage 56:>                                                         (0 + 1) / 1][Stage 56:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:53:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 56:>                                                         (0 + 1) / 1][Stage 56:>                                                         (0 + 1) / 1][Stage 56:>                                                         (0 + 1) / 1][Stage 56:>                                                         (0 + 1) / 1][Stage 56:>                                                         (0 + 1) / 1][Stage 56:>                                                         (0 + 1) / 1][Stage 56:>                                                         (0 + 1) / 1][Stage 56:>                                                         (0 + 1) / 1][Stage 56:>                                                         (0 + 1) / 1][Stage 56:>                                                         (0 + 1) / 1][Stage 56:>                                                         (0 + 1) / 1][Stage 56:>                                                         (0 + 1) / 1]                                                                                [Stage 57:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:10:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1][Stage 57:>                                                         (0 + 1) / 1]                                                                                [Stage 58:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 58:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:28:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 58:>                                                         (0 + 1) / 1][Stage 58:>                                                         (0 + 1) / 1][Stage 58:>                                                         (0 + 1) / 1][Stage 58:>                                                         (0 + 1) / 1][Stage 58:>                                                         (0 + 1) / 1][Stage 58:>                                                         (0 + 1) / 1][Stage 58:>                                                         (0 + 1) / 1][Stage 58:>                                                         (0 + 1) / 1]                                                                                [Stage 59:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:39:44] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1][Stage 59:>                                                         (0 + 1) / 1]                                                                                [Stage 60:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:56:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1][Stage 60:>                                                         (0 + 1) / 1]                                                                                [Stage 61:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:12:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 61:>                                                         (0 + 1) / 1][Stage 61:>                                                         (0 + 1) / 1]                                                                                [Stage 62:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:19:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1][Stage 62:>                                                         (0 + 1) / 1]                                                                                [Stage 63:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:43:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1][Stage 63:>                                                         (0 + 1) / 1]                                                                                [Stage 64:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:08:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1][Stage 64:>                                                         (0 + 1) / 1]                                                                                [Stage 65:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:30:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1][Stage 65:>                                                         (0 + 1) / 1]                                                                                [Stage 66:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:51:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1][Stage 66:>                                                         (0 + 1) / 1]                                                                                [Stage 67:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 67:>                                                         (0 + 1) / 1][Stage 67:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:10:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 67:>                                                         (0 + 1) / 1][Stage 67:>                                                         (0 + 1) / 1][Stage 67:>                                                         (0 + 1) / 1][Stage 67:>                                                         (0 + 1) / 1][Stage 67:>                                                         (0 + 1) / 1][Stage 67:>                                                         (0 + 1) / 1][Stage 67:>                                                         (0 + 1) / 1][Stage 67:>                                                         (0 + 1) / 1][Stage 67:>                                                         (0 + 1) / 1][Stage 67:>                                                         (0 + 1) / 1][Stage 67:>                                                         (0 + 1) / 1]                                                                                [Stage 68:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:24:44] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1][Stage 68:>                                                         (0 + 1) / 1]                                                                                [Stage 69:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:42:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1][Stage 69:>                                                         (0 + 1) / 1]                                                                                [Stage 70:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:05:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1][Stage 70:>                                                         (0 + 1) / 1]                                                                                [Stage 71:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:28:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1][Stage 71:>                                                         (0 + 1) / 1]                                                                                [Stage 72:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:46:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1][Stage 72:>                                                         (0 + 1) / 1]                                                                                [Stage 73:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:07:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1][Stage 73:>                                                         (0 + 1) / 1]                                                                                [Stage 74:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 74:>                                                         (0 + 1) / 1][Stage 74:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:28:20] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 74:>                                                         (0 + 1) / 1][Stage 74:>                                                         (0 + 1) / 1][Stage 74:>                                                         (0 + 1) / 1][Stage 74:>                                                         (0 + 1) / 1][Stage 74:>                                                         (0 + 1) / 1][Stage 74:>                                                         (0 + 1) / 1][Stage 74:>                                                         (0 + 1) / 1][Stage 74:>                                                         (0 + 1) / 1][Stage 74:>                                                         (0 + 1) / 1][Stage 74:>                                                         (0 + 1) / 1]                                                                                [Stage 75:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:44:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1][Stage 75:>                                                         (0 + 1) / 1]                                                                                [Stage 76:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:09:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1][Stage 76:>                                                         (0 + 1) / 1]                                                                                [Stage 77:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 77:>                                                         (0 + 1) / 1][Stage 77:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:25:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 77:>                                                         (0 + 1) / 1][Stage 77:>                                                         (0 + 1) / 1][Stage 77:>                                                         (0 + 1) / 1][Stage 77:>                                                         (0 + 1) / 1][Stage 77:>                                                         (0 + 1) / 1][Stage 77:>                                                         (0 + 1) / 1][Stage 77:>                                                         (0 + 1) / 1][Stage 77:>                                                         (0 + 1) / 1]                                                                                [Stage 78:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 78:>                                                         (0 + 1) / 1][Stage 78:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:35:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 78:>                                                         (0 + 1) / 1][Stage 78:>                                                         (0 + 1) / 1][Stage 78:>                                                         (0 + 1) / 1][Stage 78:>                                                         (0 + 1) / 1][Stage 78:>                                                         (0 + 1) / 1][Stage 78:>                                                         (0 + 1) / 1][Stage 78:>                                                         (0 + 1) / 1][Stage 78:>                                                         (0 + 1) / 1][Stage 78:>                                                         (0 + 1) / 1]                                                                                [Stage 79:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:48:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1][Stage 79:>                                                         (0 + 1) / 1]                                                                                [Stage 80:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:05:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1][Stage 80:>                                                         (0 + 1) / 1]                                                                                [Stage 81:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:22:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1][Stage 81:>                                                         (0 + 1) / 1]                                                                                [Stage 82:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 82:>                                                         (0 + 1) / 1][Stage 82:>                                                         (0 + 1) / 1][Stage 82:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:38:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 82:>                                                         (0 + 1) / 1][Stage 82:>                                                         (0 + 1) / 1][Stage 82:>                                                         (0 + 1) / 1][Stage 82:>                                                         (0 + 1) / 1][Stage 82:>                                                         (0 + 1) / 1][Stage 82:>                                                         (0 + 1) / 1][Stage 82:>                                                         (0 + 1) / 1][Stage 82:>                                                         (0 + 1) / 1][Stage 82:>                                                         (0 + 1) / 1][Stage 82:>                                                         (0 + 1) / 1][Stage 82:>                                                         (0 + 1) / 1]                                                                                [Stage 83:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 83:>                                                         (0 + 1) / 1][Stage 83:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:52:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 83:>                                                         (0 + 1) / 1][Stage 83:>                                                         (0 + 1) / 1][Stage 83:>                                                         (0 + 1) / 1][Stage 83:>                                                         (0 + 1) / 1][Stage 83:>                                                         (0 + 1) / 1][Stage 83:>                                                         (0 + 1) / 1][Stage 83:>                                                         (0 + 1) / 1][Stage 83:>                                                         (0 + 1) / 1][Stage 83:>                                                         (0 + 1) / 1][Stage 83:>                                                         (0 + 1) / 1][Stage 83:>                                                         (0 + 1) / 1]                                                                                [Stage 84:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:07:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1][Stage 84:>                                                         (0 + 1) / 1]                                                                                [Stage 85:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 85:>                                                         (0 + 1) / 1][Stage 85:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:26:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 85:>                                                         (0 + 1) / 1][Stage 85:>                                                         (0 + 1) / 1][Stage 85:>                                                         (0 + 1) / 1][Stage 85:>                                                         (0 + 1) / 1][Stage 85:>                                                         (0 + 1) / 1][Stage 85:>                                                         (0 + 1) / 1][Stage 85:>                                                         (0 + 1) / 1][Stage 85:>                                                         (0 + 1) / 1][Stage 85:>                                                         (0 + 1) / 1][Stage 85:>                                                         (0 + 1) / 1]                                                                                [Stage 86:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:37:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 86:>                                                         (0 + 1) / 1][Stage 86:>                                                         (0 + 1) / 1][Stage 86:>                                                         (0 + 1) / 1][Stage 86:>                                                         (0 + 1) / 1]                                                                                [Stage 87:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 87:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:42:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 87:>                                                         (0 + 1) / 1][Stage 87:>                                                         (0 + 1) / 1][Stage 87:>                                                         (0 + 1) / 1][Stage 87:>                                                         (0 + 1) / 1][Stage 87:>                                                         (0 + 1) / 1]                                                                                [Stage 88:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 88:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:49:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 88:>                                                         (0 + 1) / 1][Stage 88:>                                                         (0 + 1) / 1][Stage 88:>                                                         (0 + 1) / 1][Stage 88:>                                                         (0 + 1) / 1][Stage 88:>                                                         (0 + 1) / 1][Stage 88:>                                                         (0 + 1) / 1][Stage 88:>                                                         (0 + 1) / 1]                                                                                [Stage 89:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 89:>                                                         (0 + 1) / 1][Stage 89:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:58:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 89:>                                                         (0 + 1) / 1][Stage 89:>                                                         (0 + 1) / 1][Stage 89:>                                                         (0 + 1) / 1][Stage 89:>                                                         (0 + 1) / 1][Stage 89:>                                                         (0 + 1) / 1][Stage 89:>                                                         (0 + 1) / 1][Stage 89:>                                                         (0 + 1) / 1][Stage 89:>                                                         (0 + 1) / 1][Stage 89:>                                                         (0 + 1) / 1][Stage 89:>                                                         (0 + 1) / 1][Stage 89:>                                                         (0 + 1) / 1][Stage 89:>                                                         (0 + 1) / 1]                                                                                [Stage 90:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 90:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:11:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 90:>                                                         (0 + 1) / 1][Stage 90:>                                                         (0 + 1) / 1][Stage 90:>                                                         (0 + 1) / 1][Stage 90:>                                                         (0 + 1) / 1][Stage 90:>                                                         (0 + 1) / 1][Stage 90:>                                                         (0 + 1) / 1][Stage 90:>                                                         (0 + 1) / 1][Stage 90:>                                                         (0 + 1) / 1]                                                                                [Stage 91:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 91:>                                                         (0 + 1) / 1][Stage 91:>                                                         (0 + 1) / 1][Stage 91:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:22:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 91:>                                                         (0 + 1) / 1][Stage 91:>                                                         (0 + 1) / 1][Stage 91:>                                                         (0 + 1) / 1][Stage 91:>                                                         (0 + 1) / 1][Stage 91:>                                                         (0 + 1) / 1][Stage 91:>                                                         (0 + 1) / 1][Stage 91:>                                                         (0 + 1) / 1][Stage 91:>                                                         (0 + 1) / 1][Stage 91:>                                                         (0 + 1) / 1][Stage 91:>                                                         (0 + 1) / 1][Stage 91:>                                                         (0 + 1) / 1]                                                                                [Stage 92:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 92:>                                                         (0 + 1) / 1][Stage 92:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:36:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 92:>                                                         (0 + 1) / 1][Stage 92:>                                                         (0 + 1) / 1][Stage 92:>                                                         (0 + 1) / 1][Stage 92:>                                                         (0 + 1) / 1][Stage 92:>                                                         (0 + 1) / 1][Stage 92:>                                                         (0 + 1) / 1][Stage 92:>                                                         (0 + 1) / 1][Stage 92:>                                                         (0 + 1) / 1][Stage 92:>                                                         (0 + 1) / 1][Stage 92:>                                                         (0 + 1) / 1]                                                                                [Stage 93:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 93:>                                                         (0 + 1) / 1][Stage 93:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:49:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 93:>                                                         (0 + 1) / 1][Stage 93:>                                                         (0 + 1) / 1][Stage 93:>                                                         (0 + 1) / 1][Stage 93:>                                                         (0 + 1) / 1][Stage 93:>                                                         (0 + 1) / 1][Stage 93:>                                                         (0 + 1) / 1][Stage 93:>                                                         (0 + 1) / 1][Stage 93:>                                                         (0 + 1) / 1][Stage 93:>                                                         (0 + 1) / 1][Stage 93:>                                                         (0 + 1) / 1][Stage 93:>                                                         (0 + 1) / 1][Stage 93:>                                                         (0 + 1) / 1]                                                                                [Stage 94:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:06:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1][Stage 94:>                                                         (0 + 1) / 1]                                                                                [Stage 95:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 95:>                                                         (0 + 1) / 1][Stage 95:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:30:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 95:>                                                         (0 + 1) / 1][Stage 95:>                                                         (0 + 1) / 1][Stage 95:>                                                         (0 + 1) / 1][Stage 95:>                                                         (0 + 1) / 1][Stage 95:>                                                         (0 + 1) / 1][Stage 95:>                                                         (0 + 1) / 1][Stage 95:>                                                         (0 + 1) / 1][Stage 95:>                                                         (0 + 1) / 1]                                                                                [Stage 96:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:43:32] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1][Stage 96:>                                                         (0 + 1) / 1]                                                                                [Stage 97:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 97:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:00:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 97:>                                                         (0 + 1) / 1][Stage 97:>                                                         (0 + 1) / 1][Stage 97:>                                                         (0 + 1) / 1][Stage 97:>                                                         (0 + 1) / 1]                                                                                [Stage 98:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 98:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:05:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 98:>                                                         (0 + 1) / 1][Stage 98:>                                                         (0 + 1) / 1][Stage 98:>                                                         (0 + 1) / 1][Stage 98:>                                                         (0 + 1) / 1][Stage 98:>                                                         (0 + 1) / 1][Stage 98:>                                                         (0 + 1) / 1]                                                                                [Stage 99:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 99:>                                                         (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:13:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 99:>                                                         (0 + 1) / 1][Stage 99:>                                                         (0 + 1) / 1][Stage 99:>                                                         (0 + 1) / 1][Stage 99:>                                                         (0 + 1) / 1][Stage 99:>                                                         (0 + 1) / 1][Stage 99:>                                                         (0 + 1) / 1][Stage 99:>                                                         (0 + 1) / 1][Stage 99:>                                                         (0 + 1) / 1]                                                                                [Stage 100:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 100:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:22:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 100:>                                                        (0 + 1) / 1][Stage 100:>                                                        (0 + 1) / 1][Stage 100:>                                                        (0 + 1) / 1][Stage 100:>                                                        (0 + 1) / 1][Stage 100:>                                                        (0 + 1) / 1][Stage 100:>                                                        (0 + 1) / 1]                                                                                [Stage 101:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 101:>                                                        (0 + 1) / 1][Stage 101:>                                                        (0 + 1) / 1][Stage 101:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:31:32] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 101:>                                                        (0 + 1) / 1][Stage 101:>                                                        (0 + 1) / 1][Stage 101:>                                                        (0 + 1) / 1][Stage 101:>                                                        (0 + 1) / 1][Stage 101:>                                                        (0 + 1) / 1][Stage 101:>                                                        (0 + 1) / 1][Stage 101:>                                                        (0 + 1) / 1][Stage 101:>                                                        (0 + 1) / 1][Stage 101:>                                                        (0 + 1) / 1][Stage 101:>                                                        (0 + 1) / 1][Stage 101:>                                                        (0 + 1) / 1][Stage 101:>                                                        (0 + 1) / 1]                                                                                [Stage 102:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 102:>                                                        (0 + 1) / 1][Stage 102:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:46:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 102:>                                                        (0 + 1) / 1][Stage 102:>                                                        (0 + 1) / 1][Stage 102:>                                                        (0 + 1) / 1][Stage 102:>                                                        (0 + 1) / 1][Stage 102:>                                                        (0 + 1) / 1][Stage 102:>                                                        (0 + 1) / 1][Stage 102:>                                                        (0 + 1) / 1][Stage 102:>                                                        (0 + 1) / 1][Stage 102:>                                                        (0 + 1) / 1]                                                                                [Stage 103:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 103:>                                                        (0 + 1) / 1][Stage 103:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:58:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 103:>                                                        (0 + 1) / 1][Stage 103:>                                                        (0 + 1) / 1][Stage 103:>                                                        (0 + 1) / 1][Stage 103:>                                                        (0 + 1) / 1][Stage 103:>                                                        (0 + 1) / 1][Stage 103:>                                                        (0 + 1) / 1][Stage 103:>                                                        (0 + 1) / 1][Stage 103:>                                                        (0 + 1) / 1]                                                                                [Stage 104:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 104:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:08:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 104:>                                                        (0 + 1) / 1][Stage 104:>                                                        (0 + 1) / 1][Stage 104:>                                                        (0 + 1) / 1][Stage 104:>                                                        (0 + 1) / 1][Stage 104:>                                                        (0 + 1) / 1]                                                                                [Stage 105:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 105:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:14:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 105:>                                                        (0 + 1) / 1][Stage 105:>                                                        (0 + 1) / 1][Stage 105:>                                                        (0 + 1) / 1][Stage 105:>                                                        (0 + 1) / 1]                                                                                [Stage 106:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 106:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:20:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 106:>                                                        (0 + 1) / 1][Stage 106:>                                                        (0 + 1) / 1][Stage 106:>                                                        (0 + 1) / 1][Stage 106:>                                                        (0 + 1) / 1][Stage 106:>                                                        (0 + 1) / 1][Stage 106:>                                                        (0 + 1) / 1][Stage 106:>                                                        (0 + 1) / 1][Stage 106:>                                                        (0 + 1) / 1]                                                                                [Stage 107:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 107:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:29:31] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 107:>                                                        (0 + 1) / 1][Stage 107:>                                                        (0 + 1) / 1][Stage 107:>                                                        (0 + 1) / 1][Stage 107:>                                                        (0 + 1) / 1][Stage 107:>                                                        (0 + 1) / 1][Stage 107:>                                                        (0 + 1) / 1][Stage 107:>                                                        (0 + 1) / 1]                                                                                [Stage 108:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:40:58] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1][Stage 108:>                                                        (0 + 1) / 1]                                                                                [Stage 109:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 109:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:05:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 109:>                                                        (0 + 1) / 1][Stage 109:>                                                        (0 + 1) / 1][Stage 109:>                                                        (0 + 1) / 1][Stage 109:>                                                        (0 + 1) / 1][Stage 109:>                                                        (0 + 1) / 1]                                                                                [Stage 110:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:15:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1][Stage 110:>                                                        (0 + 1) / 1]                                                                                [Stage 111:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:38:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1][Stage 111:>                                                        (0 + 1) / 1]                                                                                [Stage 112:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 112:>                                                        (0 + 1) / 1][Stage 112:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:57:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 112:>                                                        (0 + 1) / 1][Stage 112:>                                                        (0 + 1) / 1][Stage 112:>                                                        (0 + 1) / 1][Stage 112:>                                                        (0 + 1) / 1][Stage 112:>                                                        (0 + 1) / 1][Stage 112:>                                                        (0 + 1) / 1][Stage 112:>                                                        (0 + 1) / 1][Stage 112:>                                                        (0 + 1) / 1][Stage 112:>                                                        (0 + 1) / 1][Stage 112:>                                                        (0 + 1) / 1][Stage 112:>                                                        (0 + 1) / 1]                                                                                [Stage 113:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:12:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1][Stage 113:>                                                        (0 + 1) / 1]                                                                                [Stage 114:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:33:11] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1][Stage 114:>                                                        (0 + 1) / 1]                                                                                [Stage 115:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 115:>                                                        (0 + 1) / 1][Stage 115:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:56:20] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 115:>                                                        (0 + 1) / 1][Stage 115:>                                                        (0 + 1) / 1][Stage 115:>                                                        (0 + 1) / 1][Stage 115:>                                                        (0 + 1) / 1][Stage 115:>                                                        (0 + 1) / 1][Stage 115:>                                                        (0 + 1) / 1][Stage 115:>                                                        (0 + 1) / 1][Stage 115:>                                                        (0 + 1) / 1][Stage 115:>                                                        (0 + 1) / 1][Stage 115:>                                                        (0 + 1) / 1]                                                                                [Stage 116:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:07:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 116:>                                                        (0 + 1) / 1][Stage 116:>                                                        (0 + 1) / 1][Stage 116:>                                                        (0 + 1) / 1][Stage 116:>                                                        (0 + 1) / 1]                                                                                [Stage 117:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:16:23] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1][Stage 117:>                                                        (0 + 1) / 1]                                                                                [Stage 118:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:36:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 118:>                                                        (0 + 1) / 1][Stage 118:>                                                        (0 + 1) / 1][Stage 118:>                                                        (0 + 1) / 1]                                                                                [Stage 119:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:39:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 119:>                                                        (0 + 1) / 1][Stage 119:>                                                        (0 + 1) / 1][Stage 119:>                                                        (0 + 1) / 1][Stage 119:>                                                        (0 + 1) / 1][Stage 119:>                                                        (0 + 1) / 1]                                                                                [Stage 120:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:44:36] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
                                                                                [Stage 121:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 121:>                                                        (0 + 1) / 1][Stage 121:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:47:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 121:>                                                        (0 + 1) / 1][Stage 121:>                                                        (0 + 1) / 1][Stage 121:>                                                        (0 + 1) / 1][Stage 121:>                                                        (0 + 1) / 1][Stage 121:>                                                        (0 + 1) / 1][Stage 121:>                                                        (0 + 1) / 1][Stage 121:>                                                        (0 + 1) / 1][Stage 121:>                                                        (0 + 1) / 1][Stage 121:>                                                        (0 + 1) / 1][Stage 121:>                                                        (0 + 1) / 1]                                                                                [Stage 122:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 122:>                                                        (0 + 1) / 1][Stage 122:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:00:31] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 122:>                                                        (0 + 1) / 1][Stage 122:>                                                        (0 + 1) / 1][Stage 122:>                                                        (0 + 1) / 1][Stage 122:>                                                        (0 + 1) / 1][Stage 122:>                                                        (0 + 1) / 1][Stage 122:>                                                        (0 + 1) / 1][Stage 122:>                                                        (0 + 1) / 1][Stage 122:>                                                        (0 + 1) / 1][Stage 122:>                                                        (0 + 1) / 1][Stage 122:>                                                        (0 + 1) / 1][Stage 122:>                                                        (0 + 1) / 1]                                                                                [Stage 123:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:15:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1][Stage 123:>                                                        (0 + 1) / 1]                                                                                [Stage 124:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:38:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1][Stage 124:>                                                        (0 + 1) / 1]                                                                                [Stage 125:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 125:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:53:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 125:>                                                        (0 + 1) / 1][Stage 125:>                                                        (0 + 1) / 1][Stage 125:>                                                        (0 + 1) / 1][Stage 125:>                                                        (0 + 1) / 1]                                                                                [Stage 126:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 126:>                                                        (0 + 1) / 1][Stage 126:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:01:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 126:>                                                        (0 + 1) / 1][Stage 126:>                                                        (0 + 1) / 1][Stage 126:>                                                        (0 + 1) / 1][Stage 126:>                                                        (0 + 1) / 1][Stage 126:>                                                        (0 + 1) / 1][Stage 126:>                                                        (0 + 1) / 1][Stage 126:>                                                        (0 + 1) / 1][Stage 126:>                                                        (0 + 1) / 1][Stage 126:>                                                        (0 + 1) / 1][Stage 126:>                                                        (0 + 1) / 1][Stage 126:>                                                        (0 + 1) / 1][Stage 126:>                                                        (0 + 1) / 1]                                                                                [Stage 127:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 127:>                                                        (0 + 1) / 1][Stage 127:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:16:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 127:>                                                        (0 + 1) / 1][Stage 127:>                                                        (0 + 1) / 1][Stage 127:>                                                        (0 + 1) / 1][Stage 127:>                                                        (0 + 1) / 1][Stage 127:>                                                        (0 + 1) / 1][Stage 127:>                                                        (0 + 1) / 1][Stage 127:>                                                        (0 + 1) / 1][Stage 127:>                                                        (0 + 1) / 1][Stage 127:>                                                        (0 + 1) / 1][Stage 127:>                                                        (0 + 1) / 1][Stage 127:>                                                        (0 + 1) / 1]                                                                                [Stage 128:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 128:>                                                        (0 + 1) / 1][Stage 128:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:28:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 128:>                                                        (0 + 1) / 1][Stage 128:>                                                        (0 + 1) / 1][Stage 128:>                                                        (0 + 1) / 1][Stage 128:>                                                        (0 + 1) / 1][Stage 128:>                                                        (0 + 1) / 1][Stage 128:>                                                        (0 + 1) / 1][Stage 128:>                                                        (0 + 1) / 1][Stage 128:>                                                        (0 + 1) / 1][Stage 128:>                                                        (0 + 1) / 1]                                                                                [Stage 129:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:42:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1][Stage 129:>                                                        (0 + 1) / 1]                                                                                [Stage 130:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:06:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1][Stage 130:>                                                        (0 + 1) / 1]                                                                                [Stage 131:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 131:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:25:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 131:>                                                        (0 + 1) / 1][Stage 131:>                                                        (0 + 1) / 1][Stage 131:>                                                        (0 + 1) / 1][Stage 131:>                                                        (0 + 1) / 1][Stage 131:>                                                        (0 + 1) / 1][Stage 131:>                                                        (0 + 1) / 1][Stage 131:>                                                        (0 + 1) / 1][Stage 131:>                                                        (0 + 1) / 1]                                                                                [Stage 132:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 132:>                                                        (0 + 1) / 1][Stage 132:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:35:33] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 132:>                                                        (0 + 1) / 1][Stage 132:>                                                        (0 + 1) / 1][Stage 132:>                                                        (0 + 1) / 1][Stage 132:>                                                        (0 + 1) / 1][Stage 132:>                                                        (0 + 1) / 1][Stage 132:>                                                        (0 + 1) / 1][Stage 132:>                                                        (0 + 1) / 1][Stage 132:>                                                        (0 + 1) / 1][Stage 132:>                                                        (0 + 1) / 1][Stage 132:>                                                        (0 + 1) / 1]                                                                                [Stage 133:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:49:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1][Stage 133:>                                                        (0 + 1) / 1]                                                                                [Stage 134:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:06:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1][Stage 134:>                                                        (0 + 1) / 1]                                                                                [Stage 135:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:24:48] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1][Stage 135:>                                                        (0 + 1) / 1]                                                                                [Stage 136:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 136:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:42:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 136:>                                                        (0 + 1) / 1][Stage 136:>                                                        (0 + 1) / 1][Stage 136:>                                                        (0 + 1) / 1][Stage 136:>                                                        (0 + 1) / 1][Stage 136:>                                                        (0 + 1) / 1][Stage 136:>                                                        (0 + 1) / 1][Stage 136:>                                                        (0 + 1) / 1][Stage 136:>                                                        (0 + 1) / 1]                                                                                [Stage 137:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 137:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:51:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 137:>                                                        (0 + 1) / 1][Stage 137:>                                                        (0 + 1) / 1][Stage 137:>                                                        (0 + 1) / 1][Stage 137:>                                                        (0 + 1) / 1][Stage 137:>                                                        (0 + 1) / 1]                                                                                [Stage 138:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:02:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1][Stage 138:>                                                        (0 + 1) / 1]                                                                                [Stage 139:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:26:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1][Stage 139:>                                                        (0 + 1) / 1]                                                                                Total Trials: 140: 140 succeeded, 0 failed, 0 cancelled.
[Stage 140:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:07:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1][Stage 140:>                                                        (0 + 1) / 1]                                                                                [Stage 141:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 141:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:28:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 141:>                                                        (0 + 1) / 1][Stage 141:>                                                        (0 + 1) / 1][Stage 141:>                                                        (0 + 1) / 1][Stage 141:>                                                        (0 + 1) / 1][Stage 141:>                                                        (0 + 1) / 1]                                                                                [Stage 142:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 142:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:36:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 142:>                                                        (0 + 1) / 1][Stage 142:>                                                        (0 + 1) / 1][Stage 142:>                                                        (0 + 1) / 1][Stage 142:>                                                        (0 + 1) / 1][Stage 142:>                                                        (0 + 1) / 1][Stage 142:>                                                        (0 + 1) / 1][Stage 142:>                                                        (0 + 1) / 1]                                                                                [Stage 143:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 143:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:44:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 143:>                                                        (0 + 1) / 1][Stage 143:>                                                        (0 + 1) / 1][Stage 143:>                                                        (0 + 1) / 1][Stage 143:>                                                        (0 + 1) / 1][Stage 143:>                                                        (0 + 1) / 1][Stage 143:>                                                        (0 + 1) / 1]                                                                                [Stage 144:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 144:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:52:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 144:>                                                        (0 + 1) / 1][Stage 144:>                                                        (0 + 1) / 1][Stage 144:>                                                        (0 + 1) / 1][Stage 144:>                                                        (0 + 1) / 1][Stage 144:>                                                        (0 + 1) / 1][Stage 144:>                                                        (0 + 1) / 1][Stage 144:>                                                        (0 + 1) / 1][Stage 144:>                                                        (0 + 1) / 1][Stage 144:>                                                        (0 + 1) / 1]                                                                                [Stage 145:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 145:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:02:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 145:>                                                        (0 + 1) / 1][Stage 145:>                                                        (0 + 1) / 1][Stage 145:>                                                        (0 + 1) / 1][Stage 145:>                                                        (0 + 1) / 1][Stage 145:>                                                        (0 + 1) / 1]                                                                                [Stage 146:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 146:>                                                        (0 + 1) / 1][Stage 146:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:10:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 146:>                                                        (0 + 1) / 1][Stage 146:>                                                        (0 + 1) / 1][Stage 146:>                                                        (0 + 1) / 1][Stage 146:>                                                        (0 + 1) / 1][Stage 146:>                                                        (0 + 1) / 1][Stage 146:>                                                        (0 + 1) / 1][Stage 146:>                                                        (0 + 1) / 1][Stage 146:>                                                        (0 + 1) / 1]                                                                                [Stage 147:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 147:>                                                        (0 + 1) / 1][Stage 147:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:21:36] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 147:>                                                        (0 + 1) / 1][Stage 147:>                                                        (0 + 1) / 1][Stage 147:>                                                        (0 + 1) / 1][Stage 147:>                                                        (0 + 1) / 1][Stage 147:>                                                        (0 + 1) / 1][Stage 147:>                                                        (0 + 1) / 1][Stage 147:>                                                        (0 + 1) / 1][Stage 147:>                                                        (0 + 1) / 1][Stage 147:>                                                        (0 + 1) / 1][Stage 147:>                                                        (0 + 1) / 1]                                                                                [Stage 148:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 148:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:32:42] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 148:>                                                        (0 + 1) / 1][Stage 148:>                                                        (0 + 1) / 1][Stage 148:>                                                        (0 + 1) / 1][Stage 148:>                                                        (0 + 1) / 1][Stage 148:>                                                        (0 + 1) / 1]                                                                                [Stage 149:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:38:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 149:>                                                        (0 + 1) / 1][Stage 149:>                                                        (0 + 1) / 1]                                                                                [Stage 150:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:43:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1][Stage 150:>                                                        (0 + 1) / 1]                                                                                [Stage 151:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 151:>                                                        (0 + 1) / 1][Stage 151:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:59:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 151:>                                                        (0 + 1) / 1][Stage 151:>                                                        (0 + 1) / 1][Stage 151:>                                                        (0 + 1) / 1][Stage 151:>                                                        (0 + 1) / 1][Stage 151:>                                                        (0 + 1) / 1][Stage 151:>                                                        (0 + 1) / 1][Stage 151:>                                                        (0 + 1) / 1][Stage 151:>                                                        (0 + 1) / 1][Stage 151:>                                                        (0 + 1) / 1]                                                                                [Stage 152:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 152:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:10:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 152:>                                                        (0 + 1) / 1][Stage 152:>                                                        (0 + 1) / 1][Stage 152:>                                                        (0 + 1) / 1][Stage 152:>                                                        (0 + 1) / 1]                                                                                [Stage 153:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:15:20] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 153:>                                                        (0 + 1) / 1][Stage 153:>                                                        (0 + 1) / 1][Stage 153:>                                                        (0 + 1) / 1]                                                                                [Stage 154:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 154:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:19:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 154:>                                                        (0 + 1) / 1][Stage 154:>                                                        (0 + 1) / 1][Stage 154:>                                                        (0 + 1) / 1][Stage 154:>                                                        (0 + 1) / 1][Stage 154:>                                                        (0 + 1) / 1][Stage 154:>                                                        (0 + 1) / 1]                                                                                [Stage 155:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 155:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:26:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 155:>                                                        (0 + 1) / 1][Stage 155:>                                                        (0 + 1) / 1][Stage 155:>                                                        (0 + 1) / 1][Stage 155:>                                                        (0 + 1) / 1]                                                                                [Stage 156:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 156:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:32:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 156:>                                                        (0 + 1) / 1][Stage 156:>                                                        (0 + 1) / 1][Stage 156:>                                                        (0 + 1) / 1][Stage 156:>                                                        (0 + 1) / 1][Stage 156:>                                                        (0 + 1) / 1]                                                                                [Stage 157:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:38:36] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 157:>                                                        (0 + 1) / 1][Stage 157:>                                                        (0 + 1) / 1][Stage 157:>                                                        (0 + 1) / 1][Stage 157:>                                                        (0 + 1) / 1]                                                                                [Stage 158:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 158:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:43:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 158:>                                                        (0 + 1) / 1][Stage 158:>                                                        (0 + 1) / 1][Stage 158:>                                                        (0 + 1) / 1][Stage 158:>                                                        (0 + 1) / 1][Stage 158:>                                                        (0 + 1) / 1][Stage 158:>                                                        (0 + 1) / 1]                                                                                [Stage 159:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 159:>                                                        (0 + 1) / 1][Stage 159:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:52:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 159:>                                                        (0 + 1) / 1][Stage 159:>                                                        (0 + 1) / 1][Stage 159:>                                                        (0 + 1) / 1][Stage 159:>                                                        (0 + 1) / 1][Stage 159:>                                                        (0 + 1) / 1][Stage 159:>                                                        (0 + 1) / 1][Stage 159:>                                                        (0 + 1) / 1][Stage 159:>                                                        (0 + 1) / 1][Stage 159:>                                                        (0 + 1) / 1]                                                                                [Stage 160:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 160:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:03:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 160:>                                                        (0 + 1) / 1][Stage 160:>                                                        (0 + 1) / 1][Stage 160:>                                                        (0 + 1) / 1][Stage 160:>                                                        (0 + 1) / 1][Stage 160:>                                                        (0 + 1) / 1][Stage 160:>                                                        (0 + 1) / 1][Stage 160:>                                                        (0 + 1) / 1][Stage 160:>                                                        (0 + 1) / 1]                                                                                [Stage 161:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:15:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1][Stage 161:>                                                        (0 + 1) / 1]                                                                                [Stage 162:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 162:>                                                        (0 + 1) / 1][Stage 162:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:32:48] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 162:>                                                        (0 + 1) / 1][Stage 162:>                                                        (0 + 1) / 1][Stage 162:>                                                        (0 + 1) / 1][Stage 162:>                                                        (0 + 1) / 1][Stage 162:>                                                        (0 + 1) / 1][Stage 162:>                                                        (0 + 1) / 1][Stage 162:>                                                        (0 + 1) / 1][Stage 162:>                                                        (0 + 1) / 1]                                                                                [Stage 163:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 163:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:42:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 163:>                                                        (0 + 1) / 1][Stage 163:>                                                        (0 + 1) / 1][Stage 163:>                                                        (0 + 1) / 1][Stage 163:>                                                        (0 + 1) / 1][Stage 163:>                                                        (0 + 1) / 1][Stage 163:>                                                        (0 + 1) / 1][Stage 163:>                                                        (0 + 1) / 1]                                                                                [Stage 164:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 164:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:51:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 164:>                                                        (0 + 1) / 1][Stage 164:>                                                        (0 + 1) / 1][Stage 164:>                                                        (0 + 1) / 1][Stage 164:>                                                        (0 + 1) / 1][Stage 164:>                                                        (0 + 1) / 1][Stage 164:>                                                        (0 + 1) / 1][Stage 164:>                                                        (0 + 1) / 1][Stage 164:>                                                        (0 + 1) / 1]                                                                                [Stage 165:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:59:52] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
                                                                                [Stage 166:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:04:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1][Stage 166:>                                                        (0 + 1) / 1]                                                                                [Stage 167:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 167:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:23:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 167:>                                                        (0 + 1) / 1][Stage 167:>                                                        (0 + 1) / 1][Stage 167:>                                                        (0 + 1) / 1][Stage 167:>                                                        (0 + 1) / 1]                                                                                [Stage 168:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:32:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1][Stage 168:>                                                        (0 + 1) / 1]                                                                                [Stage 169:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 169:>                                                        (0 + 1) / 1][Stage 169:>                                                        (0 + 1) / 1][Stage 169:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:57:48] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 169:>                                                        (0 + 1) / 1][Stage 169:>                                                        (0 + 1) / 1][Stage 169:>                                                        (0 + 1) / 1][Stage 169:>                                                        (0 + 1) / 1][Stage 169:>                                                        (0 + 1) / 1][Stage 169:>                                                        (0 + 1) / 1][Stage 169:>                                                        (0 + 1) / 1][Stage 169:>                                                        (0 + 1) / 1][Stage 169:>                                                        (0 + 1) / 1][Stage 169:>                                                        (0 + 1) / 1][Stage 169:>                                                        (0 + 1) / 1]                                                                                [Stage 170:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:13:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1][Stage 170:>                                                        (0 + 1) / 1]                                                                                [Stage 171:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:31:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1][Stage 171:>                                                        (0 + 1) / 1]                                                                                [Stage 172:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 172:>                                                        (0 + 1) / 1][Stage 172:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:48:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 172:>                                                        (0 + 1) / 1][Stage 172:>                                                        (0 + 1) / 1][Stage 172:>                                                        (0 + 1) / 1][Stage 172:>                                                        (0 + 1) / 1][Stage 172:>                                                        (0 + 1) / 1][Stage 172:>                                                        (0 + 1) / 1][Stage 172:>                                                        (0 + 1) / 1][Stage 172:>                                                        (0 + 1) / 1][Stage 172:>                                                        (0 + 1) / 1][Stage 172:>                                                        (0 + 1) / 1]                                                                                [Stage 173:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:01:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1][Stage 173:>                                                        (0 + 1) / 1]                                                                                [Stage 174:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:18:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1][Stage 174:>                                                        (0 + 1) / 1]                                                                                [Stage 175:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 175:>                                                        (0 + 1) / 1][Stage 175:>                                                        (0 + 1) / 1][Stage 175:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:35:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 175:>                                                        (0 + 1) / 1][Stage 175:>                                                        (0 + 1) / 1][Stage 175:>                                                        (0 + 1) / 1][Stage 175:>                                                        (0 + 1) / 1][Stage 175:>                                                        (0 + 1) / 1][Stage 175:>                                                        (0 + 1) / 1][Stage 175:>                                                        (0 + 1) / 1][Stage 175:>                                                        (0 + 1) / 1][Stage 175:>                                                        (0 + 1) / 1][Stage 175:>                                                        (0 + 1) / 1][Stage 175:>                                                        (0 + 1) / 1]                                                                                [Stage 176:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:51:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1][Stage 176:>                                                        (0 + 1) / 1]                                                                                [Stage 177:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:11:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1][Stage 177:>                                                        (0 + 1) / 1]                                                                                [Stage 178:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 178:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:26:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 178:>                                                        (0 + 1) / 1][Stage 178:>                                                        (0 + 1) / 1][Stage 178:>                                                        (0 + 1) / 1][Stage 178:>                                                        (0 + 1) / 1][Stage 178:>                                                        (0 + 1) / 1][Stage 178:>                                                        (0 + 1) / 1][Stage 178:>                                                        (0 + 1) / 1][Stage 178:>                                                        (0 + 1) / 1]                                                                                [Stage 179:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 179:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:35:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 179:>                                                        (0 + 1) / 1][Stage 179:>                                                        (0 + 1) / 1][Stage 179:>                                                        (0 + 1) / 1][Stage 179:>                                                        (0 + 1) / 1][Stage 179:>                                                        (0 + 1) / 1][Stage 179:>                                                        (0 + 1) / 1][Stage 179:>                                                        (0 + 1) / 1][Stage 179:>                                                        (0 + 1) / 1][Stage 179:>                                                        (0 + 1) / 1][Stage 179:>                                                        (0 + 1) / 1]                                                                                [Stage 180:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 180:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:46:47] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 180:>                                                        (0 + 1) / 1][Stage 180:>                                                        (0 + 1) / 1][Stage 180:>                                                        (0 + 1) / 1][Stage 180:>                                                        (0 + 1) / 1][Stage 180:>                                                        (0 + 1) / 1][Stage 180:>                                                        (0 + 1) / 1][Stage 180:>                                                        (0 + 1) / 1]                                                                                [Stage 181:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 181:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:54:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 181:>                                                        (0 + 1) / 1][Stage 181:>                                                        (0 + 1) / 1][Stage 181:>                                                        (0 + 1) / 1][Stage 181:>                                                        (0 + 1) / 1]                                                                                [Stage 182:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 182:>                                                        (0 + 1) / 1][Stage 182:>                                                        (0 + 1) / 1][Stage 182:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:02:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 182:>                                                        (0 + 1) / 1][Stage 182:>                                                        (0 + 1) / 1][Stage 182:>                                                        (0 + 1) / 1][Stage 182:>                                                        (0 + 1) / 1][Stage 182:>                                                        (0 + 1) / 1][Stage 182:>                                                        (0 + 1) / 1][Stage 182:>                                                        (0 + 1) / 1][Stage 182:>                                                        (0 + 1) / 1][Stage 182:>                                                        (0 + 1) / 1][Stage 182:>                                                        (0 + 1) / 1][Stage 182:>                                                        (0 + 1) / 1][Stage 182:>                                                        (0 + 1) / 1]                                                                                [Stage 183:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:14:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 183:>                                                        (0 + 1) / 1][Stage 183:>                                                        (0 + 1) / 1][Stage 183:>                                                        (0 + 1) / 1]                                                                                [Stage 184:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 184:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:18:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 184:>                                                        (0 + 1) / 1][Stage 184:>                                                        (0 + 1) / 1][Stage 184:>                                                        (0 + 1) / 1]                                                                                [Stage 185:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 185:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:24:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 185:>                                                        (0 + 1) / 1][Stage 185:>                                                        (0 + 1) / 1][Stage 185:>                                                        (0 + 1) / 1][Stage 185:>                                                        (0 + 1) / 1][Stage 185:>                                                        (0 + 1) / 1][Stage 185:>                                                        (0 + 1) / 1][Stage 185:>                                                        (0 + 1) / 1][Stage 185:>                                                        (0 + 1) / 1]                                                                                [Stage 186:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:36:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1][Stage 186:>                                                        (0 + 1) / 1]                                                                                [Stage 187:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 187:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:52:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 187:>                                                        (0 + 1) / 1][Stage 187:>                                                        (0 + 1) / 1][Stage 187:>                                                        (0 + 1) / 1][Stage 187:>                                                        (0 + 1) / 1][Stage 187:>                                                        (0 + 1) / 1][Stage 187:>                                                        (0 + 1) / 1][Stage 187:>                                                        (0 + 1) / 1][Stage 187:>                                                        (0 + 1) / 1]                                                                                [Stage 188:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:03:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1][Stage 188:>                                                        (0 + 1) / 1]                                                                                [Stage 189:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:22:11] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1][Stage 189:>                                                        (0 + 1) / 1]                                                                                [Stage 190:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 190:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:38:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 190:>                                                        (0 + 1) / 1][Stage 190:>                                                        (0 + 1) / 1][Stage 190:>                                                        (0 + 1) / 1][Stage 190:>                                                        (0 + 1) / 1][Stage 190:>                                                        (0 + 1) / 1]                                                                                [Stage 191:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 191:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:44:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 191:>                                                        (0 + 1) / 1][Stage 191:>                                                        (0 + 1) / 1][Stage 191:>                                                        (0 + 1) / 1][Stage 191:>                                                        (0 + 1) / 1][Stage 191:>                                                        (0 + 1) / 1][Stage 191:>                                                        (0 + 1) / 1][Stage 191:>                                                        (0 + 1) / 1]                                                                                [Stage 192:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 192:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:53:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 192:>                                                        (0 + 1) / 1][Stage 192:>                                                        (0 + 1) / 1][Stage 192:>                                                        (0 + 1) / 1][Stage 192:>                                                        (0 + 1) / 1][Stage 192:>                                                        (0 + 1) / 1][Stage 192:>                                                        (0 + 1) / 1][Stage 192:>                                                        (0 + 1) / 1][Stage 192:>                                                        (0 + 1) / 1]                                                                                [Stage 193:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:01:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 193:>                                                        (0 + 1) / 1][Stage 193:>                                                        (0 + 1) / 1]                                                                                [Stage 194:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 194:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:04:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 194:>                                                        (0 + 1) / 1][Stage 194:>                                                        (0 + 1) / 1][Stage 194:>                                                        (0 + 1) / 1][Stage 194:>                                                        (0 + 1) / 1]                                                                                [Stage 195:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:13:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1][Stage 195:>                                                        (0 + 1) / 1]                                                                                [Stage 196:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:34:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 196:>                                                        (0 + 1) / 1][Stage 196:>                                                        (0 + 1) / 1][Stage 196:>                                                        (0 + 1) / 1]                                                                                [Stage 197:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 197:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:38:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 197:>                                                        (0 + 1) / 1][Stage 197:>                                                        (0 + 1) / 1][Stage 197:>                                                        (0 + 1) / 1][Stage 197:>                                                        (0 + 1) / 1][Stage 197:>                                                        (0 + 1) / 1][Stage 197:>                                                        (0 + 1) / 1][Stage 197:>                                                        (0 + 1) / 1][Stage 197:>                                                        (0 + 1) / 1]                                                                                [Stage 198:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 198:>                                                        (0 + 1) / 1][Stage 198:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:48:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 198:>                                                        (0 + 1) / 1][Stage 198:>                                                        (0 + 1) / 1][Stage 198:>                                                        (0 + 1) / 1][Stage 198:>                                                        (0 + 1) / 1][Stage 198:>                                                        (0 + 1) / 1][Stage 198:>                                                        (0 + 1) / 1][Stage 198:>                                                        (0 + 1) / 1][Stage 198:>                                                        (0 + 1) / 1][Stage 198:>                                                        (0 + 1) / 1][Stage 198:>                                                        (0 + 1) / 1][Stage 198:>                                                        (0 + 1) / 1]                                                                                [Stage 199:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 199:>                                                        (0 + 1) / 1][Stage 199:>                                                        (0 + 1) / 1][Stage 199:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:03:23] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 199:>                                                        (0 + 1) / 1][Stage 199:>                                                        (0 + 1) / 1][Stage 199:>                                                        (0 + 1) / 1][Stage 199:>                                                        (0 + 1) / 1][Stage 199:>                                                        (0 + 1) / 1][Stage 199:>                                                        (0 + 1) / 1][Stage 199:>                                                        (0 + 1) / 1][Stage 199:>                                                        (0 + 1) / 1][Stage 199:>                                                        (0 + 1) / 1][Stage 199:>                                                        (0 + 1) / 1][Stage 199:>                                                        (0 + 1) / 1][Stage 199:>                                                        (0 + 1) / 1]                                                                                [Stage 200:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 200:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:17:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 200:>                                                        (0 + 1) / 1][Stage 200:>                                                        (0 + 1) / 1][Stage 200:>                                                        (0 + 1) / 1][Stage 200:>                                                        (0 + 1) / 1][Stage 200:>                                                        (0 + 1) / 1]                                                                                [Stage 201:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 201:>                                                        (0 + 1) / 1][Stage 201:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:25:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 201:>                                                        (0 + 1) / 1][Stage 201:>                                                        (0 + 1) / 1][Stage 201:>                                                        (0 + 1) / 1][Stage 201:>                                                        (0 + 1) / 1][Stage 201:>                                                        (0 + 1) / 1][Stage 201:>                                                        (0 + 1) / 1][Stage 201:>                                                        (0 + 1) / 1][Stage 201:>                                                        (0 + 1) / 1]                                                                                [Stage 202:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 202:>                                                        (0 + 1) / 1][Stage 202:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:36:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 202:>                                                        (0 + 1) / 1][Stage 202:>                                                        (0 + 1) / 1][Stage 202:>                                                        (0 + 1) / 1][Stage 202:>                                                        (0 + 1) / 1][Stage 202:>                                                        (0 + 1) / 1][Stage 202:>                                                        (0 + 1) / 1][Stage 202:>                                                        (0 + 1) / 1][Stage 202:>                                                        (0 + 1) / 1][Stage 202:>                                                        (0 + 1) / 1][Stage 202:>                                                        (0 + 1) / 1]                                                                                [Stage 203:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:49:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1][Stage 203:>                                                        (0 + 1) / 1]                                                                                [Stage 204:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:08:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1][Stage 204:>                                                        (0 + 1) / 1]                                                                                [Stage 205:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 205:>                                                        (0 + 1) / 1][Stage 205:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:23:31] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 205:>                                                        (0 + 1) / 1][Stage 205:>                                                        (0 + 1) / 1][Stage 205:>                                                        (0 + 1) / 1][Stage 205:>                                                        (0 + 1) / 1][Stage 205:>                                                        (0 + 1) / 1][Stage 205:>                                                        (0 + 1) / 1][Stage 205:>                                                        (0 + 1) / 1][Stage 205:>                                                        (0 + 1) / 1][Stage 205:>                                                        (0 + 1) / 1][Stage 205:>                                                        (0 + 1) / 1]                                                                                [Stage 206:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 206:>                                                        (0 + 1) / 1][Stage 206:>                                                        (0 + 1) / 1][Stage 206:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:36:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 206:>                                                        (0 + 1) / 1][Stage 206:>                                                        (0 + 1) / 1][Stage 206:>                                                        (0 + 1) / 1][Stage 206:>                                                        (0 + 1) / 1][Stage 206:>                                                        (0 + 1) / 1][Stage 206:>                                                        (0 + 1) / 1][Stage 206:>                                                        (0 + 1) / 1][Stage 206:>                                                        (0 + 1) / 1][Stage 206:>                                                        (0 + 1) / 1][Stage 206:>                                                        (0 + 1) / 1][Stage 206:>                                                        (0 + 1) / 1][Stage 206:>                                                        (0 + 1) / 1]                                                                                [Stage 207:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 207:>                                                        (0 + 1) / 1][Stage 207:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:52:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 207:>                                                        (0 + 1) / 1][Stage 207:>                                                        (0 + 1) / 1][Stage 207:>                                                        (0 + 1) / 1][Stage 207:>                                                        (0 + 1) / 1][Stage 207:>                                                        (0 + 1) / 1][Stage 207:>                                                        (0 + 1) / 1][Stage 207:>                                                        (0 + 1) / 1][Stage 207:>                                                        (0 + 1) / 1][Stage 207:>                                                        (0 + 1) / 1][Stage 207:>                                                        (0 + 1) / 1][Stage 207:>                                                        (0 + 1) / 1]                                                                                [Stage 208:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 208:>                                                        (0 + 1) / 1][Stage 208:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 208:>                                                        (0 + 1) / 1][Stage 208:>                                                        (0 + 1) / 1][Stage 208:>                                                        (0 + 1) / 1][Stage 208:>                                                        (0 + 1) / 1][Stage 208:>                                                        (0 + 1) / 1][Stage 208:>                                                        (0 + 1) / 1][Stage 208:>                                                        (0 + 1) / 1][Stage 208:>                                                        (0 + 1) / 1][Stage 208:>                                                        (0 + 1) / 1][Stage 208:>                                                        (0 + 1) / 1][Stage 208:>                                                        (0 + 1) / 1][Stage 208:>                                                        (0 + 1) / 1]                                                                                [Stage 209:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:20:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1][Stage 209:>                                                        (0 + 1) / 1]                                                                                [Stage 210:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:39:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1][Stage 210:>                                                        (0 + 1) / 1]                                                                                [Stage 211:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:03:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1][Stage 211:>                                                        (0 + 1) / 1]                                                                                [Stage 212:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:22:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1][Stage 212:>                                                        (0 + 1) / 1]                                                                                [Stage 213:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 213:>                                                        (0 + 1) / 1][Stage 213:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:40:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 213:>                                                        (0 + 1) / 1][Stage 213:>                                                        (0 + 1) / 1][Stage 213:>                                                        (0 + 1) / 1][Stage 213:>                                                        (0 + 1) / 1][Stage 213:>                                                        (0 + 1) / 1][Stage 213:>                                                        (0 + 1) / 1][Stage 213:>                                                        (0 + 1) / 1][Stage 213:>                                                        (0 + 1) / 1]                                                                                [Stage 214:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:52:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1][Stage 214:>                                                        (0 + 1) / 1]                                                                                [Stage 215:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 215:>                                                        (0 + 1) / 1][Stage 215:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:08:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 215:>                                                        (0 + 1) / 1][Stage 215:>                                                        (0 + 1) / 1][Stage 215:>                                                        (0 + 1) / 1][Stage 215:>                                                        (0 + 1) / 1][Stage 215:>                                                        (0 + 1) / 1][Stage 215:>                                                        (0 + 1) / 1][Stage 215:>                                                        (0 + 1) / 1]                                                                                [Stage 216:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 216:>                                                        (0 + 1) / 1][Stage 216:>                                                        (0 + 1) / 1][Stage 216:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:19:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 216:>                                                        (0 + 1) / 1][Stage 216:>                                                        (0 + 1) / 1][Stage 216:>                                                        (0 + 1) / 1][Stage 216:>                                                        (0 + 1) / 1][Stage 216:>                                                        (0 + 1) / 1][Stage 216:>                                                        (0 + 1) / 1][Stage 216:>                                                        (0 + 1) / 1][Stage 216:>                                                        (0 + 1) / 1][Stage 216:>                                                        (0 + 1) / 1][Stage 216:>                                                        (0 + 1) / 1]                                                                                [Stage 217:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 217:>                                                        (0 + 1) / 1][Stage 217:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:32:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 217:>                                                        (0 + 1) / 1][Stage 217:>                                                        (0 + 1) / 1][Stage 217:>                                                        (0 + 1) / 1][Stage 217:>                                                        (0 + 1) / 1][Stage 217:>                                                        (0 + 1) / 1][Stage 217:>                                                        (0 + 1) / 1][Stage 217:>                                                        (0 + 1) / 1][Stage 217:>                                                        (0 + 1) / 1][Stage 217:>                                                        (0 + 1) / 1]                                                                                [Stage 218:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 218:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:42:36] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 218:>                                                        (0 + 1) / 1][Stage 218:>                                                        (0 + 1) / 1][Stage 218:>                                                        (0 + 1) / 1][Stage 218:>                                                        (0 + 1) / 1][Stage 218:>                                                        (0 + 1) / 1]                                                                                [Stage 219:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 219:>                                                        (0 + 1) / 1][Stage 219:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:50:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 219:>                                                        (0 + 1) / 1][Stage 219:>                                                        (0 + 1) / 1][Stage 219:>                                                        (0 + 1) / 1][Stage 219:>                                                        (0 + 1) / 1][Stage 219:>                                                        (0 + 1) / 1][Stage 219:>                                                        (0 + 1) / 1][Stage 219:>                                                        (0 + 1) / 1][Stage 219:>                                                        (0 + 1) / 1][Stage 219:>                                                        (0 + 1) / 1][Stage 219:>                                                        (0 + 1) / 1][Stage 219:>                                                        (0 + 1) / 1]                                                                                [Stage 220:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 220:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:02:42] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 220:>                                                        (0 + 1) / 1][Stage 220:>                                                        (0 + 1) / 1][Stage 220:>                                                        (0 + 1) / 1][Stage 220:>                                                        (0 + 1) / 1][Stage 220:>                                                        (0 + 1) / 1][Stage 220:>                                                        (0 + 1) / 1][Stage 220:>                                                        (0 + 1) / 1]                                                                                [Stage 221:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 221:>                                                        (0 + 1) / 1][Stage 221:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:12:20] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 221:>                                                        (0 + 1) / 1][Stage 221:>                                                        (0 + 1) / 1][Stage 221:>                                                        (0 + 1) / 1][Stage 221:>                                                        (0 + 1) / 1][Stage 221:>                                                        (0 + 1) / 1][Stage 221:>                                                        (0 + 1) / 1][Stage 221:>                                                        (0 + 1) / 1][Stage 221:>                                                        (0 + 1) / 1][Stage 221:>                                                        (0 + 1) / 1][Stage 221:>                                                        (0 + 1) / 1][Stage 221:>                                                        (0 + 1) / 1]                                                                                [Stage 222:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 222:>                                                        (0 + 1) / 1][Stage 222:>                                                        (0 + 1) / 1][Stage 222:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:25:52] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 222:>                                                        (0 + 1) / 1][Stage 222:>                                                        (0 + 1) / 1][Stage 222:>                                                        (0 + 1) / 1][Stage 222:>                                                        (0 + 1) / 1][Stage 222:>                                                        (0 + 1) / 1][Stage 222:>                                                        (0 + 1) / 1][Stage 222:>                                                        (0 + 1) / 1][Stage 222:>                                                        (0 + 1) / 1][Stage 222:>                                                        (0 + 1) / 1][Stage 222:>                                                        (0 + 1) / 1][Stage 222:>                                                        (0 + 1) / 1][Stage 222:>                                                        (0 + 1) / 1]                                                                                [Stage 223:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 223:>                                                        (0 + 1) / 1][Stage 223:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:40:37] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 223:>                                                        (0 + 1) / 1][Stage 223:>                                                        (0 + 1) / 1][Stage 223:>                                                        (0 + 1) / 1][Stage 223:>                                                        (0 + 1) / 1][Stage 223:>                                                        (0 + 1) / 1][Stage 223:>                                                        (0 + 1) / 1][Stage 223:>                                                        (0 + 1) / 1][Stage 223:>                                                        (0 + 1) / 1][Stage 223:>                                                        (0 + 1) / 1][Stage 223:>                                                        (0 + 1) / 1]                                                                                [Stage 224:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 224:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:51:47] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 224:>                                                        (0 + 1) / 1][Stage 224:>                                                        (0 + 1) / 1][Stage 224:>                                                        (0 + 1) / 1][Stage 224:>                                                        (0 + 1) / 1][Stage 224:>                                                        (0 + 1) / 1]                                                                                [Stage 225:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:00:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1][Stage 225:>                                                        (0 + 1) / 1]                                                                                [Stage 226:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:18:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1][Stage 226:>                                                        (0 + 1) / 1]                                                                                [Stage 227:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 227:>                                                        (0 + 1) / 1][Stage 227:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:37:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 227:>                                                        (0 + 1) / 1][Stage 227:>                                                        (0 + 1) / 1][Stage 227:>                                                        (0 + 1) / 1][Stage 227:>                                                        (0 + 1) / 1][Stage 227:>                                                        (0 + 1) / 1][Stage 227:>                                                        (0 + 1) / 1][Stage 227:>                                                        (0 + 1) / 1][Stage 227:>                                                        (0 + 1) / 1][Stage 227:>                                                        (0 + 1) / 1][Stage 227:>                                                        (0 + 1) / 1][Stage 227:>                                                        (0 + 1) / 1][Stage 227:>                                                        (0 + 1) / 1]                                                                                [Stage 228:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:54:20] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1][Stage 228:>                                                        (0 + 1) / 1]                                                                                [Stage 229:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:16:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1][Stage 229:>                                                        (0 + 1) / 1]                                                                                [Stage 230:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 230:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:41:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 230:>                                                        (0 + 1) / 1][Stage 230:>                                                        (0 + 1) / 1][Stage 230:>                                                        (0 + 1) / 1][Stage 230:>                                                        (0 + 1) / 1]                                                                                [Stage 231:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 231:>                                                        (0 + 1) / 1][Stage 231:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:47:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 231:>                                                        (0 + 1) / 1][Stage 231:>                                                        (0 + 1) / 1][Stage 231:>                                                        (0 + 1) / 1][Stage 231:>                                                        (0 + 1) / 1][Stage 231:>                                                        (0 + 1) / 1][Stage 231:>                                                        (0 + 1) / 1][Stage 231:>                                                        (0 + 1) / 1][Stage 231:>                                                        (0 + 1) / 1]                                                                                [Stage 232:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 232:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:57:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 232:>                                                        (0 + 1) / 1][Stage 232:>                                                        (0 + 1) / 1][Stage 232:>                                                        (0 + 1) / 1]                                                                                [Stage 233:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:01:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 233:>                                                        (0 + 1) / 1][Stage 233:>                                                        (0 + 1) / 1]                                                                                [Stage 234:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 234:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:05:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 234:>                                                        (0 + 1) / 1][Stage 234:>                                                        (0 + 1) / 1][Stage 234:>                                                        (0 + 1) / 1][Stage 234:>                                                        (0 + 1) / 1][Stage 234:>                                                        (0 + 1) / 1][Stage 234:>                                                        (0 + 1) / 1][Stage 234:>                                                        (0 + 1) / 1][Stage 234:>                                                        (0 + 1) / 1]                                                                                [Stage 235:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 235:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:14:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 235:>                                                        (0 + 1) / 1][Stage 235:>                                                        (0 + 1) / 1][Stage 235:>                                                        (0 + 1) / 1][Stage 235:>                                                        (0 + 1) / 1][Stage 235:>                                                        (0 + 1) / 1]                                                                                [Stage 236:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:23:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1][Stage 236:>                                                        (0 + 1) / 1]                                                                                [Stage 237:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:41:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1][Stage 237:>                                                        (0 + 1) / 1]                                                                                [Stage 238:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 238:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:56:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 238:>                                                        (0 + 1) / 1][Stage 238:>                                                        (0 + 1) / 1][Stage 238:>                                                        (0 + 1) / 1][Stage 238:>                                                        (0 + 1) / 1]                                                                                [Stage 239:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 239:>                                                        (0 + 1) / 1][Stage 239:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:03:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 239:>                                                        (0 + 1) / 1][Stage 239:>                                                        (0 + 1) / 1][Stage 239:>                                                        (0 + 1) / 1][Stage 239:>                                                        (0 + 1) / 1][Stage 239:>                                                        (0 + 1) / 1][Stage 239:>                                                        (0 + 1) / 1][Stage 239:>                                                        (0 + 1) / 1][Stage 239:>                                                        (0 + 1) / 1][Stage 239:>                                                        (0 + 1) / 1][Stage 239:>                                                        (0 + 1) / 1]                                                                                [Stage 240:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:17:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1][Stage 240:>                                                        (0 + 1) / 1]                                                                                [Stage 241:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:39:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1][Stage 241:>                                                        (0 + 1) / 1]                                                                                [Stage 242:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 242:>                                                        (0 + 1) / 1][Stage 242:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:55:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 242:>                                                        (0 + 1) / 1][Stage 242:>                                                        (0 + 1) / 1][Stage 242:>                                                        (0 + 1) / 1][Stage 242:>                                                        (0 + 1) / 1][Stage 242:>                                                        (0 + 1) / 1][Stage 242:>                                                        (0 + 1) / 1][Stage 242:>                                                        (0 + 1) / 1]                                                                                [Stage 243:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 243:>                                                        (0 + 1) / 1][Stage 243:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:05:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 243:>                                                        (0 + 1) / 1][Stage 243:>                                                        (0 + 1) / 1][Stage 243:>                                                        (0 + 1) / 1][Stage 243:>                                                        (0 + 1) / 1][Stage 243:>                                                        (0 + 1) / 1][Stage 243:>                                                        (0 + 1) / 1][Stage 243:>                                                        (0 + 1) / 1][Stage 243:>                                                        (0 + 1) / 1]                                                                                [Stage 244:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 244:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:15:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 244:>                                                        (0 + 1) / 1][Stage 244:>                                                        (0 + 1) / 1][Stage 244:>                                                        (0 + 1) / 1][Stage 244:>                                                        (0 + 1) / 1]                                                                                [Stage 245:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 245:>                                                        (0 + 1) / 1][Stage 245:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:22:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 245:>                                                        (0 + 1) / 1][Stage 245:>                                                        (0 + 1) / 1][Stage 245:>                                                        (0 + 1) / 1][Stage 245:>                                                        (0 + 1) / 1][Stage 245:>                                                        (0 + 1) / 1][Stage 245:>                                                        (0 + 1) / 1][Stage 245:>                                                        (0 + 1) / 1][Stage 245:>                                                        (0 + 1) / 1][Stage 245:>                                                        (0 + 1) / 1][Stage 245:>                                                        (0 + 1) / 1]                                                                                [Stage 246:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 246:>                                                        (0 + 1) / 1][Stage 246:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:34:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 246:>                                                        (0 + 1) / 1][Stage 246:>                                                        (0 + 1) / 1][Stage 246:>                                                        (0 + 1) / 1][Stage 246:>                                                        (0 + 1) / 1][Stage 246:>                                                        (0 + 1) / 1][Stage 246:>                                                        (0 + 1) / 1][Stage 246:>                                                        (0 + 1) / 1][Stage 246:>                                                        (0 + 1) / 1][Stage 246:>                                                        (0 + 1) / 1]                                                                                [Stage 247:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 247:>                                                        (0 + 1) / 1][Stage 247:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:45:47] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 247:>                                                        (0 + 1) / 1][Stage 247:>                                                        (0 + 1) / 1][Stage 247:>                                                        (0 + 1) / 1][Stage 247:>                                                        (0 + 1) / 1][Stage 247:>                                                        (0 + 1) / 1][Stage 247:>                                                        (0 + 1) / 1][Stage 247:>                                                        (0 + 1) / 1][Stage 247:>                                                        (0 + 1) / 1][Stage 247:>                                                        (0 + 1) / 1][Stage 247:>                                                        (0 + 1) / 1][Stage 247:>                                                        (0 + 1) / 1]                                                                                [Stage 248:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 248:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:57:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 248:>                                                        (0 + 1) / 1][Stage 248:>                                                        (0 + 1) / 1][Stage 248:>                                                        (0 + 1) / 1][Stage 248:>                                                        (0 + 1) / 1][Stage 248:>                                                        (0 + 1) / 1]                                                                                [Stage 249:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 249:>                                                        (0 + 1) / 1][Stage 249:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:05:44] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 249:>                                                        (0 + 1) / 1][Stage 249:>                                                        (0 + 1) / 1][Stage 249:>                                                        (0 + 1) / 1][Stage 249:>                                                        (0 + 1) / 1][Stage 249:>                                                        (0 + 1) / 1][Stage 249:>                                                        (0 + 1) / 1][Stage 249:>                                                        (0 + 1) / 1][Stage 249:>                                                        (0 + 1) / 1][Stage 249:>                                                        (0 + 1) / 1][Stage 249:>                                                        (0 + 1) / 1]                                                                                [Stage 250:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 250:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:17:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 250:>                                                        (0 + 1) / 1][Stage 250:>                                                        (0 + 1) / 1][Stage 250:>                                                        (0 + 1) / 1][Stage 250:>                                                        (0 + 1) / 1][Stage 250:>                                                        (0 + 1) / 1][Stage 250:>                                                        (0 + 1) / 1][Stage 250:>                                                        (0 + 1) / 1]                                                                                [Stage 251:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 251:>                                                        (0 + 1) / 1][Stage 251:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:26:32] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 251:>                                                        (0 + 1) / 1][Stage 251:>                                                        (0 + 1) / 1][Stage 251:>                                                        (0 + 1) / 1][Stage 251:>                                                        (0 + 1) / 1][Stage 251:>                                                        (0 + 1) / 1][Stage 251:>                                                        (0 + 1) / 1][Stage 251:>                                                        (0 + 1) / 1][Stage 251:>                                                        (0 + 1) / 1]                                                                                [Stage 252:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 252:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:36:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 252:>                                                        (0 + 1) / 1][Stage 252:>                                                        (0 + 1) / 1][Stage 252:>                                                        (0 + 1) / 1][Stage 252:>                                                        (0 + 1) / 1][Stage 252:>                                                        (0 + 1) / 1][Stage 252:>                                                        (0 + 1) / 1][Stage 252:>                                                        (0 + 1) / 1]                                                                                [Stage 253:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:44:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 253:>                                                        (0 + 1) / 1][Stage 253:>                                                        (0 + 1) / 1][Stage 253:>                                                        (0 + 1) / 1][Stage 253:>                                                        (0 + 1) / 1]                                                                                [Stage 254:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:52:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1][Stage 254:>                                                        (0 + 1) / 1]                                                                                [Stage 255:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 255:>                                                        (0 + 1) / 1][Stage 255:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:13:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 255:>                                                        (0 + 1) / 1][Stage 255:>                                                        (0 + 1) / 1][Stage 255:>                                                        (0 + 1) / 1][Stage 255:>                                                        (0 + 1) / 1][Stage 255:>                                                        (0 + 1) / 1][Stage 255:>                                                        (0 + 1) / 1][Stage 255:>                                                        (0 + 1) / 1][Stage 255:>                                                        (0 + 1) / 1]                                                                                [Stage 256:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:24:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1][Stage 256:>                                                        (0 + 1) / 1]                                                                                [Stage 257:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 257:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:40:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 257:>                                                        (0 + 1) / 1][Stage 257:>                                                        (0 + 1) / 1][Stage 257:>                                                        (0 + 1) / 1][Stage 257:>                                                        (0 + 1) / 1][Stage 257:>                                                        (0 + 1) / 1][Stage 257:>                                                        (0 + 1) / 1][Stage 257:>                                                        (0 + 1) / 1]                                                                                [Stage 258:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:51:52] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1][Stage 258:>                                                        (0 + 1) / 1]                                                                                [Stage 259:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 259:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:14:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 259:>                                                        (0 + 1) / 1][Stage 259:>                                                        (0 + 1) / 1][Stage 259:>                                                        (0 + 1) / 1][Stage 259:>                                                        (0 + 1) / 1][Stage 259:>                                                        (0 + 1) / 1]                                                                                [Stage 260:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:24:37] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1][Stage 260:>                                                        (0 + 1) / 1]                                                                                [Stage 261:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 261:>                                                        (0 + 1) / 1][Stage 261:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:47:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 261:>                                                        (0 + 1) / 1][Stage 261:>                                                        (0 + 1) / 1][Stage 261:>                                                        (0 + 1) / 1][Stage 261:>                                                        (0 + 1) / 1][Stage 261:>                                                        (0 + 1) / 1][Stage 261:>                                                        (0 + 1) / 1][Stage 261:>                                                        (0 + 1) / 1][Stage 261:>                                                        (0 + 1) / 1][Stage 261:>                                                        (0 + 1) / 1][Stage 261:>                                                        (0 + 1) / 1]                                                                                [Stage 262:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 262:>                                                        (0 + 1) / 1][Stage 262:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:59:48] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 262:>                                                        (0 + 1) / 1][Stage 262:>                                                        (0 + 1) / 1][Stage 262:>                                                        (0 + 1) / 1][Stage 262:>                                                        (0 + 1) / 1][Stage 262:>                                                        (0 + 1) / 1][Stage 262:>                                                        (0 + 1) / 1][Stage 262:>                                                        (0 + 1) / 1][Stage 262:>                                                        (0 + 1) / 1][Stage 262:>                                                        (0 + 1) / 1][Stage 262:>                                                        (0 + 1) / 1][Stage 262:>                                                        (0 + 1) / 1]                                                                                [Stage 263:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 263:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:12:33] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 263:>                                                        (0 + 1) / 1][Stage 263:>                                                        (0 + 1) / 1][Stage 263:>                                                        (0 + 1) / 1][Stage 263:>                                                        (0 + 1) / 1][Stage 263:>                                                        (0 + 1) / 1][Stage 263:>                                                        (0 + 1) / 1][Stage 263:>                                                        (0 + 1) / 1]                                                                                [Stage 264:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:22:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1][Stage 264:>                                                        (0 + 1) / 1]                                                                                [Stage 265:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 265:>                                                        (0 + 1) / 1][Stage 265:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:38:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 265:>                                                        (0 + 1) / 1][Stage 265:>                                                        (0 + 1) / 1][Stage 265:>                                                        (0 + 1) / 1][Stage 265:>                                                        (0 + 1) / 1][Stage 265:>                                                        (0 + 1) / 1][Stage 265:>                                                        (0 + 1) / 1][Stage 265:>                                                        (0 + 1) / 1][Stage 265:>                                                        (0 + 1) / 1][Stage 265:>                                                        (0 + 1) / 1][Stage 265:>                                                        (0 + 1) / 1]                                                                                [Stage 266:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 266:>                                                        (0 + 1) / 1][Stage 266:>                                                        (0 + 1) / 1][Stage 266:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:51:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 266:>                                                        (0 + 1) / 1][Stage 266:>                                                        (0 + 1) / 1][Stage 266:>                                                        (0 + 1) / 1][Stage 266:>                                                        (0 + 1) / 1][Stage 266:>                                                        (0 + 1) / 1][Stage 266:>                                                        (0 + 1) / 1][Stage 266:>                                                        (0 + 1) / 1][Stage 266:>                                                        (0 + 1) / 1][Stage 266:>                                                        (0 + 1) / 1][Stage 266:>                                                        (0 + 1) / 1][Stage 266:>                                                        (0 + 1) / 1][Stage 266:>                                                        (0 + 1) / 1]                                                                                [Stage 267:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:07:37] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1][Stage 267:>                                                        (0 + 1) / 1]                                                                                [Stage 268:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:25:37] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1][Stage 268:>                                                        (0 + 1) / 1]                                                                                [Stage 269:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:47:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1][Stage 269:>                                                        (0 + 1) / 1]                                                                                [Stage 270:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 270:>                                                        (0 + 1) / 1][Stage 270:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:09:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 270:>                                                        (0 + 1) / 1][Stage 270:>                                                        (0 + 1) / 1][Stage 270:>                                                        (0 + 1) / 1][Stage 270:>                                                        (0 + 1) / 1][Stage 270:>                                                        (0 + 1) / 1][Stage 270:>                                                        (0 + 1) / 1][Stage 270:>                                                        (0 + 1) / 1][Stage 270:>                                                        (0 + 1) / 1][Stage 270:>                                                        (0 + 1) / 1][Stage 270:>                                                        (0 + 1) / 1][Stage 270:>                                                        (0 + 1) / 1][Stage 270:>                                                        (0 + 1) / 1]                                                                                [Stage 271:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 271:>                                                        (0 + 1) / 1][Stage 271:>                                                        (0 + 1) / 1][Stage 271:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:24:37] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 271:>                                                        (0 + 1) / 1][Stage 271:>                                                        (0 + 1) / 1][Stage 271:>                                                        (0 + 1) / 1][Stage 271:>                                                        (0 + 1) / 1][Stage 271:>                                                        (0 + 1) / 1][Stage 271:>                                                        (0 + 1) / 1][Stage 271:>                                                        (0 + 1) / 1][Stage 271:>                                                        (0 + 1) / 1][Stage 271:>                                                        (0 + 1) / 1][Stage 271:>                                                        (0 + 1) / 1][Stage 271:>                                                        (0 + 1) / 1][Stage 271:>                                                        (0 + 1) / 1]                                                                                [Stage 272:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 272:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:38:33] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 272:>                                                        (0 + 1) / 1][Stage 272:>                                                        (0 + 1) / 1][Stage 272:>                                                        (0 + 1) / 1][Stage 272:>                                                        (0 + 1) / 1][Stage 272:>                                                        (0 + 1) / 1][Stage 272:>                                                        (0 + 1) / 1][Stage 272:>                                                        (0 + 1) / 1]                                                                                [Stage 273:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 273:>                                                        (0 + 1) / 1][Stage 273:>                                                        (0 + 1) / 1][Stage 273:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:48:42] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 273:>                                                        (0 + 1) / 1][Stage 273:>                                                        (0 + 1) / 1][Stage 273:>                                                        (0 + 1) / 1][Stage 273:>                                                        (0 + 1) / 1][Stage 273:>                                                        (0 + 1) / 1][Stage 273:>                                                        (0 + 1) / 1][Stage 273:>                                                        (0 + 1) / 1][Stage 273:>                                                        (0 + 1) / 1][Stage 273:>                                                        (0 + 1) / 1][Stage 273:>                                                        (0 + 1) / 1][Stage 273:>                                                        (0 + 1) / 1]                                                                                [Stage 274:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 274:>                                                        (0 + 1) / 1][Stage 274:>                                                        (0 + 1) / 1][Stage 274:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:04:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 274:>                                                        (0 + 1) / 1][Stage 274:>                                                        (0 + 1) / 1][Stage 274:>                                                        (0 + 1) / 1][Stage 274:>                                                        (0 + 1) / 1][Stage 274:>                                                        (0 + 1) / 1][Stage 274:>                                                        (0 + 1) / 1][Stage 274:>                                                        (0 + 1) / 1][Stage 274:>                                                        (0 + 1) / 1][Stage 274:>                                                        (0 + 1) / 1][Stage 274:>                                                        (0 + 1) / 1][Stage 274:>                                                        (0 + 1) / 1][Stage 274:>                                                        (0 + 1) / 1]                                                                                [Stage 275:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 275:>                                                        (0 + 1) / 1][Stage 275:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:18:42] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 275:>                                                        (0 + 1) / 1][Stage 275:>                                                        (0 + 1) / 1][Stage 275:>                                                        (0 + 1) / 1][Stage 275:>                                                        (0 + 1) / 1][Stage 275:>                                                        (0 + 1) / 1][Stage 275:>                                                        (0 + 1) / 1][Stage 275:>                                                        (0 + 1) / 1][Stage 275:>                                                        (0 + 1) / 1]                                                                                [Stage 276:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:32:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1][Stage 276:>                                                        (0 + 1) / 1]                                                                                [Stage 277:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 277:>                                                        (0 + 1) / 1][Stage 277:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:52:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 277:>                                                        (0 + 1) / 1][Stage 277:>                                                        (0 + 1) / 1][Stage 277:>                                                        (0 + 1) / 1][Stage 277:>                                                        (0 + 1) / 1][Stage 277:>                                                        (0 + 1) / 1][Stage 277:>                                                        (0 + 1) / 1][Stage 277:>                                                        (0 + 1) / 1][Stage 277:>                                                        (0 + 1) / 1][Stage 277:>                                                        (0 + 1) / 1]                                                                                [Stage 278:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 278:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:03:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 278:>                                                        (0 + 1) / 1][Stage 278:>                                                        (0 + 1) / 1][Stage 278:>                                                        (0 + 1) / 1][Stage 278:>                                                        (0 + 1) / 1][Stage 278:>                                                        (0 + 1) / 1][Stage 278:>                                                        (0 + 1) / 1]                                                                                [Stage 279:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 279:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:11:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 279:>                                                        (0 + 1) / 1][Stage 279:>                                                        (0 + 1) / 1][Stage 279:>                                                        (0 + 1) / 1][Stage 279:>                                                        (0 + 1) / 1][Stage 279:>                                                        (0 + 1) / 1][Stage 279:>                                                        (0 + 1) / 1]                                                                                Total Trials: 140: 140 succeeded, 0 failed, 0 cancelled.
[Stage 280:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:37:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 280:>                                                        (0 + 1) / 1][Stage 280:>                                                        (0 + 1) / 1][Stage 280:>                                                        (0 + 1) / 1]                                                                                [Stage 281:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 281:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:41:11] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 281:>                                                        (0 + 1) / 1][Stage 281:>                                                        (0 + 1) / 1][Stage 281:>                                                        (0 + 1) / 1][Stage 281:>                                                        (0 + 1) / 1][Stage 281:>                                                        (0 + 1) / 1]                                                                                [Stage 282:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:46:44] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 282:>                                                        (0 + 1) / 1][Stage 282:>                                                        (0 + 1) / 1][Stage 282:>                                                        (0 + 1) / 1]                                                                                [Stage 283:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 283:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:50:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 283:>                                                        (0 + 1) / 1][Stage 283:>                                                        (0 + 1) / 1][Stage 283:>                                                        (0 + 1) / 1]                                                                                [Stage 284:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 284:>                                                        (0 + 1) / 1][Stage 284:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:56:48] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 284:>                                                        (0 + 1) / 1][Stage 284:>                                                        (0 + 1) / 1][Stage 284:>                                                        (0 + 1) / 1][Stage 284:>                                                        (0 + 1) / 1][Stage 284:>                                                        (0 + 1) / 1][Stage 284:>                                                        (0 + 1) / 1][Stage 284:>                                                        (0 + 1) / 1][Stage 284:>                                                        (0 + 1) / 1][Stage 284:>                                                        (0 + 1) / 1][Stage 284:>                                                        (0 + 1) / 1]                                                                                [Stage 285:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:07:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 285:>                                                        (0 + 1) / 1][Stage 285:>                                                        (0 + 1) / 1][Stage 285:>                                                        (0 + 1) / 1]                                                                                [Stage 286:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 286:>                                                        (0 + 1) / 1][Stage 286:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:13:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 286:>                                                        (0 + 1) / 1][Stage 286:>                                                        (0 + 1) / 1][Stage 286:>                                                        (0 + 1) / 1][Stage 286:>                                                        (0 + 1) / 1][Stage 286:>                                                        (0 + 1) / 1][Stage 286:>                                                        (0 + 1) / 1][Stage 286:>                                                        (0 + 1) / 1][Stage 286:>                                                        (0 + 1) / 1][Stage 286:>                                                        (0 + 1) / 1][Stage 286:>                                                        (0 + 1) / 1][Stage 286:>                                                        (0 + 1) / 1][Stage 286:>                                                        (0 + 1) / 1]                                                                                [Stage 287:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 287:>                                                        (0 + 1) / 1][Stage 287:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:26:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 287:>                                                        (0 + 1) / 1][Stage 287:>                                                        (0 + 1) / 1][Stage 287:>                                                        (0 + 1) / 1][Stage 287:>                                                        (0 + 1) / 1][Stage 287:>                                                        (0 + 1) / 1][Stage 287:>                                                        (0 + 1) / 1][Stage 287:>                                                        (0 + 1) / 1][Stage 287:>                                                        (0 + 1) / 1]                                                                                [Stage 288:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 288:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:36:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 288:>                                                        (0 + 1) / 1][Stage 288:>                                                        (0 + 1) / 1][Stage 288:>                                                        (0 + 1) / 1][Stage 288:>                                                        (0 + 1) / 1]                                                                                [Stage 289:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:42:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 289:>                                                        (0 + 1) / 1][Stage 289:>                                                        (0 + 1) / 1][Stage 289:>                                                        (0 + 1) / 1]                                                                                [Stage 290:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:49:29] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1][Stage 290:>                                                        (0 + 1) / 1]                                                                                [Stage 291:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:08:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 291:>                                                        (0 + 1) / 1][Stage 291:>                                                        (0 + 1) / 1]                                                                                [Stage 292:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 292:>                                                        (0 + 1) / 1][Stage 292:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:13:11] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 292:>                                                        (0 + 1) / 1][Stage 292:>                                                        (0 + 1) / 1][Stage 292:>                                                        (0 + 1) / 1][Stage 292:>                                                        (0 + 1) / 1][Stage 292:>                                                        (0 + 1) / 1][Stage 292:>                                                        (0 + 1) / 1][Stage 292:>                                                        (0 + 1) / 1][Stage 292:>                                                        (0 + 1) / 1]                                                                                [Stage 293:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 293:>                                                        (0 + 1) / 1][Stage 293:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:23:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 293:>                                                        (0 + 1) / 1][Stage 293:>                                                        (0 + 1) / 1][Stage 293:>                                                        (0 + 1) / 1][Stage 293:>                                                        (0 + 1) / 1][Stage 293:>                                                        (0 + 1) / 1][Stage 293:>                                                        (0 + 1) / 1][Stage 293:>                                                        (0 + 1) / 1][Stage 293:>                                                        (0 + 1) / 1]                                                                                [Stage 294:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:33:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 294:>                                                        (0 + 1) / 1][Stage 294:>                                                        (0 + 1) / 1][Stage 294:>                                                        (0 + 1) / 1]                                                                                [Stage 295:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 295:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:37:11] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 295:>                                                        (0 + 1) / 1][Stage 295:>                                                        (0 + 1) / 1][Stage 295:>                                                        (0 + 1) / 1][Stage 295:>                                                        (0 + 1) / 1]                                                                                [Stage 296:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 296:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:42:34] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 296:>                                                        (0 + 1) / 1][Stage 296:>                                                        (0 + 1) / 1][Stage 296:>                                                        (0 + 1) / 1][Stage 296:>                                                        (0 + 1) / 1]                                                                                [Stage 297:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:52:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1][Stage 297:>                                                        (0 + 1) / 1]                                                                                [Stage 298:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:15:43] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 298:>                                                        (0 + 1) / 1][Stage 298:>                                                        (0 + 1) / 1][Stage 298:>                                                        (0 + 1) / 1][Stage 298:>                                                        (0 + 1) / 1]                                                                                [Stage 299:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:20:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 299:>                                                        (0 + 1) / 1][Stage 299:>                                                        (0 + 1) / 1][Stage 299:>                                                        (0 + 1) / 1][Stage 299:>                                                        (0 + 1) / 1]                                                                                [Stage 300:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 300:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:24:37] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 300:>                                                        (0 + 1) / 1][Stage 300:>                                                        (0 + 1) / 1][Stage 300:>                                                        (0 + 1) / 1][Stage 300:>                                                        (0 + 1) / 1][Stage 300:>                                                        (0 + 1) / 1]                                                                                [Stage 301:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 301:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:31:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 301:>                                                        (0 + 1) / 1][Stage 301:>                                                        (0 + 1) / 1][Stage 301:>                                                        (0 + 1) / 1][Stage 301:>                                                        (0 + 1) / 1][Stage 301:>                                                        (0 + 1) / 1][Stage 301:>                                                        (0 + 1) / 1][Stage 301:>                                                        (0 + 1) / 1][Stage 301:>                                                        (0 + 1) / 1]                                                                                [Stage 302:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 302:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:40:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 302:>                                                        (0 + 1) / 1][Stage 302:>                                                        (0 + 1) / 1][Stage 302:>                                                        (0 + 1) / 1][Stage 302:>                                                        (0 + 1) / 1]                                                                                [Stage 303:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 303:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:45:58] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 303:>                                                        (0 + 1) / 1][Stage 303:>                                                        (0 + 1) / 1][Stage 303:>                                                        (0 + 1) / 1]                                                                                [Stage 304:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 304:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:51:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 304:>                                                        (0 + 1) / 1][Stage 304:>                                                        (0 + 1) / 1][Stage 304:>                                                        (0 + 1) / 1][Stage 304:>                                                        (0 + 1) / 1][Stage 304:>                                                        (0 + 1) / 1][Stage 304:>                                                        (0 + 1) / 1]                                                                                [Stage 305:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 305:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:58:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 305:>                                                        (0 + 1) / 1][Stage 305:>                                                        (0 + 1) / 1][Stage 305:>                                                        (0 + 1) / 1]                                                                                [Stage 306:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 306:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:04:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 306:>                                                        (0 + 1) / 1][Stage 306:>                                                        (0 + 1) / 1][Stage 306:>                                                        (0 + 1) / 1][Stage 306:>                                                        (0 + 1) / 1][Stage 306:>                                                        (0 + 1) / 1][Stage 306:>                                                        (0 + 1) / 1][Stage 306:>                                                        (0 + 1) / 1]                                                                                [Stage 307:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 307:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:13:11] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 307:>                                                        (0 + 1) / 1][Stage 307:>                                                        (0 + 1) / 1][Stage 307:>                                                        (0 + 1) / 1][Stage 307:>                                                        (0 + 1) / 1][Stage 307:>                                                        (0 + 1) / 1][Stage 307:>                                                        (0 + 1) / 1][Stage 307:>                                                        (0 + 1) / 1][Stage 307:>                                                        (0 + 1) / 1]                                                                                [Stage 308:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:21:58] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 308:>                                                        (0 + 1) / 1][Stage 308:>                                                        (0 + 1) / 1][Stage 308:>                                                        (0 + 1) / 1]                                                                                [Stage 309:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 309:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:26:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 309:>                                                        (0 + 1) / 1][Stage 309:>                                                        (0 + 1) / 1][Stage 309:>                                                        (0 + 1) / 1][Stage 309:>                                                        (0 + 1) / 1][Stage 309:>                                                        (0 + 1) / 1]                                                                                [Stage 310:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 310:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:32:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 310:>                                                        (0 + 1) / 1][Stage 310:>                                                        (0 + 1) / 1][Stage 310:>                                                        (0 + 1) / 1][Stage 310:>                                                        (0 + 1) / 1][Stage 310:>                                                        (0 + 1) / 1]                                                                                [Stage 311:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 311:>                                                        (0 + 1) / 1][Stage 311:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:40:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 311:>                                                        (0 + 1) / 1][Stage 311:>                                                        (0 + 1) / 1][Stage 311:>                                                        (0 + 1) / 1][Stage 311:>                                                        (0 + 1) / 1][Stage 311:>                                                        (0 + 1) / 1][Stage 311:>                                                        (0 + 1) / 1][Stage 311:>                                                        (0 + 1) / 1][Stage 311:>                                                        (0 + 1) / 1]                                                                                [Stage 312:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 312:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:49:36] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 312:>                                                        (0 + 1) / 1][Stage 312:>                                                        (0 + 1) / 1][Stage 312:>                                                        (0 + 1) / 1][Stage 312:>                                                        (0 + 1) / 1]                                                                                [Stage 313:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 313:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:55:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 313:>                                                        (0 + 1) / 1][Stage 313:>                                                        (0 + 1) / 1][Stage 313:>                                                        (0 + 1) / 1][Stage 313:>                                                        (0 + 1) / 1]                                                                                [Stage 314:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 314:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:01:37] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 314:>                                                        (0 + 1) / 1][Stage 314:>                                                        (0 + 1) / 1][Stage 314:>                                                        (0 + 1) / 1][Stage 314:>                                                        (0 + 1) / 1][Stage 314:>                                                        (0 + 1) / 1][Stage 314:>                                                        (0 + 1) / 1][Stage 314:>                                                        (0 + 1) / 1]                                                                                [Stage 315:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 315:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:10:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 315:>                                                        (0 + 1) / 1][Stage 315:>                                                        (0 + 1) / 1][Stage 315:>                                                        (0 + 1) / 1][Stage 315:>                                                        (0 + 1) / 1][Stage 315:>                                                        (0 + 1) / 1][Stage 315:>                                                        (0 + 1) / 1][Stage 315:>                                                        (0 + 1) / 1]                                                                                [Stage 316:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:21:34] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1][Stage 316:>                                                        (0 + 1) / 1]                                                                                [Stage 317:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:40:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1][Stage 317:>                                                        (0 + 1) / 1]                                                                                [Stage 318:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 318:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:00:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 318:>                                                        (0 + 1) / 1][Stage 318:>                                                        (0 + 1) / 1][Stage 318:>                                                        (0 + 1) / 1][Stage 318:>                                                        (0 + 1) / 1][Stage 318:>                                                        (0 + 1) / 1]                                                                                [Stage 319:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 319:>                                                        (0 + 1) / 1][Stage 319:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:08:33] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 319:>                                                        (0 + 1) / 1][Stage 319:>                                                        (0 + 1) / 1][Stage 319:>                                                        (0 + 1) / 1][Stage 319:>                                                        (0 + 1) / 1][Stage 319:>                                                        (0 + 1) / 1][Stage 319:>                                                        (0 + 1) / 1][Stage 319:>                                                        (0 + 1) / 1][Stage 319:>                                                        (0 + 1) / 1][Stage 319:>                                                        (0 + 1) / 1][Stage 319:>                                                        (0 + 1) / 1][Stage 319:>                                                        (0 + 1) / 1]                                                                                [Stage 320:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:20:32] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 320:>                                                        (0 + 1) / 1][Stage 320:>                                                        (0 + 1) / 1][Stage 320:>                                                        (0 + 1) / 1][Stage 320:>                                                        (0 + 1) / 1]                                                                                [Stage 321:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 321:>                                                        (0 + 1) / 1][Stage 321:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:26:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 321:>                                                        (0 + 1) / 1][Stage 321:>                                                        (0 + 1) / 1][Stage 321:>                                                        (0 + 1) / 1][Stage 321:>                                                        (0 + 1) / 1][Stage 321:>                                                        (0 + 1) / 1][Stage 321:>                                                        (0 + 1) / 1][Stage 321:>                                                        (0 + 1) / 1][Stage 321:>                                                        (0 + 1) / 1][Stage 321:>                                                        (0 + 1) / 1][Stage 321:>                                                        (0 + 1) / 1][Stage 321:>                                                        (0 + 1) / 1]                                                                                [Stage 322:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:40:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1][Stage 322:>                                                        (0 + 1) / 1]                                                                                [Stage 323:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:58:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1][Stage 323:>                                                        (0 + 1) / 1]                                                                                [Stage 324:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 324:>                                                        (0 + 1) / 1][Stage 324:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:17:44] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 324:>                                                        (0 + 1) / 1][Stage 324:>                                                        (0 + 1) / 1][Stage 324:>                                                        (0 + 1) / 1][Stage 324:>                                                        (0 + 1) / 1][Stage 324:>                                                        (0 + 1) / 1][Stage 324:>                                                        (0 + 1) / 1][Stage 324:>                                                        (0 + 1) / 1][Stage 324:>                                                        (0 + 1) / 1]                                                                                [Stage 325:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 325:>                                                        (0 + 1) / 1][Stage 325:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:29:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 325:>                                                        (0 + 1) / 1][Stage 325:>                                                        (0 + 1) / 1][Stage 325:>                                                        (0 + 1) / 1][Stage 325:>                                                        (0 + 1) / 1][Stage 325:>                                                        (0 + 1) / 1][Stage 325:>                                                        (0 + 1) / 1][Stage 325:>                                                        (0 + 1) / 1][Stage 325:>                                                        (0 + 1) / 1][Stage 325:>                                                        (0 + 1) / 1][Stage 325:>                                                        (0 + 1) / 1][Stage 325:>                                                        (0 + 1) / 1][Stage 325:>                                                        (0 + 1) / 1]                                                                                [Stage 326:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 326:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:41:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 326:>                                                        (0 + 1) / 1][Stage 326:>                                                        (0 + 1) / 1][Stage 326:>                                                        (0 + 1) / 1][Stage 326:>                                                        (0 + 1) / 1]                                                                                [Stage 327:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:47:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 327:>                                                        (0 + 1) / 1]                                                                                [Stage 328:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 328:>                                                        (0 + 1) / 1][Stage 328:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:51:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 328:>                                                        (0 + 1) / 1][Stage 328:>                                                        (0 + 1) / 1][Stage 328:>                                                        (0 + 1) / 1][Stage 328:>                                                        (0 + 1) / 1][Stage 328:>                                                        (0 + 1) / 1][Stage 328:>                                                        (0 + 1) / 1][Stage 328:>                                                        (0 + 1) / 1][Stage 328:>                                                        (0 + 1) / 1][Stage 328:>                                                        (0 + 1) / 1][Stage 328:>                                                        (0 + 1) / 1]                                                                                [Stage 329:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 329:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:02:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 329:>                                                        (0 + 1) / 1][Stage 329:>                                                        (0 + 1) / 1][Stage 329:>                                                        (0 + 1) / 1][Stage 329:>                                                        (0 + 1) / 1][Stage 329:>                                                        (0 + 1) / 1]                                                                                [Stage 330:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:11:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1][Stage 330:>                                                        (0 + 1) / 1]                                                                                [Stage 331:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:28:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 331:>                                                        (0 + 1) / 1][Stage 331:>                                                        (0 + 1) / 1]                                                                                [Stage 332:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:34:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1][Stage 332:>                                                        (0 + 1) / 1]                                                                                [Stage 333:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 333:>                                                        (0 + 1) / 1][Stage 333:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:54:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 333:>                                                        (0 + 1) / 1][Stage 333:>                                                        (0 + 1) / 1][Stage 333:>                                                        (0 + 1) / 1][Stage 333:>                                                        (0 + 1) / 1][Stage 333:>                                                        (0 + 1) / 1][Stage 333:>                                                        (0 + 1) / 1][Stage 333:>                                                        (0 + 1) / 1][Stage 333:>                                                        (0 + 1) / 1][Stage 333:>                                                        (0 + 1) / 1][Stage 333:>                                                        (0 + 1) / 1][Stage 333:>                                                        (0 + 1) / 1]                                                                                [Stage 334:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 334:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:06:23] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 334:>                                                        (0 + 1) / 1][Stage 334:>                                                        (0 + 1) / 1][Stage 334:>                                                        (0 + 1) / 1][Stage 334:>                                                        (0 + 1) / 1][Stage 334:>                                                        (0 + 1) / 1]                                                                                [Stage 335:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 335:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:13:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 335:>                                                        (0 + 1) / 1][Stage 335:>                                                        (0 + 1) / 1][Stage 335:>                                                        (0 + 1) / 1][Stage 335:>                                                        (0 + 1) / 1]                                                                                [Stage 336:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 336:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:19:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 336:>                                                        (0 + 1) / 1][Stage 336:>                                                        (0 + 1) / 1][Stage 336:>                                                        (0 + 1) / 1][Stage 336:>                                                        (0 + 1) / 1][Stage 336:>                                                        (0 + 1) / 1][Stage 336:>                                                        (0 + 1) / 1][Stage 336:>                                                        (0 + 1) / 1][Stage 336:>                                                        (0 + 1) / 1]                                                                                [Stage 337:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:28:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 337:>                                                        (0 + 1) / 1][Stage 337:>                                                        (0 + 1) / 1]                                                                                [Stage 338:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:35:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1][Stage 338:>                                                        (0 + 1) / 1]                                                                                [Stage 339:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:02:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1][Stage 339:>                                                        (0 + 1) / 1]                                                                                [Stage 340:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:20:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1][Stage 340:>                                                        (0 + 1) / 1]                                                                                [Stage 341:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 341:>                                                        (0 + 1) / 1][Stage 341:>                                                        (0 + 1) / 1][Stage 341:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:38:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 341:>                                                        (0 + 1) / 1][Stage 341:>                                                        (0 + 1) / 1][Stage 341:>                                                        (0 + 1) / 1][Stage 341:>                                                        (0 + 1) / 1][Stage 341:>                                                        (0 + 1) / 1][Stage 341:>                                                        (0 + 1) / 1][Stage 341:>                                                        (0 + 1) / 1][Stage 341:>                                                        (0 + 1) / 1][Stage 341:>                                                        (0 + 1) / 1][Stage 341:>                                                        (0 + 1) / 1][Stage 341:>                                                        (0 + 1) / 1][Stage 341:>                                                        (0 + 1) / 1]                                                                                [Stage 342:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:54:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1][Stage 342:>                                                        (0 + 1) / 1]                                                                                [Stage 343:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 343:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:10:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 343:>                                                        (0 + 1) / 1][Stage 343:>                                                        (0 + 1) / 1][Stage 343:>                                                        (0 + 1) / 1][Stage 343:>                                                        (0 + 1) / 1][Stage 343:>                                                        (0 + 1) / 1][Stage 343:>                                                        (0 + 1) / 1][Stage 343:>                                                        (0 + 1) / 1]                                                                                [Stage 344:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:21:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1][Stage 344:>                                                        (0 + 1) / 1]                                                                                [Stage 345:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 345:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:40:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 345:>                                                        (0 + 1) / 1][Stage 345:>                                                        (0 + 1) / 1][Stage 345:>                                                        (0 + 1) / 1][Stage 345:>                                                        (0 + 1) / 1][Stage 345:>                                                        (0 + 1) / 1][Stage 345:>                                                        (0 + 1) / 1]                                                                                [Stage 346:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 346:>                                                        (0 + 1) / 1][Stage 346:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:48:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 346:>                                                        (0 + 1) / 1][Stage 346:>                                                        (0 + 1) / 1][Stage 346:>                                                        (0 + 1) / 1][Stage 346:>                                                        (0 + 1) / 1][Stage 346:>                                                        (0 + 1) / 1][Stage 346:>                                                        (0 + 1) / 1][Stage 346:>                                                        (0 + 1) / 1][Stage 346:>                                                        (0 + 1) / 1][Stage 346:>                                                        (0 + 1) / 1]                                                                                [Stage 347:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 347:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:59:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 347:>                                                        (0 + 1) / 1][Stage 347:>                                                        (0 + 1) / 1][Stage 347:>                                                        (0 + 1) / 1][Stage 347:>                                                        (0 + 1) / 1][Stage 347:>                                                        (0 + 1) / 1][Stage 347:>                                                        (0 + 1) / 1]                                                                                [Stage 348:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 348:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:06:29] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 348:>                                                        (0 + 1) / 1][Stage 348:>                                                        (0 + 1) / 1][Stage 348:>                                                        (0 + 1) / 1][Stage 348:>                                                        (0 + 1) / 1][Stage 348:>                                                        (0 + 1) / 1]                                                                                [Stage 349:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 349:>                                                        (0 + 1) / 1][Stage 349:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:14:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 349:>                                                        (0 + 1) / 1][Stage 349:>                                                        (0 + 1) / 1][Stage 349:>                                                        (0 + 1) / 1][Stage 349:>                                                        (0 + 1) / 1][Stage 349:>                                                        (0 + 1) / 1][Stage 349:>                                                        (0 + 1) / 1][Stage 349:>                                                        (0 + 1) / 1]                                                                                [Stage 350:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 350:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:23:37] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 350:>                                                        (0 + 1) / 1][Stage 350:>                                                        (0 + 1) / 1][Stage 350:>                                                        (0 + 1) / 1][Stage 350:>                                                        (0 + 1) / 1][Stage 350:>                                                        (0 + 1) / 1][Stage 350:>                                                        (0 + 1) / 1][Stage 350:>                                                        (0 + 1) / 1][Stage 350:>                                                        (0 + 1) / 1]                                                                                [Stage 351:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 351:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:32:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 351:>                                                        (0 + 1) / 1][Stage 351:>                                                        (0 + 1) / 1][Stage 351:>                                                        (0 + 1) / 1][Stage 351:>                                                        (0 + 1) / 1][Stage 351:>                                                        (0 + 1) / 1][Stage 351:>                                                        (0 + 1) / 1][Stage 351:>                                                        (0 + 1) / 1][Stage 351:>                                                        (0 + 1) / 1]                                                                                [Stage 352:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 352:>                                                        (0 + 1) / 1][Stage 352:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:43:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 352:>                                                        (0 + 1) / 1][Stage 352:>                                                        (0 + 1) / 1][Stage 352:>                                                        (0 + 1) / 1][Stage 352:>                                                        (0 + 1) / 1][Stage 352:>                                                        (0 + 1) / 1][Stage 352:>                                                        (0 + 1) / 1][Stage 352:>                                                        (0 + 1) / 1][Stage 352:>                                                        (0 + 1) / 1][Stage 352:>                                                        (0 + 1) / 1][Stage 352:>                                                        (0 + 1) / 1]                                                                                [Stage 353:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 353:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:53:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 353:>                                                        (0 + 1) / 1][Stage 353:>                                                        (0 + 1) / 1][Stage 353:>                                                        (0 + 1) / 1][Stage 353:>                                                        (0 + 1) / 1]                                                                                [Stage 354:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:59:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 354:>                                                        (0 + 1) / 1][Stage 354:>                                                        (0 + 1) / 1][Stage 354:>                                                        (0 + 1) / 1][Stage 354:>                                                        (0 + 1) / 1]                                                                                [Stage 355:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 355:>                                                        (0 + 1) / 1][Stage 355:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:05:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 355:>                                                        (0 + 1) / 1][Stage 355:>                                                        (0 + 1) / 1][Stage 355:>                                                        (0 + 1) / 1][Stage 355:>                                                        (0 + 1) / 1][Stage 355:>                                                        (0 + 1) / 1][Stage 355:>                                                        (0 + 1) / 1][Stage 355:>                                                        (0 + 1) / 1][Stage 355:>                                                        (0 + 1) / 1][Stage 355:>                                                        (0 + 1) / 1][Stage 355:>                                                        (0 + 1) / 1][Stage 355:>                                                        (0 + 1) / 1][Stage 355:>                                                        (0 + 1) / 1]                                                                                [Stage 356:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 356:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:19:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 356:>                                                        (0 + 1) / 1][Stage 356:>                                                        (0 + 1) / 1][Stage 356:>                                                        (0 + 1) / 1][Stage 356:>                                                        (0 + 1) / 1][Stage 356:>                                                        (0 + 1) / 1][Stage 356:>                                                        (0 + 1) / 1]                                                                                [Stage 357:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 357:>                                                        (0 + 1) / 1][Stage 357:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:27:34] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 357:>                                                        (0 + 1) / 1][Stage 357:>                                                        (0 + 1) / 1][Stage 357:>                                                        (0 + 1) / 1][Stage 357:>                                                        (0 + 1) / 1][Stage 357:>                                                        (0 + 1) / 1][Stage 357:>                                                        (0 + 1) / 1][Stage 357:>                                                        (0 + 1) / 1][Stage 357:>                                                        (0 + 1) / 1][Stage 357:>                                                        (0 + 1) / 1]                                                                                [Stage 358:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 358:>                                                        (0 + 1) / 1][Stage 358:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:39:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 358:>                                                        (0 + 1) / 1][Stage 358:>                                                        (0 + 1) / 1][Stage 358:>                                                        (0 + 1) / 1][Stage 358:>                                                        (0 + 1) / 1][Stage 358:>                                                        (0 + 1) / 1][Stage 358:>                                                        (0 + 1) / 1][Stage 358:>                                                        (0 + 1) / 1][Stage 358:>                                                        (0 + 1) / 1][Stage 358:>                                                        (0 + 1) / 1]                                                                                [Stage 359:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 359:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:50:34] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 359:>                                                        (0 + 1) / 1][Stage 359:>                                                        (0 + 1) / 1][Stage 359:>                                                        (0 + 1) / 1][Stage 359:>                                                        (0 + 1) / 1][Stage 359:>                                                        (0 + 1) / 1][Stage 359:>                                                        (0 + 1) / 1][Stage 359:>                                                        (0 + 1) / 1][Stage 359:>                                                        (0 + 1) / 1]                                                                                [Stage 360:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 360:>                                                        (0 + 1) / 1][Stage 360:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:00:47] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 360:>                                                        (0 + 1) / 1][Stage 360:>                                                        (0 + 1) / 1][Stage 360:>                                                        (0 + 1) / 1][Stage 360:>                                                        (0 + 1) / 1][Stage 360:>                                                        (0 + 1) / 1][Stage 360:>                                                        (0 + 1) / 1][Stage 360:>                                                        (0 + 1) / 1][Stage 360:>                                                        (0 + 1) / 1][Stage 360:>                                                        (0 + 1) / 1]                                                                                [Stage 361:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 361:>                                                        (0 + 1) / 1][Stage 361:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:12:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 361:>                                                        (0 + 1) / 1][Stage 361:>                                                        (0 + 1) / 1][Stage 361:>                                                        (0 + 1) / 1][Stage 361:>                                                        (0 + 1) / 1][Stage 361:>                                                        (0 + 1) / 1][Stage 361:>                                                        (0 + 1) / 1][Stage 361:>                                                        (0 + 1) / 1][Stage 361:>                                                        (0 + 1) / 1][Stage 361:>                                                        (0 + 1) / 1][Stage 361:>                                                        (0 + 1) / 1]                                                                                [Stage 362:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 362:>                                                        (0 + 1) / 1][Stage 362:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:25:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 362:>                                                        (0 + 1) / 1][Stage 362:>                                                        (0 + 1) / 1][Stage 362:>                                                        (0 + 1) / 1][Stage 362:>                                                        (0 + 1) / 1][Stage 362:>                                                        (0 + 1) / 1][Stage 362:>                                                        (0 + 1) / 1][Stage 362:>                                                        (0 + 1) / 1][Stage 362:>                                                        (0 + 1) / 1][Stage 362:>                                                        (0 + 1) / 1][Stage 362:>                                                        (0 + 1) / 1][Stage 362:>                                                        (0 + 1) / 1]                                                                                [Stage 363:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 363:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:38:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 363:>                                                        (0 + 1) / 1][Stage 363:>                                                        (0 + 1) / 1][Stage 363:>                                                        (0 + 1) / 1][Stage 363:>                                                        (0 + 1) / 1][Stage 363:>                                                        (0 + 1) / 1]                                                                                [Stage 364:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 364:>                                                        (0 + 1) / 1][Stage 364:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:45:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 364:>                                                        (0 + 1) / 1][Stage 364:>                                                        (0 + 1) / 1][Stage 364:>                                                        (0 + 1) / 1][Stage 364:>                                                        (0 + 1) / 1][Stage 364:>                                                        (0 + 1) / 1][Stage 364:>                                                        (0 + 1) / 1][Stage 364:>                                                        (0 + 1) / 1][Stage 364:>                                                        (0 + 1) / 1]                                                                                [Stage 365:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 365:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:56:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 365:>                                                        (0 + 1) / 1][Stage 365:>                                                        (0 + 1) / 1][Stage 365:>                                                        (0 + 1) / 1][Stage 365:>                                                        (0 + 1) / 1][Stage 365:>                                                        (0 + 1) / 1][Stage 365:>                                                        (0 + 1) / 1][Stage 365:>                                                        (0 + 1) / 1][Stage 365:>                                                        (0 + 1) / 1]                                                                                [Stage 366:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:04:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 366:>                                                        (0 + 1) / 1][Stage 366:>                                                        (0 + 1) / 1][Stage 366:>                                                        (0 + 1) / 1][Stage 366:>                                                        (0 + 1) / 1]                                                                                [Stage 367:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 367:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:10:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 367:>                                                        (0 + 1) / 1][Stage 367:>                                                        (0 + 1) / 1][Stage 367:>                                                        (0 + 1) / 1][Stage 367:>                                                        (0 + 1) / 1][Stage 367:>                                                        (0 + 1) / 1][Stage 367:>                                                        (0 + 1) / 1][Stage 367:>                                                        (0 + 1) / 1]                                                                                [Stage 368:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 368:>                                                        (0 + 1) / 1][Stage 368:>                                                        (0 + 1) / 1][Stage 368:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:19:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 368:>                                                        (0 + 1) / 1][Stage 368:>                                                        (0 + 1) / 1][Stage 368:>                                                        (0 + 1) / 1][Stage 368:>                                                        (0 + 1) / 1][Stage 368:>                                                        (0 + 1) / 1][Stage 368:>                                                        (0 + 1) / 1][Stage 368:>                                                        (0 + 1) / 1][Stage 368:>                                                        (0 + 1) / 1][Stage 368:>                                                        (0 + 1) / 1][Stage 368:>                                                        (0 + 1) / 1][Stage 368:>                                                        (0 + 1) / 1][Stage 368:>                                                        (0 + 1) / 1]                                                                                [Stage 369:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:33:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 369:>                                                        (0 + 1) / 1][Stage 369:>                                                        (0 + 1) / 1][Stage 369:>                                                        (0 + 1) / 1]                                                                                [Stage 370:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 370:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:37:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 370:>                                                        (0 + 1) / 1][Stage 370:>                                                        (0 + 1) / 1][Stage 370:>                                                        (0 + 1) / 1][Stage 370:>                                                        (0 + 1) / 1]                                                                                [Stage 371:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 371:>                                                        (0 + 1) / 1][Stage 371:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:44:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 371:>                                                        (0 + 1) / 1][Stage 371:>                                                        (0 + 1) / 1][Stage 371:>                                                        (0 + 1) / 1][Stage 371:>                                                        (0 + 1) / 1][Stage 371:>                                                        (0 + 1) / 1][Stage 371:>                                                        (0 + 1) / 1][Stage 371:>                                                        (0 + 1) / 1][Stage 371:>                                                        (0 + 1) / 1][Stage 371:>                                                        (0 + 1) / 1][Stage 371:>                                                        (0 + 1) / 1]                                                                                [Stage 372:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 372:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:55:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 372:>                                                        (0 + 1) / 1][Stage 372:>                                                        (0 + 1) / 1][Stage 372:>                                                        (0 + 1) / 1][Stage 372:>                                                        (0 + 1) / 1][Stage 372:>                                                        (0 + 1) / 1]                                                                                [Stage 373:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 373:>                                                        (0 + 1) / 1][Stage 373:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:03:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 373:>                                                        (0 + 1) / 1][Stage 373:>                                                        (0 + 1) / 1][Stage 373:>                                                        (0 + 1) / 1][Stage 373:>                                                        (0 + 1) / 1][Stage 373:>                                                        (0 + 1) / 1][Stage 373:>                                                        (0 + 1) / 1][Stage 373:>                                                        (0 + 1) / 1][Stage 373:>                                                        (0 + 1) / 1][Stage 373:>                                                        (0 + 1) / 1][Stage 373:>                                                        (0 + 1) / 1][Stage 373:>                                                        (0 + 1) / 1]                                                                                [Stage 374:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:14:23] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 374:>                                                        (0 + 1) / 1][Stage 374:>                                                        (0 + 1) / 1][Stage 374:>                                                        (0 + 1) / 1]                                                                                [Stage 375:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 375:>                                                        (0 + 1) / 1][Stage 375:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:19:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 375:>                                                        (0 + 1) / 1][Stage 375:>                                                        (0 + 1) / 1][Stage 375:>                                                        (0 + 1) / 1][Stage 375:>                                                        (0 + 1) / 1][Stage 375:>                                                        (0 + 1) / 1][Stage 375:>                                                        (0 + 1) / 1][Stage 375:>                                                        (0 + 1) / 1]                                                                                [Stage 376:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 376:>                                                        (0 + 1) / 1][Stage 376:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:29:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 376:>                                                        (0 + 1) / 1][Stage 376:>                                                        (0 + 1) / 1][Stage 376:>                                                        (0 + 1) / 1][Stage 376:>                                                        (0 + 1) / 1][Stage 376:>                                                        (0 + 1) / 1][Stage 376:>                                                        (0 + 1) / 1][Stage 376:>                                                        (0 + 1) / 1]                                                                                [Stage 377:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:39:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1][Stage 377:>                                                        (0 + 1) / 1]                                                                                [Stage 378:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:53:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 378:>                                                        (0 + 1) / 1][Stage 378:>                                                        (0 + 1) / 1][Stage 378:>                                                        (0 + 1) / 1]                                                                                [Stage 379:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:01:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1][Stage 379:>                                                        (0 + 1) / 1]                                                                                [Stage 380:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:24:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1][Stage 380:>                                                        (0 + 1) / 1]                                                                                [Stage 381:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 381:>                                                        (0 + 1) / 1][Stage 381:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:44:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 381:>                                                        (0 + 1) / 1][Stage 381:>                                                        (0 + 1) / 1][Stage 381:>                                                        (0 + 1) / 1][Stage 381:>                                                        (0 + 1) / 1][Stage 381:>                                                        (0 + 1) / 1][Stage 381:>                                                        (0 + 1) / 1][Stage 381:>                                                        (0 + 1) / 1][Stage 381:>                                                        (0 + 1) / 1][Stage 381:>                                                        (0 + 1) / 1]                                                                                [Stage 382:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:57:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1][Stage 382:>                                                        (0 + 1) / 1]                                                                                [Stage 383:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 383:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:14:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 383:>                                                        (0 + 1) / 1][Stage 383:>                                                        (0 + 1) / 1][Stage 383:>                                                        (0 + 1) / 1][Stage 383:>                                                        (0 + 1) / 1][Stage 383:>                                                        (0 + 1) / 1][Stage 383:>                                                        (0 + 1) / 1][Stage 383:>                                                        (0 + 1) / 1]                                                                                [Stage 384:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 384:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:22:11] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 384:>                                                        (0 + 1) / 1][Stage 384:>                                                        (0 + 1) / 1][Stage 384:>                                                        (0 + 1) / 1][Stage 384:>                                                        (0 + 1) / 1]                                                                                [Stage 385:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 385:>                                                        (0 + 1) / 1][Stage 385:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:29:36] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 385:>                                                        (0 + 1) / 1][Stage 385:>                                                        (0 + 1) / 1][Stage 385:>                                                        (0 + 1) / 1][Stage 385:>                                                        (0 + 1) / 1][Stage 385:>                                                        (0 + 1) / 1][Stage 385:>                                                        (0 + 1) / 1][Stage 385:>                                                        (0 + 1) / 1][Stage 385:>                                                        (0 + 1) / 1][Stage 385:>                                                        (0 + 1) / 1][Stage 385:>                                                        (0 + 1) / 1]                                                                                [Stage 386:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 386:>                                                        (0 + 1) / 1][Stage 386:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:41:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 386:>                                                        (0 + 1) / 1][Stage 386:>                                                        (0 + 1) / 1][Stage 386:>                                                        (0 + 1) / 1][Stage 386:>                                                        (0 + 1) / 1][Stage 386:>                                                        (0 + 1) / 1][Stage 386:>                                                        (0 + 1) / 1][Stage 386:>                                                        (0 + 1) / 1]                                                                                [Stage 387:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:50:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 387:>                                                        (0 + 1) / 1][Stage 387:>                                                        (0 + 1) / 1][Stage 387:>                                                        (0 + 1) / 1]                                                                                [Stage 388:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 388:>                                                        (0 + 1) / 1][Stage 388:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:55:58] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 388:>                                                        (0 + 1) / 1][Stage 388:>                                                        (0 + 1) / 1][Stage 388:>                                                        (0 + 1) / 1][Stage 388:>                                                        (0 + 1) / 1][Stage 388:>                                                        (0 + 1) / 1][Stage 388:>                                                        (0 + 1) / 1][Stage 388:>                                                        (0 + 1) / 1][Stage 388:>                                                        (0 + 1) / 1][Stage 388:>                                                        (0 + 1) / 1][Stage 388:>                                                        (0 + 1) / 1][Stage 388:>                                                        (0 + 1) / 1]                                                                                [Stage 389:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 389:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:08:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 389:>                                                        (0 + 1) / 1][Stage 389:>                                                        (0 + 1) / 1][Stage 389:>                                                        (0 + 1) / 1][Stage 389:>                                                        (0 + 1) / 1][Stage 389:>                                                        (0 + 1) / 1][Stage 389:>                                                        (0 + 1) / 1]                                                                                [Stage 390:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 390:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:15:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 390:>                                                        (0 + 1) / 1][Stage 390:>                                                        (0 + 1) / 1][Stage 390:>                                                        (0 + 1) / 1][Stage 390:>                                                        (0 + 1) / 1][Stage 390:>                                                        (0 + 1) / 1]                                                                                [Stage 391:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 391:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:22:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 391:>                                                        (0 + 1) / 1][Stage 391:>                                                        (0 + 1) / 1][Stage 391:>                                                        (0 + 1) / 1][Stage 391:>                                                        (0 + 1) / 1][Stage 391:>                                                        (0 + 1) / 1]                                                                                [Stage 392:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 392:>                                                        (0 + 1) / 1][Stage 392:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:30:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 392:>                                                        (0 + 1) / 1][Stage 392:>                                                        (0 + 1) / 1][Stage 392:>                                                        (0 + 1) / 1][Stage 392:>                                                        (0 + 1) / 1][Stage 392:>                                                        (0 + 1) / 1][Stage 392:>                                                        (0 + 1) / 1][Stage 392:>                                                        (0 + 1) / 1][Stage 392:>                                                        (0 + 1) / 1][Stage 392:>                                                        (0 + 1) / 1][Stage 392:>                                                        (0 + 1) / 1]                                                                                [Stage 393:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 393:>                                                        (0 + 1) / 1][Stage 393:>                                                        (0 + 1) / 1][Stage 393:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:43:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 393:>                                                        (0 + 1) / 1][Stage 393:>                                                        (0 + 1) / 1][Stage 393:>                                                        (0 + 1) / 1][Stage 393:>                                                        (0 + 1) / 1][Stage 393:>                                                        (0 + 1) / 1][Stage 393:>                                                        (0 + 1) / 1][Stage 393:>                                                        (0 + 1) / 1][Stage 393:>                                                        (0 + 1) / 1][Stage 393:>                                                        (0 + 1) / 1][Stage 393:>                                                        (0 + 1) / 1][Stage 393:>                                                        (0 + 1) / 1]                                                                                [Stage 394:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:58:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1][Stage 394:>                                                        (0 + 1) / 1]                                                                                [Stage 395:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 395:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:13:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 395:>                                                        (0 + 1) / 1][Stage 395:>                                                        (0 + 1) / 1][Stage 395:>                                                        (0 + 1) / 1][Stage 395:>                                                        (0 + 1) / 1][Stage 395:>                                                        (0 + 1) / 1][Stage 395:>                                                        (0 + 1) / 1][Stage 395:>                                                        (0 + 1) / 1]                                                                                [Stage 396:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 396:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:21:47] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 396:>                                                        (0 + 1) / 1][Stage 396:>                                                        (0 + 1) / 1][Stage 396:>                                                        (0 + 1) / 1][Stage 396:>                                                        (0 + 1) / 1][Stage 396:>                                                        (0 + 1) / 1]                                                                                [Stage 397:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 397:>                                                        (0 + 1) / 1][Stage 397:>                                                        (0 + 1) / 1][Stage 397:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:29:52] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 397:>                                                        (0 + 1) / 1][Stage 397:>                                                        (0 + 1) / 1][Stage 397:>                                                        (0 + 1) / 1][Stage 397:>                                                        (0 + 1) / 1][Stage 397:>                                                        (0 + 1) / 1][Stage 397:>                                                        (0 + 1) / 1][Stage 397:>                                                        (0 + 1) / 1][Stage 397:>                                                        (0 + 1) / 1][Stage 397:>                                                        (0 + 1) / 1]                                                                                [Stage 398:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 398:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:41:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 398:>                                                        (0 + 1) / 1][Stage 398:>                                                        (0 + 1) / 1][Stage 398:>                                                        (0 + 1) / 1][Stage 398:>                                                        (0 + 1) / 1][Stage 398:>                                                        (0 + 1) / 1]                                                                                [Stage 399:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:50:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1][Stage 399:>                                                        (0 + 1) / 1]                                                                                [Stage 400:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 400:>                                                        (0 + 1) / 1][Stage 400:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:05:31] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 400:>                                                        (0 + 1) / 1][Stage 400:>                                                        (0 + 1) / 1][Stage 400:>                                                        (0 + 1) / 1][Stage 400:>                                                        (0 + 1) / 1][Stage 400:>                                                        (0 + 1) / 1][Stage 400:>                                                        (0 + 1) / 1][Stage 400:>                                                        (0 + 1) / 1][Stage 400:>                                                        (0 + 1) / 1][Stage 400:>                                                        (0 + 1) / 1][Stage 400:>                                                        (0 + 1) / 1]                                                                                [Stage 401:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:20:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1][Stage 401:>                                                        (0 + 1) / 1]                                                                                [Stage 402:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:43:43] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1][Stage 402:>                                                        (0 + 1) / 1]                                                                                [Stage 403:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 403:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:05:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 403:>                                                        (0 + 1) / 1][Stage 403:>                                                        (0 + 1) / 1][Stage 403:>                                                        (0 + 1) / 1][Stage 403:>                                                        (0 + 1) / 1][Stage 403:>                                                        (0 + 1) / 1]                                                                                [Stage 404:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:13:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1][Stage 404:>                                                        (0 + 1) / 1]                                                                                [Stage 405:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 405:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:31:23] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 405:>                                                        (0 + 1) / 1][Stage 405:>                                                        (0 + 1) / 1][Stage 405:>                                                        (0 + 1) / 1][Stage 405:>                                                        (0 + 1) / 1][Stage 405:>                                                        (0 + 1) / 1][Stage 405:>                                                        (0 + 1) / 1][Stage 405:>                                                        (0 + 1) / 1]                                                                                [Stage 406:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 406:>                                                        (0 + 1) / 1][Stage 406:>                                                        (0 + 1) / 1][Stage 406:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:41:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 406:>                                                        (0 + 1) / 1][Stage 406:>                                                        (0 + 1) / 1][Stage 406:>                                                        (0 + 1) / 1][Stage 406:>                                                        (0 + 1) / 1][Stage 406:>                                                        (0 + 1) / 1][Stage 406:>                                                        (0 + 1) / 1][Stage 406:>                                                        (0 + 1) / 1][Stage 406:>                                                        (0 + 1) / 1][Stage 406:>                                                        (0 + 1) / 1][Stage 406:>                                                        (0 + 1) / 1][Stage 406:>                                                        (0 + 1) / 1][Stage 406:>                                                        (0 + 1) / 1]                                                                                [Stage 407:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 407:>                                                        (0 + 1) / 1][Stage 407:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:56:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 407:>                                                        (0 + 1) / 1][Stage 407:>                                                        (0 + 1) / 1][Stage 407:>                                                        (0 + 1) / 1][Stage 407:>                                                        (0 + 1) / 1][Stage 407:>                                                        (0 + 1) / 1][Stage 407:>                                                        (0 + 1) / 1][Stage 407:>                                                        (0 + 1) / 1][Stage 407:>                                                        (0 + 1) / 1][Stage 407:>                                                        (0 + 1) / 1][Stage 407:>                                                        (0 + 1) / 1]                                                                                [Stage 408:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:07:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 408:>                                                        (0 + 1) / 1][Stage 408:>                                                        (0 + 1) / 1][Stage 408:>                                                        (0 + 1) / 1]                                                                                [Stage 409:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 409:>                                                        (0 + 1) / 1][Stage 409:>                                                        (0 + 1) / 1][Stage 409:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:13:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 409:>                                                        (0 + 1) / 1][Stage 409:>                                                        (0 + 1) / 1][Stage 409:>                                                        (0 + 1) / 1][Stage 409:>                                                        (0 + 1) / 1][Stage 409:>                                                        (0 + 1) / 1][Stage 409:>                                                        (0 + 1) / 1][Stage 409:>                                                        (0 + 1) / 1][Stage 409:>                                                        (0 + 1) / 1][Stage 409:>                                                        (0 + 1) / 1][Stage 409:>                                                        (0 + 1) / 1][Stage 409:>                                                        (0 + 1) / 1]                                                                                [Stage 410:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 410:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:26:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 410:>                                                        (0 + 1) / 1][Stage 410:>                                                        (0 + 1) / 1][Stage 410:>                                                        (0 + 1) / 1][Stage 410:>                                                        (0 + 1) / 1][Stage 410:>                                                        (0 + 1) / 1][Stage 410:>                                                        (0 + 1) / 1][Stage 410:>                                                        (0 + 1) / 1][Stage 410:>                                                        (0 + 1) / 1]                                                                                [Stage 411:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 411:>                                                        (0 + 1) / 1][Stage 411:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:36:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 411:>                                                        (0 + 1) / 1][Stage 411:>                                                        (0 + 1) / 1][Stage 411:>                                                        (0 + 1) / 1][Stage 411:>                                                        (0 + 1) / 1][Stage 411:>                                                        (0 + 1) / 1][Stage 411:>                                                        (0 + 1) / 1][Stage 411:>                                                        (0 + 1) / 1][Stage 411:>                                                        (0 + 1) / 1]                                                                                [Stage 412:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 412:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:46:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 412:>                                                        (0 + 1) / 1][Stage 412:>                                                        (0 + 1) / 1][Stage 412:>                                                        (0 + 1) / 1][Stage 412:>                                                        (0 + 1) / 1][Stage 412:>                                                        (0 + 1) / 1]                                                                                [Stage 413:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:55:23] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1][Stage 413:>                                                        (0 + 1) / 1]                                                                                [Stage 414:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:12:34] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1][Stage 414:>                                                        (0 + 1) / 1]                                                                                [Stage 415:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 415:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:31:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 415:>                                                        (0 + 1) / 1][Stage 415:>                                                        (0 + 1) / 1][Stage 415:>                                                        (0 + 1) / 1][Stage 415:>                                                        (0 + 1) / 1][Stage 415:>                                                        (0 + 1) / 1][Stage 415:>                                                        (0 + 1) / 1][Stage 415:>                                                        (0 + 1) / 1][Stage 415:>                                                        (0 + 1) / 1]                                                                                [Stage 416:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:45:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1][Stage 416:>                                                        (0 + 1) / 1]                                                                                [Stage 417:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 417:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:07:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 417:>                                                        (0 + 1) / 1][Stage 417:>                                                        (0 + 1) / 1][Stage 417:>                                                        (0 + 1) / 1][Stage 417:>                                                        (0 + 1) / 1][Stage 417:>                                                        (0 + 1) / 1][Stage 417:>                                                        (0 + 1) / 1][Stage 417:>                                                        (0 + 1) / 1][Stage 417:>                                                        (0 + 1) / 1]                                                                                [Stage 418:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 418:>                                                        (0 + 1) / 1][Stage 418:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:17:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 418:>                                                        (0 + 1) / 1][Stage 418:>                                                        (0 + 1) / 1][Stage 418:>                                                        (0 + 1) / 1][Stage 418:>                                                        (0 + 1) / 1][Stage 418:>                                                        (0 + 1) / 1][Stage 418:>                                                        (0 + 1) / 1][Stage 418:>                                                        (0 + 1) / 1][Stage 418:>                                                        (0 + 1) / 1][Stage 418:>                                                        (0 + 1) / 1][Stage 418:>                                                        (0 + 1) / 1][Stage 418:>                                                        (0 + 1) / 1]                                                                                [Stage 419:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 419:>                                                        (0 + 1) / 1][Stage 419:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:30:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 419:>                                                        (0 + 1) / 1][Stage 419:>                                                        (0 + 1) / 1][Stage 419:>                                                        (0 + 1) / 1][Stage 419:>                                                        (0 + 1) / 1][Stage 419:>                                                        (0 + 1) / 1][Stage 419:>                                                        (0 + 1) / 1][Stage 419:>                                                        (0 + 1) / 1][Stage 419:>                                                        (0 + 1) / 1][Stage 419:>                                                        (0 + 1) / 1]                                                                                Total Trials: 140: 140 succeeded, 0 failed, 0 cancelled.
[Stage 420:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 420:>                                                        (0 + 1) / 1][Stage 420:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:59:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 420:>                                                        (0 + 1) / 1][Stage 420:>                                                        (0 + 1) / 1][Stage 420:>                                                        (0 + 1) / 1][Stage 420:>                                                        (0 + 1) / 1][Stage 420:>                                                        (0 + 1) / 1][Stage 420:>                                                        (0 + 1) / 1][Stage 420:>                                                        (0 + 1) / 1][Stage 420:>                                                        (0 + 1) / 1][Stage 420:>                                                        (0 + 1) / 1][Stage 420:>                                                        (0 + 1) / 1]                                                                                [Stage 421:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 421:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:11:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 421:>                                                        (0 + 1) / 1][Stage 421:>                                                        (0 + 1) / 1][Stage 421:>                                                        (0 + 1) / 1][Stage 421:>                                                        (0 + 1) / 1]                                                                                [Stage 422:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:16:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 422:>                                                        (0 + 1) / 1][Stage 422:>                                                        (0 + 1) / 1]                                                                                [Stage 423:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:22:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1][Stage 423:>                                                        (0 + 1) / 1]                                                                                [Stage 424:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:40:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 424:>                                                        (0 + 1) / 1]                                                                                [Stage 425:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:42:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 425:>                                                        (0 + 1) / 1][Stage 425:>                                                        (0 + 1) / 1]                                                                                [Stage 426:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:45:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 426:>                                                        (0 + 1) / 1][Stage 426:>                                                        (0 + 1) / 1]                                                                                [Stage 427:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:50:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1][Stage 427:>                                                        (0 + 1) / 1]                                                                                [Stage 428:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:08:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1][Stage 428:>                                                        (0 + 1) / 1]                                                                                [Stage 429:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:26:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 429:>                                                        (0 + 1) / 1][Stage 429:>                                                        (0 + 1) / 1][Stage 429:>                                                        (0 + 1) / 1][Stage 429:>                                                        (0 + 1) / 1]                                                                                [Stage 430:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:33:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1][Stage 430:>                                                        (0 + 1) / 1]                                                                                [Stage 431:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 431:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:47:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 431:>                                                        (0 + 1) / 1][Stage 431:>                                                        (0 + 1) / 1][Stage 431:>                                                        (0 + 1) / 1][Stage 431:>                                                        (0 + 1) / 1][Stage 431:>                                                        (0 + 1) / 1][Stage 431:>                                                        (0 + 1) / 1]                                                                                [Stage 432:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 432:>                                                        (0 + 1) / 1][Stage 432:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:56:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 432:>                                                        (0 + 1) / 1][Stage 432:>                                                        (0 + 1) / 1][Stage 432:>                                                        (0 + 1) / 1][Stage 432:>                                                        (0 + 1) / 1][Stage 432:>                                                        (0 + 1) / 1][Stage 432:>                                                        (0 + 1) / 1][Stage 432:>                                                        (0 + 1) / 1][Stage 432:>                                                        (0 + 1) / 1]                                                                                [Stage 433:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 433:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:06:33] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 433:>                                                        (0 + 1) / 1][Stage 433:>                                                        (0 + 1) / 1][Stage 433:>                                                        (0 + 1) / 1][Stage 433:>                                                        (0 + 1) / 1][Stage 433:>                                                        (0 + 1) / 1][Stage 433:>                                                        (0 + 1) / 1]                                                                                [Stage 434:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:13:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 434:>                                                        (0 + 1) / 1][Stage 434:>                                                        (0 + 1) / 1][Stage 434:>                                                        (0 + 1) / 1]                                                                                [Stage 435:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 435:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:17:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 435:>                                                        (0 + 1) / 1][Stage 435:>                                                        (0 + 1) / 1][Stage 435:>                                                        (0 + 1) / 1][Stage 435:>                                                        (0 + 1) / 1][Stage 435:>                                                        (0 + 1) / 1][Stage 435:>                                                        (0 + 1) / 1]                                                                                [Stage 436:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:23:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 436:>                                                        (0 + 1) / 1][Stage 436:>                                                        (0 + 1) / 1][Stage 436:>                                                        (0 + 1) / 1][Stage 436:>                                                        (0 + 1) / 1][Stage 436:>                                                        (0 + 1) / 1][Stage 436:>                                                        (0 + 1) / 1][Stage 436:>                                                        (0 + 1) / 1][Stage 436:>                                                        (0 + 1) / 1][Stage 436:>                                                        (0 + 1) / 1][Stage 436:>                                                        (0 + 1) / 1]                                                                                [Stage 437:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 437:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:35:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 437:>                                                        (0 + 1) / 1][Stage 437:>                                                        (0 + 1) / 1][Stage 437:>                                                        (0 + 1) / 1][Stage 437:>                                                        (0 + 1) / 1]                                                                                [Stage 438:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 438:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:41:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 438:>                                                        (0 + 1) / 1][Stage 438:>                                                        (0 + 1) / 1][Stage 438:>                                                        (0 + 1) / 1]                                                                                [Stage 439:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:48:47] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1][Stage 439:>                                                        (0 + 1) / 1]                                                                                [Stage 440:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 440:>                                                        (0 + 1) / 1][Stage 440:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:06:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 440:>                                                        (0 + 1) / 1][Stage 440:>                                                        (0 + 1) / 1][Stage 440:>                                                        (0 + 1) / 1][Stage 440:>                                                        (0 + 1) / 1][Stage 440:>                                                        (0 + 1) / 1][Stage 440:>                                                        (0 + 1) / 1][Stage 440:>                                                        (0 + 1) / 1][Stage 440:>                                                        (0 + 1) / 1][Stage 440:>                                                        (0 + 1) / 1][Stage 440:>                                                        (0 + 1) / 1][Stage 440:>                                                        (0 + 1) / 1]                                                                                [Stage 441:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 441:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:19:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 441:>                                                        (0 + 1) / 1][Stage 441:>                                                        (0 + 1) / 1][Stage 441:>                                                        (0 + 1) / 1]                                                                                [Stage 442:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 442:>                                                        (0 + 1) / 1][Stage 442:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:25:44] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 442:>                                                        (0 + 1) / 1][Stage 442:>                                                        (0 + 1) / 1][Stage 442:>                                                        (0 + 1) / 1][Stage 442:>                                                        (0 + 1) / 1][Stage 442:>                                                        (0 + 1) / 1][Stage 442:>                                                        (0 + 1) / 1][Stage 442:>                                                        (0 + 1) / 1][Stage 442:>                                                        (0 + 1) / 1][Stage 442:>                                                        (0 + 1) / 1][Stage 442:>                                                        (0 + 1) / 1][Stage 442:>                                                        (0 + 1) / 1]                                                                                [Stage 443:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 443:>                                                        (0 + 1) / 1][Stage 443:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:39:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 443:>                                                        (0 + 1) / 1][Stage 443:>                                                        (0 + 1) / 1][Stage 443:>                                                        (0 + 1) / 1][Stage 443:>                                                        (0 + 1) / 1][Stage 443:>                                                        (0 + 1) / 1][Stage 443:>                                                        (0 + 1) / 1][Stage 443:>                                                        (0 + 1) / 1][Stage 443:>                                                        (0 + 1) / 1][Stage 443:>                                                        (0 + 1) / 1][Stage 443:>                                                        (0 + 1) / 1][Stage 443:>                                                        (0 + 1) / 1]                                                                                [Stage 444:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 444:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:51:44] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 444:>                                                        (0 + 1) / 1][Stage 444:>                                                        (0 + 1) / 1][Stage 444:>                                                        (0 + 1) / 1][Stage 444:>                                                        (0 + 1) / 1][Stage 444:>                                                        (0 + 1) / 1][Stage 444:>                                                        (0 + 1) / 1]                                                                                [Stage 445:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 445:>                                                        (0 + 1) / 1][Stage 445:>                                                        (0 + 1) / 1][Stage 445:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 445:>                                                        (0 + 1) / 1][Stage 445:>                                                        (0 + 1) / 1][Stage 445:>                                                        (0 + 1) / 1][Stage 445:>                                                        (0 + 1) / 1][Stage 445:>                                                        (0 + 1) / 1][Stage 445:>                                                        (0 + 1) / 1][Stage 445:>                                                        (0 + 1) / 1][Stage 445:>                                                        (0 + 1) / 1][Stage 445:>                                                        (0 + 1) / 1][Stage 445:>                                                        (0 + 1) / 1][Stage 445:>                                                        (0 + 1) / 1]                                                                                [Stage 446:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:18:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1][Stage 446:>                                                        (0 + 1) / 1]                                                                                [Stage 447:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 447:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:36:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 447:>                                                        (0 + 1) / 1][Stage 447:>                                                        (0 + 1) / 1][Stage 447:>                                                        (0 + 1) / 1]                                                                                [Stage 448:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:44:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1][Stage 448:>                                                        (0 + 1) / 1]                                                                                [Stage 449:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 449:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:59:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 449:>                                                        (0 + 1) / 1][Stage 449:>                                                        (0 + 1) / 1][Stage 449:>                                                        (0 + 1) / 1][Stage 449:>                                                        (0 + 1) / 1][Stage 449:>                                                        (0 + 1) / 1]                                                                                [Stage 450:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 450:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:05:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 450:>                                                        (0 + 1) / 1][Stage 450:>                                                        (0 + 1) / 1][Stage 450:>                                                        (0 + 1) / 1][Stage 450:>                                                        (0 + 1) / 1][Stage 450:>                                                        (0 + 1) / 1][Stage 450:>                                                        (0 + 1) / 1]                                                                                [Stage 451:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 451:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:13:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 451:>                                                        (0 + 1) / 1][Stage 451:>                                                        (0 + 1) / 1][Stage 451:>                                                        (0 + 1) / 1]                                                                                [Stage 452:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 452:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:18:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 452:>                                                        (0 + 1) / 1][Stage 452:>                                                        (0 + 1) / 1][Stage 452:>                                                        (0 + 1) / 1][Stage 452:>                                                        (0 + 1) / 1][Stage 452:>                                                        (0 + 1) / 1]                                                                                [Stage 453:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 453:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:25:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 453:>                                                        (0 + 1) / 1][Stage 453:>                                                        (0 + 1) / 1][Stage 453:>                                                        (0 + 1) / 1][Stage 453:>                                                        (0 + 1) / 1][Stage 453:>                                                        (0 + 1) / 1][Stage 453:>                                                        (0 + 1) / 1][Stage 453:>                                                        (0 + 1) / 1]                                                                                [Stage 454:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 454:>                                                        (0 + 1) / 1][Stage 454:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:34:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 454:>                                                        (0 + 1) / 1][Stage 454:>                                                        (0 + 1) / 1][Stage 454:>                                                        (0 + 1) / 1][Stage 454:>                                                        (0 + 1) / 1][Stage 454:>                                                        (0 + 1) / 1][Stage 454:>                                                        (0 + 1) / 1][Stage 454:>                                                        (0 + 1) / 1]                                                                                [Stage 455:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 455:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:43:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 455:>                                                        (0 + 1) / 1][Stage 455:>                                                        (0 + 1) / 1][Stage 455:>                                                        (0 + 1) / 1][Stage 455:>                                                        (0 + 1) / 1][Stage 455:>                                                        (0 + 1) / 1][Stage 455:>                                                        (0 + 1) / 1]                                                                                [Stage 456:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 456:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:50:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 456:>                                                        (0 + 1) / 1][Stage 456:>                                                        (0 + 1) / 1][Stage 456:>                                                        (0 + 1) / 1]                                                                                [Stage 457:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:59:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1][Stage 457:>                                                        (0 + 1) / 1]                                                                                [Stage 458:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:23:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1][Stage 458:>                                                        (0 + 1) / 1]                                                                                [Stage 459:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 459:>                                                        (0 + 1) / 1][Stage 459:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:39:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 459:>                                                        (0 + 1) / 1][Stage 459:>                                                        (0 + 1) / 1][Stage 459:>                                                        (0 + 1) / 1][Stage 459:>                                                        (0 + 1) / 1][Stage 459:>                                                        (0 + 1) / 1][Stage 459:>                                                        (0 + 1) / 1][Stage 459:>                                                        (0 + 1) / 1][Stage 459:>                                                        (0 + 1) / 1][Stage 459:>                                                        (0 + 1) / 1][Stage 459:>                                                        (0 + 1) / 1][Stage 459:>                                                        (0 + 1) / 1]                                                                                [Stage 460:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 460:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:50:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 460:>                                                        (0 + 1) / 1][Stage 460:>                                                        (0 + 1) / 1][Stage 460:>                                                        (0 + 1) / 1]                                                                                [Stage 461:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 461:>                                                        (0 + 1) / 1][Stage 461:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:56:32] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 461:>                                                        (0 + 1) / 1][Stage 461:>                                                        (0 + 1) / 1][Stage 461:>                                                        (0 + 1) / 1][Stage 461:>                                                        (0 + 1) / 1][Stage 461:>                                                        (0 + 1) / 1][Stage 461:>                                                        (0 + 1) / 1]                                                                                [Stage 462:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 462:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:05:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 462:>                                                        (0 + 1) / 1][Stage 462:>                                                        (0 + 1) / 1][Stage 462:>                                                        (0 + 1) / 1][Stage 462:>                                                        (0 + 1) / 1][Stage 462:>                                                        (0 + 1) / 1]                                                                                [Stage 463:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:15:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1][Stage 463:>                                                        (0 + 1) / 1]                                                                                [Stage 464:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:31:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 464:>                                                        (0 + 1) / 1][Stage 464:>                                                        (0 + 1) / 1][Stage 464:>                                                        (0 + 1) / 1]                                                                                [Stage 465:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 465:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:36:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 465:>                                                        (0 + 1) / 1][Stage 465:>                                                        (0 + 1) / 1][Stage 465:>                                                        (0 + 1) / 1][Stage 465:>                                                        (0 + 1) / 1][Stage 465:>                                                        (0 + 1) / 1][Stage 465:>                                                        (0 + 1) / 1][Stage 465:>                                                        (0 + 1) / 1]                                                                                [Stage 466:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 466:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:44:34] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 466:>                                                        (0 + 1) / 1][Stage 466:>                                                        (0 + 1) / 1][Stage 466:>                                                        (0 + 1) / 1][Stage 466:>                                                        (0 + 1) / 1][Stage 466:>                                                        (0 + 1) / 1]                                                                                [Stage 467:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:53:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1][Stage 467:>                                                        (0 + 1) / 1]                                                                                [Stage 468:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 468:>                                                        (0 + 1) / 1][Stage 468:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:11:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 468:>                                                        (0 + 1) / 1][Stage 468:>                                                        (0 + 1) / 1][Stage 468:>                                                        (0 + 1) / 1][Stage 468:>                                                        (0 + 1) / 1][Stage 468:>                                                        (0 + 1) / 1][Stage 468:>                                                        (0 + 1) / 1][Stage 468:>                                                        (0 + 1) / 1][Stage 468:>                                                        (0 + 1) / 1][Stage 468:>                                                        (0 + 1) / 1][Stage 468:>                                                        (0 + 1) / 1][Stage 468:>                                                        (0 + 1) / 1][Stage 468:>                                                        (0 + 1) / 1]                                                                                [Stage 469:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:23:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 469:>                                                        (0 + 1) / 1]                                                                                [Stage 470:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:25:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 470:>                                                        (0 + 1) / 1][Stage 470:>                                                        (0 + 1) / 1][Stage 470:>                                                        (0 + 1) / 1]                                                                                [Stage 471:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 471:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:30:42] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 471:>                                                        (0 + 1) / 1][Stage 471:>                                                        (0 + 1) / 1][Stage 471:>                                                        (0 + 1) / 1][Stage 471:>                                                        (0 + 1) / 1][Stage 471:>                                                        (0 + 1) / 1][Stage 471:>                                                        (0 + 1) / 1][Stage 471:>                                                        (0 + 1) / 1]                                                                                [Stage 472:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 472:>                                                        (0 + 1) / 1][Stage 472:>                                                        (0 + 1) / 1][Stage 472:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:40:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 472:>                                                        (0 + 1) / 1][Stage 472:>                                                        (0 + 1) / 1][Stage 472:>                                                        (0 + 1) / 1][Stage 472:>                                                        (0 + 1) / 1][Stage 472:>                                                        (0 + 1) / 1][Stage 472:>                                                        (0 + 1) / 1][Stage 472:>                                                        (0 + 1) / 1][Stage 472:>                                                        (0 + 1) / 1][Stage 472:>                                                        (0 + 1) / 1][Stage 472:>                                                        (0 + 1) / 1][Stage 472:>                                                        (0 + 1) / 1]                                                                                [Stage 473:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 473:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:53:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 473:>                                                        (0 + 1) / 1][Stage 473:>                                                        (0 + 1) / 1][Stage 473:>                                                        (0 + 1) / 1][Stage 473:>                                                        (0 + 1) / 1]                                                                                [Stage 474:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 474:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:59:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 474:>                                                        (0 + 1) / 1][Stage 474:>                                                        (0 + 1) / 1][Stage 474:>                                                        (0 + 1) / 1]                                                                                [Stage 475:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 475:>                                                        (0 + 1) / 1][Stage 475:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:05:42] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 475:>                                                        (0 + 1) / 1][Stage 475:>                                                        (0 + 1) / 1][Stage 475:>                                                        (0 + 1) / 1][Stage 475:>                                                        (0 + 1) / 1][Stage 475:>                                                        (0 + 1) / 1][Stage 475:>                                                        (0 + 1) / 1][Stage 475:>                                                        (0 + 1) / 1][Stage 475:>                                                        (0 + 1) / 1][Stage 475:>                                                        (0 + 1) / 1]                                                                                [Stage 476:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 476:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:16:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 476:>                                                        (0 + 1) / 1][Stage 476:>                                                        (0 + 1) / 1][Stage 476:>                                                        (0 + 1) / 1][Stage 476:>                                                        (0 + 1) / 1][Stage 476:>                                                        (0 + 1) / 1][Stage 476:>                                                        (0 + 1) / 1]                                                                                [Stage 477:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 477:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:24:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 477:>                                                        (0 + 1) / 1][Stage 477:>                                                        (0 + 1) / 1][Stage 477:>                                                        (0 + 1) / 1][Stage 477:>                                                        (0 + 1) / 1][Stage 477:>                                                        (0 + 1) / 1][Stage 477:>                                                        (0 + 1) / 1][Stage 477:>                                                        (0 + 1) / 1]                                                                                [Stage 478:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:35:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1][Stage 478:>                                                        (0 + 1) / 1]                                                                                [Stage 479:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 479:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:51:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 479:>                                                        (0 + 1) / 1][Stage 479:>                                                        (0 + 1) / 1][Stage 479:>                                                        (0 + 1) / 1][Stage 479:>                                                        (0 + 1) / 1]                                                                                [Stage 480:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 480:>                                                        (0 + 1) / 1][Stage 480:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:57:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 480:>                                                        (0 + 1) / 1][Stage 480:>                                                        (0 + 1) / 1][Stage 480:>                                                        (0 + 1) / 1][Stage 480:>                                                        (0 + 1) / 1][Stage 480:>                                                        (0 + 1) / 1][Stage 480:>                                                        (0 + 1) / 1][Stage 480:>                                                        (0 + 1) / 1][Stage 480:>                                                        (0 + 1) / 1]                                                                                [Stage 481:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 481:>                                                        (0 + 1) / 1][Stage 481:>                                                        (0 + 1) / 1][Stage 481:>                                                        (0 + 1) / 1]                                                                                [Stage 482:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:15:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1][Stage 482:>                                                        (0 + 1) / 1]                                                                                [Stage 483:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 483:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:42:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 483:>                                                        (0 + 1) / 1][Stage 483:>                                                        (0 + 1) / 1][Stage 483:>                                                        (0 + 1) / 1][Stage 483:>                                                        (0 + 1) / 1][Stage 483:>                                                        (0 + 1) / 1]                                                                                [Stage 484:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:50:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1][Stage 484:>                                                        (0 + 1) / 1]                                                                                [Stage 485:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 485:>                                                        (0 + 1) / 1][Stage 485:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:07:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 485:>                                                        (0 + 1) / 1][Stage 485:>                                                        (0 + 1) / 1][Stage 485:>                                                        (0 + 1) / 1][Stage 485:>                                                        (0 + 1) / 1][Stage 485:>                                                        (0 + 1) / 1][Stage 485:>                                                        (0 + 1) / 1][Stage 485:>                                                        (0 + 1) / 1][Stage 485:>                                                        (0 + 1) / 1][Stage 485:>                                                        (0 + 1) / 1][Stage 485:>                                                        (0 + 1) / 1][Stage 485:>                                                        (0 + 1) / 1]                                                                                [Stage 486:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 486:>                                                        (0 + 1) / 1][Stage 486:>                                                        (0 + 1) / 1][Stage 486:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:22:23] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 486:>                                                        (0 + 1) / 1][Stage 486:>                                                        (0 + 1) / 1][Stage 486:>                                                        (0 + 1) / 1][Stage 486:>                                                        (0 + 1) / 1][Stage 486:>                                                        (0 + 1) / 1][Stage 486:>                                                        (0 + 1) / 1][Stage 486:>                                                        (0 + 1) / 1][Stage 486:>                                                        (0 + 1) / 1][Stage 486:>                                                        (0 + 1) / 1][Stage 486:>                                                        (0 + 1) / 1][Stage 486:>                                                        (0 + 1) / 1][Stage 486:>                                                        (0 + 1) / 1]                                                                                [Stage 487:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 487:>                                                        (0 + 1) / 1][Stage 487:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:37:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 487:>                                                        (0 + 1) / 1][Stage 487:>                                                        (0 + 1) / 1][Stage 487:>                                                        (0 + 1) / 1][Stage 487:>                                                        (0 + 1) / 1][Stage 487:>                                                        (0 + 1) / 1][Stage 487:>                                                        (0 + 1) / 1][Stage 487:>                                                        (0 + 1) / 1][Stage 487:>                                                        (0 + 1) / 1]                                                                                [Stage 488:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 488:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:47:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 488:>                                                        (0 + 1) / 1][Stage 488:>                                                        (0 + 1) / 1][Stage 488:>                                                        (0 + 1) / 1][Stage 488:>                                                        (0 + 1) / 1][Stage 488:>                                                        (0 + 1) / 1][Stage 488:>                                                        (0 + 1) / 1][Stage 488:>                                                        (0 + 1) / 1]                                                                                [Stage 489:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 489:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:56:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 489:>                                                        (0 + 1) / 1][Stage 489:>                                                        (0 + 1) / 1][Stage 489:>                                                        (0 + 1) / 1][Stage 489:>                                                        (0 + 1) / 1]                                                                                [Stage 490:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 490:>                                                        (0 + 1) / 1][Stage 490:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:02:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 490:>                                                        (0 + 1) / 1][Stage 490:>                                                        (0 + 1) / 1][Stage 490:>                                                        (0 + 1) / 1][Stage 490:>                                                        (0 + 1) / 1][Stage 490:>                                                        (0 + 1) / 1][Stage 490:>                                                        (0 + 1) / 1][Stage 490:>                                                        (0 + 1) / 1][Stage 490:>                                                        (0 + 1) / 1][Stage 490:>                                                        (0 + 1) / 1]                                                                                [Stage 491:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:15:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1][Stage 491:>                                                        (0 + 1) / 1]                                                                                [Stage 492:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 492:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:33:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 492:>                                                        (0 + 1) / 1][Stage 492:>                                                        (0 + 1) / 1][Stage 492:>                                                        (0 + 1) / 1][Stage 492:>                                                        (0 + 1) / 1][Stage 492:>                                                        (0 + 1) / 1][Stage 492:>                                                        (0 + 1) / 1][Stage 492:>                                                        (0 + 1) / 1][Stage 492:>                                                        (0 + 1) / 1]                                                                                [Stage 493:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:44:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1][Stage 493:>                                                        (0 + 1) / 1]                                                                                [Stage 494:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 494:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:01:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 494:>                                                        (0 + 1) / 1][Stage 494:>                                                        (0 + 1) / 1][Stage 494:>                                                        (0 + 1) / 1]                                                                                [Stage 495:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:10:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1][Stage 495:>                                                        (0 + 1) / 1]                                                                                [Stage 496:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:31:37] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1][Stage 496:>                                                        (0 + 1) / 1]                                                                                [Stage 497:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 497:>                                                        (0 + 1) / 1][Stage 497:>                                                        (0 + 1) / 1][Stage 497:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:50:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 497:>                                                        (0 + 1) / 1][Stage 497:>                                                        (0 + 1) / 1][Stage 497:>                                                        (0 + 1) / 1][Stage 497:>                                                        (0 + 1) / 1][Stage 497:>                                                        (0 + 1) / 1][Stage 497:>                                                        (0 + 1) / 1][Stage 497:>                                                        (0 + 1) / 1][Stage 497:>                                                        (0 + 1) / 1][Stage 497:>                                                        (0 + 1) / 1][Stage 497:>                                                        (0 + 1) / 1][Stage 497:>                                                        (0 + 1) / 1][Stage 497:>                                                        (0 + 1) / 1]                                                                                [Stage 498:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 498:>                                                        (0 + 1) / 1][Stage 498:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:04:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 498:>                                                        (0 + 1) / 1][Stage 498:>                                                        (0 + 1) / 1][Stage 498:>                                                        (0 + 1) / 1][Stage 498:>                                                        (0 + 1) / 1][Stage 498:>                                                        (0 + 1) / 1][Stage 498:>                                                        (0 + 1) / 1][Stage 498:>                                                        (0 + 1) / 1][Stage 498:>                                                        (0 + 1) / 1]                                                                                [Stage 499:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:16:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1][Stage 499:>                                                        (0 + 1) / 1]                                                                                [Stage 500:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:34:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1][Stage 500:>                                                        (0 + 1) / 1]                                                                                [Stage 501:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 501:>                                                        (0 + 1) / 1][Stage 501:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:55:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 501:>                                                        (0 + 1) / 1][Stage 501:>                                                        (0 + 1) / 1][Stage 501:>                                                        (0 + 1) / 1][Stage 501:>                                                        (0 + 1) / 1][Stage 501:>                                                        (0 + 1) / 1][Stage 501:>                                                        (0 + 1) / 1][Stage 501:>                                                        (0 + 1) / 1][Stage 501:>                                                        (0 + 1) / 1]                                                                                [Stage 502:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 502:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:05:32] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 502:>                                                        (0 + 1) / 1][Stage 502:>                                                        (0 + 1) / 1][Stage 502:>                                                        (0 + 1) / 1][Stage 502:>                                                        (0 + 1) / 1][Stage 502:>                                                        (0 + 1) / 1]                                                                                [Stage 503:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:16:29] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1][Stage 503:>                                                        (0 + 1) / 1]                                                                                [Stage 504:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 504:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:39:11] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 504:>                                                        (0 + 1) / 1][Stage 504:>                                                        (0 + 1) / 1][Stage 504:>                                                        (0 + 1) / 1]                                                                                [Stage 505:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:44:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 505:>                                                        (0 + 1) / 1][Stage 505:>                                                        (0 + 1) / 1][Stage 505:>                                                        (0 + 1) / 1]                                                                                [Stage 506:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 506:>                                                        (0 + 1) / 1][Stage 506:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:49:32] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 506:>                                                        (0 + 1) / 1][Stage 506:>                                                        (0 + 1) / 1][Stage 506:>                                                        (0 + 1) / 1][Stage 506:>                                                        (0 + 1) / 1][Stage 506:>                                                        (0 + 1) / 1][Stage 506:>                                                        (0 + 1) / 1][Stage 506:>                                                        (0 + 1) / 1][Stage 506:>                                                        (0 + 1) / 1][Stage 506:>                                                        (0 + 1) / 1][Stage 506:>                                                        (0 + 1) / 1]                                                                                [Stage 507:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 507:>                                                        (0 + 1) / 1][Stage 507:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:01:22] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 507:>                                                        (0 + 1) / 1][Stage 507:>                                                        (0 + 1) / 1][Stage 507:>                                                        (0 + 1) / 1][Stage 507:>                                                        (0 + 1) / 1][Stage 507:>                                                        (0 + 1) / 1][Stage 507:>                                                        (0 + 1) / 1][Stage 507:>                                                        (0 + 1) / 1]                                                                                [Stage 508:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:13:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1][Stage 508:>                                                        (0 + 1) / 1]                                                                                [Stage 509:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:30:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 509:>                                                        (0 + 1) / 1][Stage 509:>                                                        (0 + 1) / 1][Stage 509:>                                                        (0 + 1) / 1]                                                                                [Stage 510:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 510:>                                                        (0 + 1) / 1][Stage 510:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:35:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 510:>                                                        (0 + 1) / 1][Stage 510:>                                                        (0 + 1) / 1][Stage 510:>                                                        (0 + 1) / 1][Stage 510:>                                                        (0 + 1) / 1][Stage 510:>                                                        (0 + 1) / 1][Stage 510:>                                                        (0 + 1) / 1][Stage 510:>                                                        (0 + 1) / 1][Stage 510:>                                                        (0 + 1) / 1][Stage 510:>                                                        (0 + 1) / 1][Stage 510:>                                                        (0 + 1) / 1]                                                                                [Stage 511:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:46:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 511:>                                                        (0 + 1) / 1]                                                                                [Stage 512:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:53:47] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1][Stage 512:>                                                        (0 + 1) / 1]                                                                                [Stage 513:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:18:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 513:>                                                        (0 + 1) / 1][Stage 513:>                                                        (0 + 1) / 1][Stage 513:>                                                        (0 + 1) / 1][Stage 513:>                                                        (0 + 1) / 1]                                                                                [Stage 514:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 514:>                                                        (0 + 1) / 1][Stage 514:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:23:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 514:>                                                        (0 + 1) / 1][Stage 514:>                                                        (0 + 1) / 1][Stage 514:>                                                        (0 + 1) / 1][Stage 514:>                                                        (0 + 1) / 1][Stage 514:>                                                        (0 + 1) / 1][Stage 514:>                                                        (0 + 1) / 1][Stage 514:>                                                        (0 + 1) / 1][Stage 514:>                                                        (0 + 1) / 1][Stage 514:>                                                        (0 + 1) / 1]                                                                                [Stage 515:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 515:>                                                        (0 + 1) / 1][Stage 515:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:35:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 515:>                                                        (0 + 1) / 1][Stage 515:>                                                        (0 + 1) / 1][Stage 515:>                                                        (0 + 1) / 1][Stage 515:>                                                        (0 + 1) / 1][Stage 515:>                                                        (0 + 1) / 1][Stage 515:>                                                        (0 + 1) / 1][Stage 515:>                                                        (0 + 1) / 1][Stage 515:>                                                        (0 + 1) / 1]                                                                                [Stage 516:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:44:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 516:>                                                        (0 + 1) / 1]                                                                                [Stage 517:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 517:>                                                        (0 + 1) / 1][Stage 517:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:47:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 517:>                                                        (0 + 1) / 1][Stage 517:>                                                        (0 + 1) / 1][Stage 517:>                                                        (0 + 1) / 1][Stage 517:>                                                        (0 + 1) / 1][Stage 517:>                                                        (0 + 1) / 1][Stage 517:>                                                        (0 + 1) / 1][Stage 517:>                                                        (0 + 1) / 1][Stage 517:>                                                        (0 + 1) / 1][Stage 517:>                                                        (0 + 1) / 1]                                                                                [Stage 518:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 518:>                                                        (0 + 1) / 1][Stage 518:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:59:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 518:>                                                        (0 + 1) / 1][Stage 518:>                                                        (0 + 1) / 1][Stage 518:>                                                        (0 + 1) / 1][Stage 518:>                                                        (0 + 1) / 1][Stage 518:>                                                        (0 + 1) / 1][Stage 518:>                                                        (0 + 1) / 1][Stage 518:>                                                        (0 + 1) / 1][Stage 518:>                                                        (0 + 1) / 1][Stage 518:>                                                        (0 + 1) / 1][Stage 518:>                                                        (0 + 1) / 1][Stage 518:>                                                        (0 + 1) / 1][Stage 518:=========================================================(1 + 0) / 1]                                                                                [Stage 519:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 519:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:11:33] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 519:>                                                        (0 + 1) / 1][Stage 519:>                                                        (0 + 1) / 1][Stage 519:>                                                        (0 + 1) / 1][Stage 519:>                                                        (0 + 1) / 1][Stage 519:>                                                        (0 + 1) / 1]                                                                                [Stage 520:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 520:>                                                        (0 + 1) / 1][Stage 520:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:18:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 520:>                                                        (0 + 1) / 1][Stage 520:>                                                        (0 + 1) / 1][Stage 520:>                                                        (0 + 1) / 1][Stage 520:>                                                        (0 + 1) / 1][Stage 520:>                                                        (0 + 1) / 1][Stage 520:>                                                        (0 + 1) / 1][Stage 520:>                                                        (0 + 1) / 1][Stage 520:>                                                        (0 + 1) / 1]                                                                                [Stage 521:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:31:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1][Stage 521:>                                                        (0 + 1) / 1]                                                                                [Stage 522:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 522:>                                                        (0 + 1) / 1][Stage 522:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:49:20] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 522:>                                                        (0 + 1) / 1][Stage 522:>                                                        (0 + 1) / 1][Stage 522:>                                                        (0 + 1) / 1][Stage 522:>                                                        (0 + 1) / 1][Stage 522:>                                                        (0 + 1) / 1][Stage 522:>                                                        (0 + 1) / 1][Stage 522:>                                                        (0 + 1) / 1][Stage 522:>                                                        (0 + 1) / 1]                                                                                [Stage 523:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:02:47] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1][Stage 523:>                                                        (0 + 1) / 1]                                                                                [Stage 524:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:23:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 524:>                                                        (0 + 1) / 1][Stage 524:>                                                        (0 + 1) / 1][Stage 524:>                                                        (0 + 1) / 1]                                                                                [Stage 525:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:27:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 525:>                                                        (0 + 1) / 1][Stage 525:>                                                        (0 + 1) / 1][Stage 525:>                                                        (0 + 1) / 1][Stage 525:>                                                        (0 + 1) / 1]                                                                                [Stage 526:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 526:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:32:36] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 526:>                                                        (0 + 1) / 1][Stage 526:>                                                        (0 + 1) / 1][Stage 526:>                                                        (0 + 1) / 1][Stage 526:>                                                        (0 + 1) / 1][Stage 526:>                                                        (0 + 1) / 1][Stage 526:>                                                        (0 + 1) / 1]                                                                                [Stage 527:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:43:32] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1][Stage 527:>                                                        (0 + 1) / 1]                                                                                [Stage 528:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:04:22] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1][Stage 528:>                                                        (0 + 1) / 1]                                                                                [Stage 529:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:22:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1][Stage 529:>                                                        (0 + 1) / 1]                                                                                [Stage 530:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 530:>                                                        (0 + 1) / 1][Stage 530:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:42:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 530:>                                                        (0 + 1) / 1][Stage 530:>                                                        (0 + 1) / 1][Stage 530:>                                                        (0 + 1) / 1][Stage 530:>                                                        (0 + 1) / 1][Stage 530:>                                                        (0 + 1) / 1][Stage 530:>                                                        (0 + 1) / 1][Stage 530:>                                                        (0 + 1) / 1][Stage 530:>                                                        (0 + 1) / 1]                                                                                [Stage 531:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 531:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:52:42] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 531:>                                                        (0 + 1) / 1][Stage 531:>                                                        (0 + 1) / 1][Stage 531:>                                                        (0 + 1) / 1][Stage 531:>                                                        (0 + 1) / 1][Stage 531:>                                                        (0 + 1) / 1][Stage 531:>                                                        (0 + 1) / 1][Stage 531:>                                                        (0 + 1) / 1]                                                                                [Stage 532:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:03:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1][Stage 532:>                                                        (0 + 1) / 1]                                                                                [Stage 533:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:21:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1][Stage 533:>                                                        (0 + 1) / 1]                                                                                [Stage 534:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:43:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1][Stage 534:>                                                        (0 + 1) / 1]                                                                                [Stage 535:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 535:>                                                        (0 + 1) / 1][Stage 535:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:58:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 535:>                                                        (0 + 1) / 1][Stage 535:>                                                        (0 + 1) / 1][Stage 535:>                                                        (0 + 1) / 1][Stage 535:>                                                        (0 + 1) / 1][Stage 535:>                                                        (0 + 1) / 1][Stage 535:>                                                        (0 + 1) / 1][Stage 535:>                                                        (0 + 1) / 1][Stage 535:>                                                        (0 + 1) / 1]                                                                                [Stage 536:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 536:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:08:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 536:>                                                        (0 + 1) / 1][Stage 536:>                                                        (0 + 1) / 1][Stage 536:>                                                        (0 + 1) / 1]                                                                                [Stage 537:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 537:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:13:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 537:>                                                        (0 + 1) / 1][Stage 537:>                                                        (0 + 1) / 1][Stage 537:>                                                        (0 + 1) / 1][Stage 537:>                                                        (0 + 1) / 1][Stage 537:>                                                        (0 + 1) / 1]                                                                                [Stage 538:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 538:>                                                        (0 + 1) / 1][Stage 538:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:21:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 538:>                                                        (0 + 1) / 1][Stage 538:>                                                        (0 + 1) / 1][Stage 538:>                                                        (0 + 1) / 1][Stage 538:>                                                        (0 + 1) / 1][Stage 538:>                                                        (0 + 1) / 1][Stage 538:>                                                        (0 + 1) / 1][Stage 538:>                                                        (0 + 1) / 1][Stage 538:>                                                        (0 + 1) / 1][Stage 538:>                                                        (0 + 1) / 1][Stage 538:>                                                        (0 + 1) / 1]                                                                                [Stage 539:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 539:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:32:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 539:>                                                        (0 + 1) / 1][Stage 539:>                                                        (0 + 1) / 1][Stage 539:>                                                        (0 + 1) / 1][Stage 539:>                                                        (0 + 1) / 1]                                                                                [Stage 540:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 540:>                                                        (0 + 1) / 1][Stage 540:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:39:23] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 540:>                                                        (0 + 1) / 1][Stage 540:>                                                        (0 + 1) / 1][Stage 540:>                                                        (0 + 1) / 1][Stage 540:>                                                        (0 + 1) / 1][Stage 540:>                                                        (0 + 1) / 1][Stage 540:>                                                        (0 + 1) / 1][Stage 540:>                                                        (0 + 1) / 1][Stage 540:>                                                        (0 + 1) / 1][Stage 540:>                                                        (0 + 1) / 1][Stage 540:>                                                        (0 + 1) / 1]                                                                                [Stage 541:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 541:>                                                        (0 + 1) / 1][Stage 541:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:52:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 541:>                                                        (0 + 1) / 1][Stage 541:>                                                        (0 + 1) / 1][Stage 541:>                                                        (0 + 1) / 1][Stage 541:>                                                        (0 + 1) / 1][Stage 541:>                                                        (0 + 1) / 1][Stage 541:>                                                        (0 + 1) / 1][Stage 541:>                                                        (0 + 1) / 1][Stage 541:>                                                        (0 + 1) / 1]                                                                                [Stage 542:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 542:>                                                        (0 + 1) / 1][Stage 542:>                                                        (0 + 1) / 1][Stage 542:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:04:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 542:>                                                        (0 + 1) / 1][Stage 542:>                                                        (0 + 1) / 1][Stage 542:>                                                        (0 + 1) / 1][Stage 542:>                                                        (0 + 1) / 1][Stage 542:>                                                        (0 + 1) / 1][Stage 542:>                                                        (0 + 1) / 1][Stage 542:>                                                        (0 + 1) / 1][Stage 542:>                                                        (0 + 1) / 1][Stage 542:>                                                        (0 + 1) / 1][Stage 542:>                                                        (0 + 1) / 1][Stage 542:>                                                        (0 + 1) / 1]                                                                                [Stage 543:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:18:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1][Stage 543:>                                                        (0 + 1) / 1]                                                                                [Stage 544:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 544:>                                                        (0 + 1) / 1][Stage 544:>                                                        (0 + 1) / 1][Stage 544:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:35:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 544:>                                                        (0 + 1) / 1][Stage 544:>                                                        (0 + 1) / 1][Stage 544:>                                                        (0 + 1) / 1][Stage 544:>                                                        (0 + 1) / 1][Stage 544:>                                                        (0 + 1) / 1][Stage 544:>                                                        (0 + 1) / 1][Stage 544:>                                                        (0 + 1) / 1][Stage 544:>                                                        (0 + 1) / 1][Stage 544:>                                                        (0 + 1) / 1][Stage 544:>                                                        (0 + 1) / 1][Stage 544:>                                                        (0 + 1) / 1]                                                                                [Stage 545:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 545:>                                                        (0 + 1) / 1][Stage 545:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:48:58] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 545:>                                                        (0 + 1) / 1][Stage 545:>                                                        (0 + 1) / 1][Stage 545:>                                                        (0 + 1) / 1][Stage 545:>                                                        (0 + 1) / 1][Stage 545:>                                                        (0 + 1) / 1][Stage 545:>                                                        (0 + 1) / 1][Stage 545:>                                                        (0 + 1) / 1][Stage 545:>                                                        (0 + 1) / 1][Stage 545:>                                                        (0 + 1) / 1]                                                                                [Stage 546:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 546:>                                                        (0 + 1) / 1][Stage 546:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:00:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 546:>                                                        (0 + 1) / 1][Stage 546:>                                                        (0 + 1) / 1][Stage 546:>                                                        (0 + 1) / 1][Stage 546:>                                                        (0 + 1) / 1][Stage 546:>                                                        (0 + 1) / 1][Stage 546:>                                                        (0 + 1) / 1][Stage 546:>                                                        (0 + 1) / 1][Stage 546:>                                                        (0 + 1) / 1]                                                                                [Stage 547:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 547:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:09:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 547:>                                                        (0 + 1) / 1][Stage 547:>                                                        (0 + 1) / 1][Stage 547:>                                                        (0 + 1) / 1][Stage 547:>                                                        (0 + 1) / 1]                                                                                [Stage 548:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:18:29] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1][Stage 548:>                                                        (0 + 1) / 1]                                                                                [Stage 549:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 549:>                                                        (0 + 1) / 1][Stage 549:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:34:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 549:>                                                        (0 + 1) / 1][Stage 549:>                                                        (0 + 1) / 1][Stage 549:>                                                        (0 + 1) / 1][Stage 549:>                                                        (0 + 1) / 1][Stage 549:>                                                        (0 + 1) / 1][Stage 549:>                                                        (0 + 1) / 1][Stage 549:>                                                        (0 + 1) / 1][Stage 549:>                                                        (0 + 1) / 1][Stage 549:>                                                        (0 + 1) / 1]                                                                                [Stage 550:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:48:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1][Stage 550:>                                                        (0 + 1) / 1]                                                                                [Stage 551:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:07:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1][Stage 551:>                                                        (0 + 1) / 1]                                                                                [Stage 552:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:28:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1][Stage 552:>                                                        (0 + 1) / 1]                                                                                [Stage 553:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:48:11] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1][Stage 553:>                                                        (0 + 1) / 1]                                                                                [Stage 554:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:06:36] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1][Stage 554:>                                                        (0 + 1) / 1]                                                                                [Stage 555:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 555:>                                                        (0 + 1) / 1][Stage 555:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:22:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 555:>                                                        (0 + 1) / 1][Stage 555:>                                                        (0 + 1) / 1][Stage 555:>                                                        (0 + 1) / 1][Stage 555:>                                                        (0 + 1) / 1][Stage 555:>                                                        (0 + 1) / 1][Stage 555:>                                                        (0 + 1) / 1][Stage 555:>                                                        (0 + 1) / 1][Stage 555:>                                                        (0 + 1) / 1]                                                                                [Stage 556:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:35:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1][Stage 556:>                                                        (0 + 1) / 1]                                                                                [Stage 557:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:58:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1][Stage 557:>                                                        (0 + 1) / 1]                                                                                [Stage 558:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:23:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1][Stage 558:>                                                        (0 + 1) / 1]                                                                                trial task 139 failed, exception is An error occurred while calling o12.broadcast.
: java.lang.OutOfMemoryError: Java heap space
	at java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)
	at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)
	at org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1(TorrentBroadcast.scala:360)
	at org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1$adapted(TorrentBroadcast.scala:360)
	at org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1148/0x000000084082a040.apply(Unknown Source)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)
	at net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)
	at java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1849)
	at java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:708)
	at org.apache.spark.util.Utils$.$anonfun$copyStream$1(Utils.scala:272)
	at org.apache.spark.util.Utils$$$Lambda$1154/0x000000084082f440.apply$mcJ$sp(Unknown Source)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:279)
	at org.apache.spark.api.python.PythonBroadcast.$anonfun$writeObject$1(PythonRDD.scala:772)
	at org.apache.spark.api.python.PythonBroadcast$$Lambda$1153/0x000000084082fc40.apply$mcJ$sp(Unknown Source)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)
	at org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)
	at org.apache.spark.api.python.PythonBroadcast.writeObject(PythonRDD.scala:768)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at java.base/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1016)
	at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1487)
	at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)
.
 None
Total Trials: 140: 139 succeeded, 1 failed, 0 cancelled.
[Stage 559:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 559:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:00:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 559:>                                                        (0 + 1) / 1][Stage 559:>                                                        (0 + 1) / 1][Stage 559:>                                                        (0 + 1) / 1][Stage 559:>                                                        (0 + 1) / 1][Stage 559:>                                                        (0 + 1) / 1][Stage 559:>                                                        (0 + 1) / 1]                                                                                [Stage 560:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 560:>                                                        (0 + 1) / 1][Stage 560:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:08:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 560:>                                                        (0 + 1) / 1][Stage 560:>                                                        (0 + 1) / 1][Stage 560:>                                                        (0 + 1) / 1][Stage 560:>                                                        (0 + 1) / 1][Stage 560:>                                                        (0 + 1) / 1][Stage 560:>                                                        (0 + 1) / 1]                                                                                [Stage 561:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:16:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 561:>                                                        (0 + 1) / 1][Stage 561:>                                                        (0 + 1) / 1][Stage 561:>                                                        (0 + 1) / 1]                                                                                [Stage 562:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 562:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:20:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 562:>                                                        (0 + 1) / 1][Stage 562:>                                                        (0 + 1) / 1][Stage 562:>                                                        (0 + 1) / 1][Stage 562:>                                                        (0 + 1) / 1][Stage 562:>                                                        (0 + 1) / 1]                                                                                [Stage 563:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:26:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 563:>                                                        (0 + 1) / 1][Stage 563:>                                                        (0 + 1) / 1]                                                                                [Stage 564:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:30:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 564:>                                                        (0 + 1) / 1][Stage 564:>                                                        (0 + 1) / 1][Stage 564:>                                                        (0 + 1) / 1]                                                                                [Stage 565:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 565:>                                                        (0 + 1) / 1][Stage 565:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:35:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 565:>                                                        (0 + 1) / 1][Stage 565:>                                                        (0 + 1) / 1][Stage 565:>                                                        (0 + 1) / 1][Stage 565:>                                                        (0 + 1) / 1][Stage 565:>                                                        (0 + 1) / 1][Stage 565:>                                                        (0 + 1) / 1][Stage 565:>                                                        (0 + 1) / 1][Stage 565:>                                                        (0 + 1) / 1]                                                                                [Stage 566:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 566:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 566:>                                                        (0 + 1) / 1][Stage 566:>                                                        (0 + 1) / 1][Stage 566:>                                                        (0 + 1) / 1][Stage 566:>                                                        (0 + 1) / 1][Stage 566:>                                                        (0 + 1) / 1][Stage 566:>                                                        (0 + 1) / 1]                                                                                [Stage 567:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 567:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:53:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 567:>                                                        (0 + 1) / 1][Stage 567:>                                                        (0 + 1) / 1][Stage 567:>                                                        (0 + 1) / 1]                                                                                [Stage 568:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:57:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 568:>                                                        (0 + 1) / 1][Stage 568:>                                                        (0 + 1) / 1][Stage 568:>                                                        (0 + 1) / 1]                                                                                [Stage 569:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 569:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:01:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 569:>                                                        (0 + 1) / 1][Stage 569:>                                                        (0 + 1) / 1][Stage 569:>                                                        (0 + 1) / 1]                                                                                [Stage 570:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 570:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:06:23] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 570:>                                                        (0 + 1) / 1][Stage 570:>                                                        (0 + 1) / 1][Stage 570:>                                                        (0 + 1) / 1]                                                                                [Stage 571:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 571:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:11:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 571:>                                                        (0 + 1) / 1][Stage 571:>                                                        (0 + 1) / 1][Stage 571:>                                                        (0 + 1) / 1][Stage 571:>                                                        (0 + 1) / 1][Stage 571:>                                                        (0 + 1) / 1]                                                                                [Stage 572:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:17:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 572:>                                                        (0 + 1) / 1][Stage 572:>                                                        (0 + 1) / 1]                                                                                [Stage 573:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 573:>                                                        (0 + 1) / 1][Stage 573:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:21:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 573:>                                                        (0 + 1) / 1][Stage 573:>                                                        (0 + 1) / 1][Stage 573:>                                                        (0 + 1) / 1][Stage 573:>                                                        (0 + 1) / 1][Stage 573:>                                                        (0 + 1) / 1][Stage 573:>                                                        (0 + 1) / 1][Stage 573:>                                                        (0 + 1) / 1]                                                                                [Stage 574:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:29:23] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 574:>                                                        (0 + 1) / 1][Stage 574:>                                                        (0 + 1) / 1]                                                                                [Stage 575:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:32:20] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 575:>                                                        (0 + 1) / 1][Stage 575:>                                                        (0 + 1) / 1][Stage 575:>                                                        (0 + 1) / 1][Stage 575:>                                                        (0 + 1) / 1]                                                                                [Stage 576:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 576:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:36:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 576:>                                                        (0 + 1) / 1][Stage 576:>                                                        (0 + 1) / 1][Stage 576:>                                                        (0 + 1) / 1][Stage 576:>                                                        (0 + 1) / 1]                                                                                [Stage 577:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 577:>                                                        (0 + 1) / 1][Stage 577:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:44:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 577:>                                                        (0 + 1) / 1][Stage 577:>                                                        (0 + 1) / 1][Stage 577:>                                                        (0 + 1) / 1][Stage 577:>                                                        (0 + 1) / 1][Stage 577:>                                                        (0 + 1) / 1][Stage 577:>                                                        (0 + 1) / 1][Stage 577:>                                                        (0 + 1) / 1][Stage 577:>                                                        (0 + 1) / 1][Stage 577:>                                                        (0 + 1) / 1][Stage 577:>                                                        (0 + 1) / 1][Stage 577:>                                                        (0 + 1) / 1]                                                                                [Stage 578:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 578:>                                                        (0 + 1) / 1][Stage 578:>                                                        (0 + 1) / 1][Stage 578:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:58:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 578:>                                                        (0 + 1) / 1][Stage 578:>                                                        (0 + 1) / 1][Stage 578:>                                                        (0 + 1) / 1][Stage 578:>                                                        (0 + 1) / 1][Stage 578:>                                                        (0 + 1) / 1][Stage 578:>                                                        (0 + 1) / 1][Stage 578:>                                                        (0 + 1) / 1][Stage 578:>                                                        (0 + 1) / 1][Stage 578:>                                                        (0 + 1) / 1][Stage 578:>                                                        (0 + 1) / 1][Stage 578:>                                                        (0 + 1) / 1][Stage 578:>                                                        (0 + 1) / 1]                                                                                [Stage 579:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 579:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:12:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 579:>                                                        (0 + 1) / 1][Stage 579:>                                                        (0 + 1) / 1][Stage 579:>                                                        (0 + 1) / 1][Stage 579:>                                                        (0 + 1) / 1][Stage 579:>                                                        (0 + 1) / 1][Stage 579:>                                                        (0 + 1) / 1]                                                                                [Stage 580:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 580:>                                                        (0 + 1) / 1][Stage 580:>                                                        (0 + 1) / 1][Stage 580:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:22:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 580:>                                                        (0 + 1) / 1][Stage 580:>                                                        (0 + 1) / 1][Stage 580:>                                                        (0 + 1) / 1][Stage 580:>                                                        (0 + 1) / 1][Stage 580:>                                                        (0 + 1) / 1][Stage 580:>                                                        (0 + 1) / 1][Stage 580:>                                                        (0 + 1) / 1][Stage 580:>                                                        (0 + 1) / 1][Stage 580:>                                                        (0 + 1) / 1][Stage 580:>                                                        (0 + 1) / 1][Stage 580:>                                                        (0 + 1) / 1]                                                                                [Stage 581:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 581:>                                                        (0 + 1) / 1][Stage 581:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:36:32] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 581:>                                                        (0 + 1) / 1][Stage 581:>                                                        (0 + 1) / 1][Stage 581:>                                                        (0 + 1) / 1][Stage 581:>                                                        (0 + 1) / 1][Stage 581:>                                                        (0 + 1) / 1][Stage 581:>                                                        (0 + 1) / 1][Stage 581:>                                                        (0 + 1) / 1][Stage 581:>                                                        (0 + 1) / 1][Stage 581:>                                                        (0 + 1) / 1][Stage 581:>                                                        (0 + 1) / 1]                                                                                [Stage 582:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:52:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1][Stage 582:>                                                        (0 + 1) / 1]                                                                                [Stage 583:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:18:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1][Stage 583:>                                                        (0 + 1) / 1]                                                                                [Stage 584:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 584:>                                                        (0 + 1) / 1][Stage 584:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:39:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 584:>                                                        (0 + 1) / 1][Stage 584:>                                                        (0 + 1) / 1][Stage 584:>                                                        (0 + 1) / 1][Stage 584:>                                                        (0 + 1) / 1][Stage 584:>                                                        (0 + 1) / 1][Stage 584:>                                                        (0 + 1) / 1][Stage 584:>                                                        (0 + 1) / 1][Stage 584:>                                                        (0 + 1) / 1][Stage 584:>                                                        (0 + 1) / 1]                                                                                [Stage 585:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:53:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1][Stage 585:>                                                        (0 + 1) / 1]                                                                                [Stage 586:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 586:>                                                        (0 + 1) / 1][Stage 586:>                                                        (0 + 1) / 1][Stage 586:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:13:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 586:>                                                        (0 + 1) / 1][Stage 586:>                                                        (0 + 1) / 1][Stage 586:>                                                        (0 + 1) / 1][Stage 586:>                                                        (0 + 1) / 1][Stage 586:>                                                        (0 + 1) / 1][Stage 586:>                                                        (0 + 1) / 1][Stage 586:>                                                        (0 + 1) / 1][Stage 586:>                                                        (0 + 1) / 1][Stage 586:>                                                        (0 + 1) / 1][Stage 586:>                                                        (0 + 1) / 1][Stage 586:>                                                        (0 + 1) / 1][Stage 586:>                                                        (0 + 1) / 1]                                                                                [Stage 587:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:28:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1][Stage 587:>                                                        (0 + 1) / 1]                                                                                [Stage 588:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:47:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1][Stage 588:>                                                        (0 + 1) / 1]                                                                                [Stage 589:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 589:>                                                        (0 + 1) / 1][Stage 589:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:09:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 589:>                                                        (0 + 1) / 1][Stage 589:>                                                        (0 + 1) / 1][Stage 589:>                                                        (0 + 1) / 1][Stage 589:>                                                        (0 + 1) / 1][Stage 589:>                                                        (0 + 1) / 1][Stage 589:>                                                        (0 + 1) / 1][Stage 589:>                                                        (0 + 1) / 1][Stage 589:>                                                        (0 + 1) / 1][Stage 589:>                                                        (0 + 1) / 1][Stage 589:>                                                        (0 + 1) / 1]                                                                                [Stage 590:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 590:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:20:42] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 590:>                                                        (0 + 1) / 1][Stage 590:>                                                        (0 + 1) / 1][Stage 590:>                                                        (0 + 1) / 1][Stage 590:>                                                        (0 + 1) / 1][Stage 590:>                                                        (0 + 1) / 1]                                                                                [Stage 591:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 591:>                                                        (0 + 1) / 1][Stage 591:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:28:36] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 591:>                                                        (0 + 1) / 1][Stage 591:>                                                        (0 + 1) / 1][Stage 591:>                                                        (0 + 1) / 1][Stage 591:>                                                        (0 + 1) / 1][Stage 591:>                                                        (0 + 1) / 1][Stage 591:>                                                        (0 + 1) / 1][Stage 591:>                                                        (0 + 1) / 1][Stage 591:>                                                        (0 + 1) / 1][Stage 591:>                                                        (0 + 1) / 1]                                                                                [Stage 592:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 592:>                                                        (0 + 1) / 1][Stage 592:>                                                        (0 + 1) / 1][Stage 592:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:41:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 592:>                                                        (0 + 1) / 1][Stage 592:>                                                        (0 + 1) / 1][Stage 592:>                                                        (0 + 1) / 1][Stage 592:>                                                        (0 + 1) / 1][Stage 592:>                                                        (0 + 1) / 1][Stage 592:>                                                        (0 + 1) / 1][Stage 592:>                                                        (0 + 1) / 1][Stage 592:>                                                        (0 + 1) / 1][Stage 592:>                                                        (0 + 1) / 1][Stage 592:>                                                        (0 + 1) / 1][Stage 592:>                                                        (0 + 1) / 1][Stage 592:>                                                        (0 + 1) / 1]                                                                                [Stage 593:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 593:>                                                        (0 + 1) / 1][Stage 593:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:56:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 593:>                                                        (0 + 1) / 1][Stage 593:>                                                        (0 + 1) / 1][Stage 593:>                                                        (0 + 1) / 1][Stage 593:>                                                        (0 + 1) / 1][Stage 593:>                                                        (0 + 1) / 1][Stage 593:>                                                        (0 + 1) / 1][Stage 593:>                                                        (0 + 1) / 1][Stage 593:>                                                        (0 + 1) / 1][Stage 593:>                                                        (0 + 1) / 1]                                                                                [Stage 594:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:34] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1][Stage 594:>                                                        (0 + 1) / 1]                                                                                [Stage 595:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:25:42] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1][Stage 595:>                                                        (0 + 1) / 1]                                                                                [Stage 596:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:42:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1][Stage 596:>                                                        (0 + 1) / 1]                                                                                [Stage 597:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 597:>                                                        (0 + 1) / 1][Stage 597:>                                                        (0 + 1) / 1][Stage 597:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:59:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 597:>                                                        (0 + 1) / 1][Stage 597:>                                                        (0 + 1) / 1][Stage 597:>                                                        (0 + 1) / 1][Stage 597:>                                                        (0 + 1) / 1][Stage 597:>                                                        (0 + 1) / 1][Stage 597:>                                                        (0 + 1) / 1][Stage 597:>                                                        (0 + 1) / 1][Stage 597:>                                                        (0 + 1) / 1][Stage 597:>                                                        (0 + 1) / 1][Stage 597:>                                                        (0 + 1) / 1][Stage 597:>                                                        (0 + 1) / 1][Stage 597:>                                                        (0 + 1) / 1]                                                                                [Stage 598:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 598:>                                                        (0 + 1) / 1][Stage 598:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:14:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 598:>                                                        (0 + 1) / 1][Stage 598:>                                                        (0 + 1) / 1][Stage 598:>                                                        (0 + 1) / 1][Stage 598:>                                                        (0 + 1) / 1][Stage 598:>                                                        (0 + 1) / 1][Stage 598:>                                                        (0 + 1) / 1][Stage 598:>                                                        (0 + 1) / 1][Stage 598:>                                                        (0 + 1) / 1]                                                                                [Stage 599:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:26:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1][Stage 599:>                                                        (0 + 1) / 1]                                                                                [Stage 600:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:44:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1][Stage 600:>                                                        (0 + 1) / 1]                                                                                [Stage 601:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:03:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1][Stage 601:>                                                        (0 + 1) / 1]                                                                                [Stage 602:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 602:>                                                        (0 + 1) / 1][Stage 602:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:21:48] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 602:>                                                        (0 + 1) / 1][Stage 602:>                                                        (0 + 1) / 1][Stage 602:>                                                        (0 + 1) / 1][Stage 602:>                                                        (0 + 1) / 1][Stage 602:>                                                        (0 + 1) / 1][Stage 602:>                                                        (0 + 1) / 1][Stage 602:>                                                        (0 + 1) / 1][Stage 602:>                                                        (0 + 1) / 1][Stage 602:>                                                        (0 + 1) / 1][Stage 602:>                                                        (0 + 1) / 1][Stage 602:>                                                        (0 + 1) / 1]                                                                                [Stage 603:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:36:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1][Stage 603:>                                                        (0 + 1) / 1]                                                                                [Stage 604:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:51:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 604:>                                                        (0 + 1) / 1][Stage 604:>                                                        (0 + 1) / 1]                                                                                [Stage 605:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:57:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1][Stage 605:>                                                        (0 + 1) / 1]                                                                                [Stage 606:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 606:>                                                        (0 + 1) / 1][Stage 606:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:15:22] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 606:>                                                        (0 + 1) / 1][Stage 606:>                                                        (0 + 1) / 1][Stage 606:>                                                        (0 + 1) / 1][Stage 606:>                                                        (0 + 1) / 1][Stage 606:>                                                        (0 + 1) / 1][Stage 606:>                                                        (0 + 1) / 1]                                                                                [Stage 607:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 607:>                                                        (0 + 1) / 1][Stage 607:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:24:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 607:>                                                        (0 + 1) / 1][Stage 607:>                                                        (0 + 1) / 1][Stage 607:>                                                        (0 + 1) / 1][Stage 607:>                                                        (0 + 1) / 1][Stage 607:>                                                        (0 + 1) / 1][Stage 607:>                                                        (0 + 1) / 1][Stage 607:>                                                        (0 + 1) / 1]                                                                                [Stage 608:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 608:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:34:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 608:>                                                        (0 + 1) / 1][Stage 608:>                                                        (0 + 1) / 1][Stage 608:>                                                        (0 + 1) / 1][Stage 608:>                                                        (0 + 1) / 1][Stage 608:>                                                        (0 + 1) / 1][Stage 608:>                                                        (0 + 1) / 1][Stage 608:>                                                        (0 + 1) / 1]                                                                                [Stage 609:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 609:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:42:48] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 609:>                                                        (0 + 1) / 1][Stage 609:>                                                        (0 + 1) / 1][Stage 609:>                                                        (0 + 1) / 1][Stage 609:>                                                        (0 + 1) / 1][Stage 609:>                                                        (0 + 1) / 1][Stage 609:>                                                        (0 + 1) / 1]                                                                                [Stage 610:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 610:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:50:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 610:>                                                        (0 + 1) / 1][Stage 610:>                                                        (0 + 1) / 1][Stage 610:>                                                        (0 + 1) / 1][Stage 610:>                                                        (0 + 1) / 1][Stage 610:>                                                        (0 + 1) / 1][Stage 610:>                                                        (0 + 1) / 1]                                                                                [Stage 611:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 611:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [18:57:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 611:>                                                        (0 + 1) / 1][Stage 611:>                                                        (0 + 1) / 1][Stage 611:>                                                        (0 + 1) / 1][Stage 611:>                                                        (0 + 1) / 1]                                                                                [Stage 612:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 612:>                                                        (0 + 1) / 1][Stage 612:>                                                        (0 + 1) / 1][Stage 612:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:05:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 612:>                                                        (0 + 1) / 1][Stage 612:>                                                        (0 + 1) / 1][Stage 612:>                                                        (0 + 1) / 1][Stage 612:>                                                        (0 + 1) / 1][Stage 612:>                                                        (0 + 1) / 1][Stage 612:>                                                        (0 + 1) / 1][Stage 612:>                                                        (0 + 1) / 1][Stage 612:>                                                        (0 + 1) / 1][Stage 612:>                                                        (0 + 1) / 1][Stage 612:>                                                        (0 + 1) / 1][Stage 612:>                                                        (0 + 1) / 1]                                                                                [Stage 613:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 613:>                                                        (0 + 1) / 1][Stage 613:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:19:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 613:>                                                        (0 + 1) / 1][Stage 613:>                                                        (0 + 1) / 1][Stage 613:>                                                        (0 + 1) / 1][Stage 613:>                                                        (0 + 1) / 1][Stage 613:>                                                        (0 + 1) / 1][Stage 613:>                                                        (0 + 1) / 1][Stage 613:>                                                        (0 + 1) / 1][Stage 613:>                                                        (0 + 1) / 1][Stage 613:>                                                        (0 + 1) / 1][Stage 613:>                                                        (0 + 1) / 1][Stage 613:>                                                        (0 + 1) / 1]                                                                                [Stage 614:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:34:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1][Stage 614:>                                                        (0 + 1) / 1]                                                                                [Stage 615:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:51:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1][Stage 615:>                                                        (0 + 1) / 1]                                                                                [Stage 616:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:09:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1][Stage 616:>                                                        (0 + 1) / 1]                                                                                [Stage 617:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 617:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:27:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 617:>                                                        (0 + 1) / 1][Stage 617:>                                                        (0 + 1) / 1][Stage 617:>                                                        (0 + 1) / 1][Stage 617:>                                                        (0 + 1) / 1][Stage 617:>                                                        (0 + 1) / 1]                                                                                [Stage 618:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:33:31] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 618:>                                                        (0 + 1) / 1][Stage 618:>                                                        (0 + 1) / 1][Stage 618:>                                                        (0 + 1) / 1]                                                                                [Stage 619:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 619:>                                                        (0 + 1) / 1][Stage 619:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:39:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 619:>                                                        (0 + 1) / 1][Stage 619:>                                                        (0 + 1) / 1][Stage 619:>                                                        (0 + 1) / 1][Stage 619:>                                                        (0 + 1) / 1][Stage 619:>                                                        (0 + 1) / 1][Stage 619:>                                                        (0 + 1) / 1][Stage 619:>                                                        (0 + 1) / 1][Stage 619:>                                                        (0 + 1) / 1][Stage 619:>                                                        (0 + 1) / 1]                                                                                [Stage 620:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 620:>                                                        (0 + 1) / 1][Stage 620:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:50:31] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 620:>                                                        (0 + 1) / 1][Stage 620:>                                                        (0 + 1) / 1][Stage 620:>                                                        (0 + 1) / 1][Stage 620:>                                                        (0 + 1) / 1][Stage 620:>                                                        (0 + 1) / 1][Stage 620:>                                                        (0 + 1) / 1]                                                                                [Stage 621:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 621:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:59:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 621:>                                                        (0 + 1) / 1][Stage 621:>                                                        (0 + 1) / 1][Stage 621:>                                                        (0 + 1) / 1][Stage 621:>                                                        (0 + 1) / 1][Stage 621:>                                                        (0 + 1) / 1]                                                                                [Stage 622:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:09:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1][Stage 622:>                                                        (0 + 1) / 1]                                                                                [Stage 623:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 623:>                                                        (0 + 1) / 1][Stage 623:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:35:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 623:>                                                        (0 + 1) / 1][Stage 623:>                                                        (0 + 1) / 1][Stage 623:>                                                        (0 + 1) / 1][Stage 623:>                                                        (0 + 1) / 1][Stage 623:>                                                        (0 + 1) / 1][Stage 623:>                                                        (0 + 1) / 1][Stage 623:>                                                        (0 + 1) / 1][Stage 623:>                                                        (0 + 1) / 1][Stage 623:>                                                        (0 + 1) / 1][Stage 623:>                                                        (0 + 1) / 1]                                                                                [Stage 624:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 624:>                                                        (0 + 1) / 1][Stage 624:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:48:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 624:>                                                        (0 + 1) / 1][Stage 624:>                                                        (0 + 1) / 1][Stage 624:>                                                        (0 + 1) / 1][Stage 624:>                                                        (0 + 1) / 1][Stage 624:>                                                        (0 + 1) / 1][Stage 624:>                                                        (0 + 1) / 1][Stage 624:>                                                        (0 + 1) / 1][Stage 624:>                                                        (0 + 1) / 1][Stage 624:>                                                        (0 + 1) / 1][Stage 624:>                                                        (0 + 1) / 1]                                                                                [Stage 625:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 625:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:59:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 625:>                                                        (0 + 1) / 1][Stage 625:>                                                        (0 + 1) / 1][Stage 625:>                                                        (0 + 1) / 1][Stage 625:>                                                        (0 + 1) / 1][Stage 625:>                                                        (0 + 1) / 1][Stage 625:>                                                        (0 + 1) / 1][Stage 625:>                                                        (0 + 1) / 1]                                                                                [Stage 626:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:10:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1][Stage 626:>                                                        (0 + 1) / 1]                                                                                [Stage 627:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 627:>                                                        (0 + 1) / 1][Stage 627:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:25:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 627:>                                                        (0 + 1) / 1][Stage 627:>                                                        (0 + 1) / 1][Stage 627:>                                                        (0 + 1) / 1][Stage 627:>                                                        (0 + 1) / 1][Stage 627:>                                                        (0 + 1) / 1][Stage 627:>                                                        (0 + 1) / 1][Stage 627:>                                                        (0 + 1) / 1][Stage 627:>                                                        (0 + 1) / 1][Stage 627:>                                                        (0 + 1) / 1][Stage 627:>                                                        (0 + 1) / 1][Stage 627:>                                                        (0 + 1) / 1]                                                                                [Stage 628:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 628:>                                                        (0 + 1) / 1][Stage 628:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:39:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 628:>                                                        (0 + 1) / 1][Stage 628:>                                                        (0 + 1) / 1][Stage 628:>                                                        (0 + 1) / 1][Stage 628:>                                                        (0 + 1) / 1][Stage 628:>                                                        (0 + 1) / 1][Stage 628:>                                                        (0 + 1) / 1][Stage 628:>                                                        (0 + 1) / 1][Stage 628:>                                                        (0 + 1) / 1][Stage 628:>                                                        (0 + 1) / 1][Stage 628:>                                                        (0 + 1) / 1]                                                                                [Stage 629:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 629:>                                                        (0 + 1) / 1][Stage 629:>                                                        (0 + 1) / 1][Stage 629:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:53:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 629:>                                                        (0 + 1) / 1][Stage 629:>                                                        (0 + 1) / 1][Stage 629:>                                                        (0 + 1) / 1][Stage 629:>                                                        (0 + 1) / 1][Stage 629:>                                                        (0 + 1) / 1][Stage 629:>                                                        (0 + 1) / 1][Stage 629:>                                                        (0 + 1) / 1][Stage 629:>                                                        (0 + 1) / 1][Stage 629:>                                                        (0 + 1) / 1][Stage 629:>                                                        (0 + 1) / 1][Stage 629:>                                                        (0 + 1) / 1][Stage 629:>                                                        (0 + 1) / 1]                                                                                [Stage 630:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:09:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1][Stage 630:>                                                        (0 + 1) / 1]                                                                                [Stage 631:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 631:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:25:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 631:>                                                        (0 + 1) / 1][Stage 631:>                                                        (0 + 1) / 1][Stage 631:>                                                        (0 + 1) / 1][Stage 631:>                                                        (0 + 1) / 1][Stage 631:>                                                        (0 + 1) / 1]                                                                                [Stage 632:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 632:>                                                        (0 + 1) / 1][Stage 632:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:33:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 632:>                                                        (0 + 1) / 1][Stage 632:>                                                        (0 + 1) / 1][Stage 632:>                                                        (0 + 1) / 1][Stage 632:>                                                        (0 + 1) / 1][Stage 632:>                                                        (0 + 1) / 1][Stage 632:>                                                        (0 + 1) / 1][Stage 632:>                                                        (0 + 1) / 1]                                                                                [Stage 633:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 633:>                                                        (0 + 1) / 1][Stage 633:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:43:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 633:>                                                        (0 + 1) / 1][Stage 633:>                                                        (0 + 1) / 1][Stage 633:>                                                        (0 + 1) / 1][Stage 633:>                                                        (0 + 1) / 1][Stage 633:>                                                        (0 + 1) / 1][Stage 633:>                                                        (0 + 1) / 1][Stage 633:>                                                        (0 + 1) / 1][Stage 633:>                                                        (0 + 1) / 1]                                                                                [Stage 634:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:56:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1][Stage 634:>                                                        (0 + 1) / 1]                                                                                [Stage 635:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 635:>                                                        (0 + 1) / 1][Stage 635:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:17:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 635:>                                                        (0 + 1) / 1][Stage 635:>                                                        (0 + 1) / 1][Stage 635:>                                                        (0 + 1) / 1][Stage 635:>                                                        (0 + 1) / 1][Stage 635:>                                                        (0 + 1) / 1][Stage 635:>                                                        (0 + 1) / 1][Stage 635:>                                                        (0 + 1) / 1][Stage 635:>                                                        (0 + 1) / 1][Stage 635:>                                                        (0 + 1) / 1]                                                                                [Stage 636:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:27:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 636:>                                                        (0 + 1) / 1][Stage 636:>                                                        (0 + 1) / 1][Stage 636:>                                                        (0 + 1) / 1]                                                                                [Stage 637:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:34:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1][Stage 637:>                                                        (0 + 1) / 1]                                                                                [Stage 638:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [00:55:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1][Stage 638:>                                                        (0 + 1) / 1]                                                                                [Stage 639:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:20:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1][Stage 639:>                                                        (0 + 1) / 1]                                                                                [Stage 640:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [01:44:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1][Stage 640:>                                                        (0 + 1) / 1]                                                                                [Stage 641:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:07:22] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1][Stage 641:>                                                        (0 + 1) / 1]                                                                                [Stage 642:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 642:>                                                        (0 + 1) / 1][Stage 642:>                                                        (0 + 1) / 1][Stage 642:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:24:37] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 642:>                                                        (0 + 1) / 1][Stage 642:>                                                        (0 + 1) / 1][Stage 642:>                                                        (0 + 1) / 1][Stage 642:>                                                        (0 + 1) / 1][Stage 642:>                                                        (0 + 1) / 1][Stage 642:>                                                        (0 + 1) / 1][Stage 642:>                                                        (0 + 1) / 1][Stage 642:>                                                        (0 + 1) / 1][Stage 642:>                                                        (0 + 1) / 1][Stage 642:>                                                        (0 + 1) / 1][Stage 642:>                                                        (0 + 1) / 1][Stage 642:>                                                        (0 + 1) / 1]                                                                                [Stage 643:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:39:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1][Stage 643:>                                                        (0 + 1) / 1]                                                                                [Stage 644:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 644:>                                                        (0 + 1) / 1][Stage 644:>                                                        (0 + 1) / 1][Stage 644:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:57:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 644:>                                                        (0 + 1) / 1][Stage 644:>                                                        (0 + 1) / 1][Stage 644:>                                                        (0 + 1) / 1][Stage 644:>                                                        (0 + 1) / 1][Stage 644:>                                                        (0 + 1) / 1][Stage 644:>                                                        (0 + 1) / 1][Stage 644:>                                                        (0 + 1) / 1][Stage 644:>                                                        (0 + 1) / 1][Stage 644:>                                                        (0 + 1) / 1][Stage 644:>                                                        (0 + 1) / 1][Stage 644:>                                                        (0 + 1) / 1]                                                                                [Stage 645:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 645:>                                                        (0 + 1) / 1][Stage 645:>                                                        (0 + 1) / 1][Stage 645:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:12:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 645:>                                                        (0 + 1) / 1][Stage 645:>                                                        (0 + 1) / 1][Stage 645:>                                                        (0 + 1) / 1][Stage 645:>                                                        (0 + 1) / 1][Stage 645:>                                                        (0 + 1) / 1][Stage 645:>                                                        (0 + 1) / 1][Stage 645:>                                                        (0 + 1) / 1][Stage 645:>                                                        (0 + 1) / 1][Stage 645:>                                                        (0 + 1) / 1][Stage 645:>                                                        (0 + 1) / 1][Stage 645:>                                                        (0 + 1) / 1][Stage 645:>                                                        (0 + 1) / 1]                                                                                [Stage 646:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 646:>                                                        (0 + 1) / 1][Stage 646:>                                                        (0 + 1) / 1][Stage 646:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:27:44] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 646:>                                                        (0 + 1) / 1][Stage 646:>                                                        (0 + 1) / 1][Stage 646:>                                                        (0 + 1) / 1][Stage 646:>                                                        (0 + 1) / 1][Stage 646:>                                                        (0 + 1) / 1][Stage 646:>                                                        (0 + 1) / 1][Stage 646:>                                                        (0 + 1) / 1][Stage 646:>                                                        (0 + 1) / 1][Stage 646:>                                                        (0 + 1) / 1][Stage 646:>                                                        (0 + 1) / 1][Stage 646:>                                                        (0 + 1) / 1][Stage 646:>                                                        (0 + 1) / 1]                                                                                [Stage 647:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:43:31] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1][Stage 647:>                                                        (0 + 1) / 1]                                                                                [Stage 648:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:02:32] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1][Stage 648:>                                                        (0 + 1) / 1]                                                                                [Stage 649:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 649:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:19:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 649:>                                                        (0 + 1) / 1][Stage 649:>                                                        (0 + 1) / 1][Stage 649:>                                                        (0 + 1) / 1][Stage 649:>                                                        (0 + 1) / 1][Stage 649:>                                                        (0 + 1) / 1][Stage 649:>                                                        (0 + 1) / 1][Stage 649:>                                                        (0 + 1) / 1]                                                                                [Stage 650:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 650:>                                                        (0 + 1) / 1][Stage 650:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:27:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 650:>                                                        (0 + 1) / 1][Stage 650:>                                                        (0 + 1) / 1][Stage 650:>                                                        (0 + 1) / 1][Stage 650:>                                                        (0 + 1) / 1][Stage 650:>                                                        (0 + 1) / 1][Stage 650:>                                                        (0 + 1) / 1][Stage 650:>                                                        (0 + 1) / 1][Stage 650:>                                                        (0 + 1) / 1]                                                                                [Stage 651:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:37:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 651:>                                                        (0 + 1) / 1]                                                                                [Stage 652:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 652:>                                                        (0 + 1) / 1][Stage 652:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:40:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 652:>                                                        (0 + 1) / 1][Stage 652:>                                                        (0 + 1) / 1][Stage 652:>                                                        (0 + 1) / 1][Stage 652:>                                                        (0 + 1) / 1][Stage 652:>                                                        (0 + 1) / 1][Stage 652:>                                                        (0 + 1) / 1][Stage 652:>                                                        (0 + 1) / 1]                                                                                [Stage 653:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 653:>                                                        (0 + 1) / 1][Stage 653:>                                                        (0 + 1) / 1][Stage 653:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [04:51:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 653:>                                                        (0 + 1) / 1][Stage 653:>                                                        (0 + 1) / 1][Stage 653:>                                                        (0 + 1) / 1][Stage 653:>                                                        (0 + 1) / 1][Stage 653:>                                                        (0 + 1) / 1][Stage 653:>                                                        (0 + 1) / 1][Stage 653:>                                                        (0 + 1) / 1][Stage 653:>                                                        (0 + 1) / 1][Stage 653:>                                                        (0 + 1) / 1][Stage 653:>                                                        (0 + 1) / 1][Stage 653:>                                                        (0 + 1) / 1][Stage 653:>                                                        (0 + 1) / 1]                                                                                [Stage 654:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 654:>                                                        (0 + 1) / 1][Stage 654:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:06:36] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 654:>                                                        (0 + 1) / 1][Stage 654:>                                                        (0 + 1) / 1][Stage 654:>                                                        (0 + 1) / 1][Stage 654:>                                                        (0 + 1) / 1][Stage 654:>                                                        (0 + 1) / 1][Stage 654:>                                                        (0 + 1) / 1][Stage 654:>                                                        (0 + 1) / 1][Stage 654:>                                                        (0 + 1) / 1][Stage 654:>                                                        (0 + 1) / 1]                                                                                [Stage 655:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:19:22] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1][Stage 655:>                                                        (0 + 1) / 1]                                                                                [Stage 656:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 656:>                                                        (0 + 1) / 1][Stage 656:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:35:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 656:>                                                        (0 + 1) / 1][Stage 656:>                                                        (0 + 1) / 1][Stage 656:>                                                        (0 + 1) / 1][Stage 656:>                                                        (0 + 1) / 1][Stage 656:>                                                        (0 + 1) / 1][Stage 656:>                                                        (0 + 1) / 1][Stage 656:>                                                        (0 + 1) / 1][Stage 656:>                                                        (0 + 1) / 1][Stage 656:>                                                        (0 + 1) / 1]                                                                                [Stage 657:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [05:47:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1][Stage 657:>                                                        (0 + 1) / 1]                                                                                [Stage 658:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 658:>                                                        (0 + 1) / 1][Stage 658:>                                                        (0 + 1) / 1][Stage 658:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:04:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 658:>                                                        (0 + 1) / 1][Stage 658:>                                                        (0 + 1) / 1][Stage 658:>                                                        (0 + 1) / 1][Stage 658:>                                                        (0 + 1) / 1][Stage 658:>                                                        (0 + 1) / 1][Stage 658:>                                                        (0 + 1) / 1][Stage 658:>                                                        (0 + 1) / 1][Stage 658:>                                                        (0 + 1) / 1][Stage 658:>                                                        (0 + 1) / 1][Stage 658:>                                                        (0 + 1) / 1][Stage 658:>                                                        (0 + 1) / 1][Stage 658:>                                                        (0 + 1) / 1]                                                                                [Stage 659:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 659:>                                                        (0 + 1) / 1][Stage 659:>                                                        (0 + 1) / 1][Stage 659:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:19:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 659:>                                                        (0 + 1) / 1][Stage 659:>                                                        (0 + 1) / 1][Stage 659:>                                                        (0 + 1) / 1][Stage 659:>                                                        (0 + 1) / 1][Stage 659:>                                                        (0 + 1) / 1][Stage 659:>                                                        (0 + 1) / 1][Stage 659:>                                                        (0 + 1) / 1][Stage 659:>                                                        (0 + 1) / 1][Stage 659:>                                                        (0 + 1) / 1][Stage 659:>                                                        (0 + 1) / 1]                                                                                [Stage 660:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 660:>                                                        (0 + 1) / 1][Stage 660:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:32:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 660:>                                                        (0 + 1) / 1][Stage 660:>                                                        (0 + 1) / 1][Stage 660:>                                                        (0 + 1) / 1][Stage 660:>                                                        (0 + 1) / 1][Stage 660:>                                                        (0 + 1) / 1][Stage 660:>                                                        (0 + 1) / 1][Stage 660:>                                                        (0 + 1) / 1][Stage 660:>                                                        (0 + 1) / 1][Stage 660:>                                                        (0 + 1) / 1]                                                                                [Stage 661:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 661:>                                                        (0 + 1) / 1][Stage 661:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:44:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 661:>                                                        (0 + 1) / 1][Stage 661:>                                                        (0 + 1) / 1][Stage 661:>                                                        (0 + 1) / 1][Stage 661:>                                                        (0 + 1) / 1][Stage 661:>                                                        (0 + 1) / 1][Stage 661:>                                                        (0 + 1) / 1][Stage 661:>                                                        (0 + 1) / 1][Stage 661:>                                                        (0 + 1) / 1][Stage 661:>                                                        (0 + 1) / 1]                                                                                [Stage 662:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 662:>                                                        (0 + 1) / 1][Stage 662:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [06:56:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 662:>                                                        (0 + 1) / 1][Stage 662:>                                                        (0 + 1) / 1][Stage 662:>                                                        (0 + 1) / 1][Stage 662:>                                                        (0 + 1) / 1][Stage 662:>                                                        (0 + 1) / 1][Stage 662:>                                                        (0 + 1) / 1][Stage 662:>                                                        (0 + 1) / 1][Stage 662:>                                                        (0 + 1) / 1][Stage 662:>                                                        (0 + 1) / 1]                                                                                [Stage 663:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:08:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1][Stage 663:>                                                        (0 + 1) / 1]                                                                                [Stage 664:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 664:>                                                        (0 + 1) / 1][Stage 664:>                                                        (0 + 1) / 1][Stage 664:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:25:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 664:>                                                        (0 + 1) / 1][Stage 664:>                                                        (0 + 1) / 1][Stage 664:>                                                        (0 + 1) / 1][Stage 664:>                                                        (0 + 1) / 1][Stage 664:>                                                        (0 + 1) / 1][Stage 664:>                                                        (0 + 1) / 1][Stage 664:>                                                        (0 + 1) / 1][Stage 664:>                                                        (0 + 1) / 1]                                                                                [Stage 665:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:37:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1][Stage 665:>                                                        (0 + 1) / 1]                                                                                [Stage 666:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [07:56:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1][Stage 666:>                                                        (0 + 1) / 1]                                                                                [Stage 667:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 667:>                                                        (0 + 1) / 1][Stage 667:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:19:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 667:>                                                        (0 + 1) / 1][Stage 667:>                                                        (0 + 1) / 1][Stage 667:>                                                        (0 + 1) / 1][Stage 667:>                                                        (0 + 1) / 1][Stage 667:>                                                        (0 + 1) / 1][Stage 667:>                                                        (0 + 1) / 1][Stage 667:>                                                        (0 + 1) / 1][Stage 667:>                                                        (0 + 1) / 1][Stage 667:>                                                        (0 + 1) / 1][Stage 667:>                                                        (0 + 1) / 1]                                                                                [Stage 668:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 668:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:31:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 668:>                                                        (0 + 1) / 1][Stage 668:>                                                        (0 + 1) / 1][Stage 668:>                                                        (0 + 1) / 1]                                                                                [Stage 669:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:38:20] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1][Stage 669:>                                                        (0 + 1) / 1]                                                                                [Stage 670:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 670:>                                                        (0 + 1) / 1][Stage 670:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [08:53:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 670:>                                                        (0 + 1) / 1][Stage 670:>                                                        (0 + 1) / 1][Stage 670:>                                                        (0 + 1) / 1][Stage 670:>                                                        (0 + 1) / 1][Stage 670:>                                                        (0 + 1) / 1][Stage 670:>                                                        (0 + 1) / 1][Stage 670:>                                                        (0 + 1) / 1]                                                                                [Stage 671:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:05:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1][Stage 671:>                                                        (0 + 1) / 1]                                                                                [Stage 672:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 672:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:22:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 672:>                                                        (0 + 1) / 1][Stage 672:>                                                        (0 + 1) / 1][Stage 672:>                                                        (0 + 1) / 1][Stage 672:>                                                        (0 + 1) / 1][Stage 672:>                                                        (0 + 1) / 1][Stage 672:>                                                        (0 + 1) / 1]                                                                                [Stage 673:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 673:>                                                        (0 + 1) / 1][Stage 673:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:30:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 673:>                                                        (0 + 1) / 1][Stage 673:>                                                        (0 + 1) / 1][Stage 673:>                                                        (0 + 1) / 1][Stage 673:>                                                        (0 + 1) / 1][Stage 673:>                                                        (0 + 1) / 1][Stage 673:>                                                        (0 + 1) / 1][Stage 673:>                                                        (0 + 1) / 1]                                                                                [Stage 674:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 674:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:39:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 674:>                                                        (0 + 1) / 1][Stage 674:>                                                        (0 + 1) / 1][Stage 674:>                                                        (0 + 1) / 1][Stage 674:>                                                        (0 + 1) / 1]                                                                                [Stage 675:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 675:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:45:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 675:>                                                        (0 + 1) / 1][Stage 675:>                                                        (0 + 1) / 1][Stage 675:>                                                        (0 + 1) / 1]                                                                                [Stage 676:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 676:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:49:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 676:>                                                        (0 + 1) / 1][Stage 676:>                                                        (0 + 1) / 1][Stage 676:>                                                        (0 + 1) / 1]                                                                                [Stage 677:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 677:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:55:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 677:>                                                        (0 + 1) / 1][Stage 677:>                                                        (0 + 1) / 1][Stage 677:>                                                        (0 + 1) / 1][Stage 677:>                                                        (0 + 1) / 1][Stage 677:>                                                        (0 + 1) / 1][Stage 677:>                                                        (0 + 1) / 1][Stage 677:>                                                        (0 + 1) / 1]                                                                                [Stage 678:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 678:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:04:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 678:>                                                        (0 + 1) / 1][Stage 678:>                                                        (0 + 1) / 1][Stage 678:>                                                        (0 + 1) / 1][Stage 678:>                                                        (0 + 1) / 1][Stage 678:>                                                        (0 + 1) / 1]                                                                                [Stage 679:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 679:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:11:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 679:>                                                        (0 + 1) / 1][Stage 679:>                                                        (0 + 1) / 1][Stage 679:>                                                        (0 + 1) / 1][Stage 679:>                                                        (0 + 1) / 1][Stage 679:>                                                        (0 + 1) / 1]                                                                                [Stage 680:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 680:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:18:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 680:>                                                        (0 + 1) / 1][Stage 680:>                                                        (0 + 1) / 1][Stage 680:>                                                        (0 + 1) / 1][Stage 680:>                                                        (0 + 1) / 1]                                                                                [Stage 681:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 681:>                                                        (0 + 1) / 1][Stage 681:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:25:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 681:>                                                        (0 + 1) / 1][Stage 681:>                                                        (0 + 1) / 1][Stage 681:>                                                        (0 + 1) / 1][Stage 681:>                                                        (0 + 1) / 1][Stage 681:>                                                        (0 + 1) / 1][Stage 681:>                                                        (0 + 1) / 1][Stage 681:>                                                        (0 + 1) / 1][Stage 681:>                                                        (0 + 1) / 1]                                                                                [Stage 682:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 682:>                                                        (0 + 1) / 1][Stage 682:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:36:11] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 682:>                                                        (0 + 1) / 1][Stage 682:>                                                        (0 + 1) / 1][Stage 682:>                                                        (0 + 1) / 1][Stage 682:>                                                        (0 + 1) / 1][Stage 682:>                                                        (0 + 1) / 1][Stage 682:>                                                        (0 + 1) / 1][Stage 682:>                                                        (0 + 1) / 1]                                                                                [Stage 683:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 683:>                                                        (0 + 1) / 1][Stage 683:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:46:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 683:>                                                        (0 + 1) / 1][Stage 683:>                                                        (0 + 1) / 1][Stage 683:>                                                        (0 + 1) / 1][Stage 683:>                                                        (0 + 1) / 1][Stage 683:>                                                        (0 + 1) / 1][Stage 683:>                                                        (0 + 1) / 1][Stage 683:>                                                        (0 + 1) / 1][Stage 683:>                                                        (0 + 1) / 1]                                                                                [Stage 684:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:58:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1][Stage 684:>                                                        (0 + 1) / 1]                                                                                [Stage 685:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 685:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:13:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 685:>                                                        (0 + 1) / 1][Stage 685:>                                                        (0 + 1) / 1][Stage 685:>                                                        (0 + 1) / 1][Stage 685:>                                                        (0 + 1) / 1]                                                                                [Stage 686:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 686:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:19:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 686:>                                                        (0 + 1) / 1][Stage 686:>                                                        (0 + 1) / 1][Stage 686:>                                                        (0 + 1) / 1][Stage 686:>                                                        (0 + 1) / 1][Stage 686:>                                                        (0 + 1) / 1][Stage 686:>                                                        (0 + 1) / 1][Stage 686:>                                                        (0 + 1) / 1]                                                                                [Stage 687:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 687:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:28:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 687:>                                                        (0 + 1) / 1][Stage 687:>                                                        (0 + 1) / 1][Stage 687:>                                                        (0 + 1) / 1][Stage 687:>                                                        (0 + 1) / 1][Stage 687:>                                                        (0 + 1) / 1]                                                                                [Stage 688:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 688:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:35:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 688:>                                                        (0 + 1) / 1][Stage 688:>                                                        (0 + 1) / 1][Stage 688:>                                                        (0 + 1) / 1][Stage 688:>                                                        (0 + 1) / 1][Stage 688:>                                                        (0 + 1) / 1][Stage 688:>                                                        (0 + 1) / 1][Stage 688:>                                                        (0 + 1) / 1]                                                                                [Stage 689:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:42:58] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 689:>                                                        (0 + 1) / 1][Stage 689:>                                                        (0 + 1) / 1]                                                                                [Stage 690:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 690:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:46:26] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 690:>                                                        (0 + 1) / 1][Stage 690:>                                                        (0 + 1) / 1][Stage 690:>                                                        (0 + 1) / 1]                                                                                [Stage 691:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:53:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1][Stage 691:>                                                        (0 + 1) / 1]                                                                                [Stage 692:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:11:31] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1][Stage 692:>                                                        (0 + 1) / 1]                                                                                [Stage 693:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:26:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 693:>                                                        (0 + 1) / 1][Stage 693:>                                                        (0 + 1) / 1][Stage 693:>                                                        (0 + 1) / 1]                                                                                [Stage 694:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:32:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1][Stage 694:>                                                        (0 + 1) / 1]                                                                                [Stage 695:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 695:>                                                        (0 + 1) / 1][Stage 695:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [12:48:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 695:>                                                        (0 + 1) / 1][Stage 695:>                                                        (0 + 1) / 1][Stage 695:>                                                        (0 + 1) / 1][Stage 695:>                                                        (0 + 1) / 1][Stage 695:>                                                        (0 + 1) / 1][Stage 695:>                                                        (0 + 1) / 1][Stage 695:>                                                        (0 + 1) / 1][Stage 695:>                                                        (0 + 1) / 1][Stage 695:>                                                        (0 + 1) / 1][Stage 695:>                                                        (0 + 1) / 1]                                                                                [Stage 696:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 696:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:00:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 696:>                                                        (0 + 1) / 1][Stage 696:>                                                        (0 + 1) / 1][Stage 696:>                                                        (0 + 1) / 1]                                                                                [Stage 697:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/dwalke/git/graph_aware_ml/EnsembleFramework.py:330: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)
  summed_exp_score = torch.zeros_like(exp_score).scatter(0, target,exp_score, reduce="add")
[Stage 697:>                                                        (0 + 1) / 1]/home/dwalke/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:05:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)
[Stage 697:>                                                        (0 + 1) / 1][Stage 697:>                                                        (0 + 1) / 1][Stage 697:>                                                        (0 + 1) / 1][Stage 697:>                                                        (0 + 1) / 1]                                                                                trial task 139 failed, exception is An error occurred while calling o12.broadcast.
: java.lang.OutOfMemoryError: Java heap space
.
 None
Total Trials: 140: 139 succeeded, 1 failed, 0 cancelled.
0it [00:00, ?it/s]
PRINT_GRAPH_AWARE_LOGS_START
Best params
[{'attention_config': {'cosine_eps': 0.01, 'dropout_attn': None, 'inter_layer_normalize': False, 'use_pseudo_attention': True}, 'booster': 'gbtree', 'colsample_bytree': 0.9658751354650921, 'device': 'cuda:0', 'early_stopping_rounds': 10, 'eta': 0.10200261556939544, 'eval_metric': 'error', 'gamma': 0.07335810545592145, 'hops': (3,), 'max_delta_step': 1, 'max_depth': None, 'min_child_weight': None, 'n_estimators': 1400, 'reg_alpha': 0.703286202534902, 'reg_lambda': 0.819597510933699, 'scale_pos_weight': 1.9987798382685964, 'tree_method': 'hist', 'user_function': <function user_function at 0x7f30b5c1fd90>}, {'attention_config': {'cosine_eps': 0.01, 'dropout_attn': None, 'inter_layer_normalize': False, 'use_pseudo_attention': True}, 'booster': 'gbtree', 'colsample_bytree': 0.8945908635821919, 'device': 'cuda:0', 'early_stopping_rounds': 10, 'eta': 0.1031336606252661, 'eval_metric': 'error', 'gamma': 0.0010385911682228854, 'hops': (3,), 'max_delta_step': 3, 'max_depth': None, 'min_child_weight': None, 'n_estimators': 1600, 'reg_alpha': 0.0067064380077628445, 'reg_lambda': 0.010086976153085353, 'scale_pos_weight': 1.7623981124631072, 'tree_method': 'hist', 'user_function': <function user_function at 0x7f30b5c1fd90>}, {'attention_config': {'cosine_eps': 0.01, 'dropout_attn': None, 'inter_layer_normalize': False, 'use_pseudo_attention': True}, 'booster': 'gbtree', 'colsample_bytree': 0.9622136423527458, 'device': 'cuda:0', 'early_stopping_rounds': 10, 'eta': 0.11428584063717488, 'eval_metric': 'error', 'gamma': 0.04838591559619437, 'hops': (3,), 'max_delta_step': 2, 'max_depth': None, 'min_child_weight': None, 'n_estimators': 1800, 'reg_alpha': 0.00865431190875096, 'reg_lambda': 0.05570727299337357, 'scale_pos_weight': 1.8281217416786535, 'tree_method': 'hist', 'user_function': <function user_function at 0x7f30b5c1fd90>}, {'attention_config': {'cosine_eps': 0.01, 'dropout_attn': None, 'inter_layer_normalize': False, 'use_pseudo_attention': True}, 'booster': 'gbtree', 'colsample_bytree': 0.8107372687628657, 'device': 'cuda:0', 'early_stopping_rounds': 10, 'eta': 0.10668283660822152, 'eval_metric': 'error', 'gamma': 0.011561496175672297, 'hops': (3,), 'max_delta_step': 1, 'max_depth': None, 'min_child_weight': None, 'n_estimators': 2000, 'reg_alpha': 0.21343975972664317, 'reg_lambda': 0.05941989941600696, 'scale_pos_weight': 1.6096275392952932, 'tree_method': 'hist', 'user_function': <function user_function at 0x7f30b5c1fd90>}, {'attention_config': {'cosine_eps': 0.01, 'dropout_attn': None, 'inter_layer_normalize': False, 'use_pseudo_attention': True}, 'booster': 'gbtree', 'colsample_bytree': 0.8916914810347465, 'device': 'cuda:0', 'early_stopping_rounds': 10, 'eta': 0.09137057094903273, 'eval_metric': 'error', 'gamma': 0.054727688623603304, 'hops': (3,), 'max_delta_step': 1, 'max_depth': None, 'min_child_weight': None, 'n_estimators': 2000, 'reg_alpha': 0.02105291268023254, 'reg_lambda': 0.010485513038875141, 'scale_pos_weight': 1.8781288952033601, 'tree_method': 'hist', 'user_function': <function user_function at 0x7f30b5c1fd90>}]
Outer scores
[0.93931261 0.94117327 0.92525654 0.95137    0.97817661]
0.9470578058941138
0.017645884217469914
Inner scores
0.9386380496797956
0.025782746013153834
Train times
[212.32929801940918, 185.71013164520264, 179.20878648757935, 249.40794205665588, 236.54584789276123]
212.64040122032165
27.445697323948867
PRINT_GRAPH_AWARE_LOGS_END
terminate called after throwing an instance of 'dmlc::Error'
  what():  [13:29:22] /workspace/src/common/common.h:45: /workspace/src/common/host_device_vector.cu: 264: cudaErrorCudartUnloading: driver shutting down
Stack trace:
  [bt] (0) /home/dwalke/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x7b0a7a) [0x7f2e8206da7a]
  [bt] (1) /home/dwalke/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x7b0c99) [0x7f2e8206dc99]
  [bt] (2) /home/dwalke/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x7b8021) [0x7f2e82075021]
  [bt] (3) /home/dwalke/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x7b81c8) [0x7f2e820751c8]
  [bt] (4) /home/dwalke/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x4ab32b) [0x7f2e81d6832b]
  [bt] (5) /home/dwalke/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x4b68e9) [0x7f2e81d738e9]
  [bt] (6) /home/dwalke/miniconda3/bin/../lib/libstdc++.so.6(+0xb4d98) [0x7f30b50d1d98]
  [bt] (7) /lib/x86_64-linux-gnu/libc.so.6(+0x45495) [0x7f30b5fef495]
  [bt] (8) /lib/x86_64-linux-gnu/libc.so.6(on_exit+0) [0x7f30b5fef610]


